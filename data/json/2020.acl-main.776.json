{
  "id": "2020.acl-main.776",
  "title": "Revisiting Higher-Order Dependency Parsers",
  "authors": [
    "Fonseca, Erick  and\nMartins, Andr{\\'e} F. T."
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers. This has led to a belief that neural encoders can implicitly encode structural constraints, such as siblings and grandparents in a tree. We tested this hypothesis and found that neural parsers may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures.",
  "keywords": [
    "neural encoders",
    "a powerful pre-trained encoder",
    "a belief",
    "neural",
    "bert",
    "dependency parsers",
    "encoder",
    "dependencies",
    "dependency",
    "we",
    "higher-order dependency parsers",
    "better accuracy",
    "long-range dependencies",
    "pre",
    "modifier lists"
  ],
  "url": "https://aclanthology.org/2020.acl-main.776/",
  "provenance": {
    "collected_at": "2025-06-05 07:52:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}