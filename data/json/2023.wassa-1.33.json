{
  "id": "2023.wassa-1.33",
  "title": "Can {C}hat{GPT} Understand Causal Language in Science Claims?",
  "authors": [
    "Kim, Yuheun  and\nGuo, Lu  and\nYu, Bei  and\nLi, Yingya"
  ],
  "year": "2023",
  "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis",
  "abstract": "This study evaluated ChatGPTâ€™s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.",
  "keywords": [
    "prompts",
    "accuracy",
    "prompt",
    "language",
    "chatgpt s ability",
    "bert",
    "human",
    "chain",
    "c hat gpt",
    "its accuracy",
    "gpt",
    "science",
    "chatgpt",
    "prompt performance",
    "science papers"
  ],
  "url": "https://aclanthology.org/2023.wassa-1.33/",
  "provenance": {
    "collected_at": "2025-06-05 10:33:23",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}