{
  "id": "2020.acl-main.616",
  "title": "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks",
  "authors": [
    "Chai, Yekun  and\nJin, Shuo  and\nHou, Xinwen"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.",
  "keywords": [
    "gradient descent algorithms",
    "semantic",
    "we",
    "lstm",
    "the optimization process",
    "it",
    "self",
    "information",
    "latent",
    "sequence",
    "lstm-styled gating units",
    "modulated latent embeddings",
    "dimensional",
    "internal semantic importance",
    "embeddings"
  ],
  "url": "https://aclanthology.org/2020.acl-main.616/",
  "provenance": {
    "collected_at": "2025-06-05 07:50:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}