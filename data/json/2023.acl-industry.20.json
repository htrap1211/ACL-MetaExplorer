{
  "id": "2023.acl-industry.20",
  "title": "Tab-{CQA}: A Tabular Conversational Question Answering Dataset on Financial Reports",
  "authors": [
    "Liu, Chuang  and\nLi, Junzhuo  and\nXiong, Deyi"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
  "abstract": "Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in the past 30 years. From these reports, we select 2,463 tables, and manually generate 2,463 conversations with 35,494 QA pairs. Additionally, we select 4,578 tables, from which 4,578 conversations with 73,595 QA pairs are automatically created via a template-based method. With the manually- and automatically-generated conversations, Tab-CQA contains answerable and unanswerable questions. For the answerable questions, we further diversify them to cover a wide range of skills, e.g., table retrieval, fact checking, numerical reasoning, so as to accommodate real-world scenarios. We further propose two different tabular CQA models, a text-based model and an operation-based model, and evaluate them on Tab-CQA. Experiment results show that Tab-CQA is a very challenging dataset, where a huge performance gap exists between human and neural models. We will publicly release Tab-CQA as a benchmark testbed to promote further research on Chinese tabular CQA.",
  "keywords": [
    "listed companies",
    "a tabular cqa dataset",
    "conversations",
    "question",
    "we",
    "73 595 qa pairs",
    "neural",
    "retrieval",
    "2 463 conversations",
    "35 494 qa pairs",
    "tab-cqa experiment results",
    "a tabular conversational question",
    "text",
    "existing conversational question",
    "cqa datasets"
  ],
  "url": "https://aclanthology.org/2023.acl-industry.20/",
  "provenance": {
    "collected_at": "2025-06-05 09:52:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}