{
  "id": "2020.acl-main.745",
  "title": "T}a{BERT}: Pretraining for Joint Understanding of Textual and Tabular Data",
  "authors": [
    "Yin, Pengcheng  and\nNeubig, Graham  and\nYih, Wen-tau  and\nRiedel, Sebastian"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.",
  "keywords": [
    "parsing",
    "language",
    "neural",
    "natural",
    "bert",
    "text",
    "neural semantic parsers",
    "semi- structured tables tabert",
    "pretrained language models lms",
    "form",
    "semantic",
    "we",
    "semantic parsing",
    "tabert",
    "nl sentences"
  ],
  "url": "https://aclanthology.org/2020.acl-main.745/",
  "provenance": {
    "collected_at": "2025-06-05 07:52:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}