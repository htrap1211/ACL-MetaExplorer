{
  "id": "2023.acl-long.660",
  "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale",
  "authors": [
    "Bansal, Hritik  and\nGopalakrishnan, Karthik  and\nDingliwal, Saket  and\nBodapati, Sravan  and\nKirchhoff, Katrin  and\nRoth, Dan"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: ~70% of the attention heads and ~20% of the feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These induction heads overlap with task-specific important heads, reinforcing arguments by Olsson et al. (2022) regarding induction head generality to more sophisticated behaviors associated with in-context learning. Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.",
  "keywords": [
    "variety",
    "all",
    "we",
    "a wide variety",
    "parameter",
    "the attention heads",
    "induction head generality",
    "generality",
    "learning",
    "feed",
    "attention heads",
    "a large language model",
    "un",
    "language",
    "model"
  ],
  "url": "https://aclanthology.org/2023.acl-long.660/",
  "provenance": {
    "collected_at": "2025-06-05 09:44:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}