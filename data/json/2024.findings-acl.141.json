{
  "id": "2024.findings-acl.141",
  "title": "K}o{C}ommon{GEN} v2: A Benchmark for Navigating {K}orean Commonsense Reasoning Challenges in Large Language Models",
  "authors": [
    "Seo, Jaehyung  and\nLee, Jaewook  and\nPark, Chanjun  and\nHong, SeongTae  and\nLee, Seungjun  and\nLim, Heuiseok"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "The evolution of large language models (LLMs) has culminated in a multitask model paradigm where prompts drive the generation of user-specific outputs. However, this advancement has revealed a critical challenge: LLMs frequently produce outputs against socially acceptable commonsense standards in various scenarios. To address this gap in commonsense reasoning, we present KoCommonGEN v2, a fine-grained benchmark dataset focused on Korean commonsense reasoning. This dataset, enriched with human annotations, comprises multiple-choice questions across seven error categories. These categories include commonsense memorization, numerical commonsense, toxic speech, and more, which are vulnerable to undermining the reliability of LLMs’ commonsense reasoning capabilities. The empirical results present that LLMs struggle with Korean commonsense reasoning. With human accuracy benchmarked at approximately 85%, GPT-4’s performance lags at about 74%, and other LLMs demonstrate an average accuracy of around 42%. Our findings emphasize the need for targeted improvements in Korean commonsense reasoning within LLMs, paving the way for more socially and contextually sensitive AI models.",
  "keywords": [
    "commonsense reasoning capabilities",
    "the generation",
    "we",
    "a critical challenge llms",
    "other llms",
    "vulnerable",
    "llms",
    "categories",
    "an average accuracy",
    "gen",
    "large language models llms",
    "gpt-4",
    "accuracy",
    "prompts",
    "human accuracy"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.141/",
  "provenance": {
    "collected_at": "2025-06-05 10:50:31",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}