{
  "id": "2020.acl-main.314",
  "title": "Do you have the right scissors? Tailoring Pre-trained Language Models via {M}onte-{C}arlo Methods",
  "authors": [
    "Miao, Ning  and\nSong, Yuxuan  and\nZhou, Hao  and\nLi, Lei"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.",
  "keywords": [
    "variety",
    "tuning",
    "language",
    "generation",
    "pre-trained language models",
    "model",
    "text",
    "it",
    "text generation datasets",
    "fine",
    "we",
    "text generation tasks",
    "a variety",
    "pre",
    "the fine-tuning approach"
  ],
  "url": "https://aclanthology.org/2020.acl-main.314/",
  "provenance": {
    "collected_at": "2025-06-05 07:46:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}