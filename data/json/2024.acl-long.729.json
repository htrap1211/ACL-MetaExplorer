{
  "id": "2024.acl-long.729",
  "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
  "authors": [
    "Xie, Jiawen  and\nCheng, Pengyu  and\nLiang, Xiao  and\nDai, Yong  and\nDu, Nan"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Although dominant in natural language processing, transformer-based models still struggle with long-sequence processing, due to the computational costs of their self-attention operations, which increase exponentially as the length of the input sequence grows. To address this challenge, we propose a **Sim**ple framework to enhance the long-content processing of off-the-shelf pre-trained transformers via three steps: **C**hunk, **A**lign, and **S**elect (SimCAS). More specifically, we first divide each long-sequence input into a batch of chunks, then align the inter-chunk information during the encoding steps, and finally, select the most representative hidden states from the encoder for the decoding process. With our SimCAS, the computation and memory costs can be reduced to linear complexity. In experiments, we demonstrate the effectiveness of the proposed method on various real-world long-text summarization and reading comprehension tasks, in which SimCAS significantly outperforms prior long-sequence processing baselines. The code is at [https://github.com/xjw-nlp/SimCAS](https://github.com/xjw-nlp/SimCAS).",
  "keywords": [
    "code",
    "transformers",
    "various real-world long-text summarization",
    "summarization",
    "we",
    "the encoder",
    "natural",
    "self",
    "c",
    "information",
    "sequence",
    "align",
    "processing",
    "text",
    "transformer"
  ],
  "url": "https://aclanthology.org/2024.acl-long.729/",
  "provenance": {
    "collected_at": "2025-06-05 10:44:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}