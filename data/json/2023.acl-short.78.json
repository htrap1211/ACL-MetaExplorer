{
  "id": "2023.acl-short.78",
  "title": "Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality",
  "authors": [
    "Dixit, Tanay  and\nWang, Fei  and\nChen, Muhao"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro ‘MODEL’}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.",
  "keywords": [
    "similarity-based metrics",
    "our refined candidate summaries",
    "summaries",
    "i",
    "generation",
    "cnn",
    "two metrics",
    "model",
    "either similarity-based metrics",
    "it",
    "metrics",
    "a widely studied topic",
    "abstractive summarization",
    "topic",
    "summarization"
  ],
  "url": "https://aclanthology.org/2023.acl-short.78/",
  "provenance": {
    "collected_at": "2025-06-05 09:49:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}