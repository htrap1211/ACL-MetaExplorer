{
  "id": "2024.acl-short.11",
  "title": "S}ce{MQA}: A Scientific College Entrance Level Multimodal Question Answering Benchmark",
  "authors": [
    "Liang, Zhenwen  and\nGuo, Kehan  and\nLiu, Gang  and\nGuo, Taicheng  and\nZhou, Yujun  and\nYang, Tianyu  and\nJiao, Jiajun  and\nPi, Renjie  and\nZhang, Jipeng  and\nZhang, Xiangliang"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI modelsâ€™ abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models.",
  "keywords": [
    "ai models abilities",
    "varied",
    "scientific multimodal question",
    "question",
    "we",
    "core science subjects",
    "each answer scemqa",
    "answer",
    "it",
    "science",
    "a comprehensive evaluation",
    "scientific",
    "mllm",
    "more capable mllm",
    "core"
  ],
  "url": "https://aclanthology.org/2024.acl-short.11/",
  "provenance": {
    "collected_at": "2025-06-05 10:46:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}