{
  "id": "2023.acl-long.279",
  "title": "Z}-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization",
  "authors": [
    "He, Pengcheng  and\nPeng, Baolin  and\nWang, Song  and\nLiu, Yang  and\nXu, Ruochen  and\nHassan, Hany  and\nShi, Yu  and\nZhu, Chenguang  and\nXiong, Wayne  and\nZeng, Michael  and\nGao, Jianfeng  and\nHuang, Xuedong"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state-of-the-art encoder-decoder model using three techniques. First, we use a two-phase pre-training to improve the modelâ€™s performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ createsa new state-of-the-art on 9 of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM540B on XSum, and the finetuned 200x larger GPT3175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.",
  "keywords": [
    "code",
    "13 text summarization tasks",
    "efficient",
    "a pre-trained language model",
    "summarization",
    "we",
    "fusion",
    "abstractive text summarization",
    "parameter",
    "shot",
    "training",
    "the encoder",
    "it",
    "disentangled attention layers",
    "self"
  ],
  "url": "https://aclanthology.org/2023.acl-long.279/",
  "provenance": {
    "collected_at": "2025-06-05 09:39:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}