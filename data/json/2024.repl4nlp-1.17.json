{
  "id": "2024.repl4nlp-1.17",
  "title": "Learned Transformer Position Embeddings Have a Low-Dimensional Structure",
  "authors": [
    "Wennberg, Ulme  and\nHenter, Gustav"
  ],
  "year": "2024",
  "venue": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
  "abstract": "Position embeddings have long been essential for sequence-order encoding in transformer models, yet their structure is underexplored. This study uses principal component analysis (PCA) to quantitatively compare the dimensionality of absolute position and word embeddings in BERT and ALBERT. We find that, unlike word embeddings, position embeddings occupy a low-dimensional subspace, typically utilizing under 10% of the dimensions available. Additionally, the principal vectors are dominated by a few low-frequency rotational components, a structure arising independently across models.",
  "keywords": [
    "embeddings",
    "transformer",
    "transformer models",
    "the principal vectors",
    "bert",
    "dimensionality",
    "vectors",
    "albert",
    "dimensional",
    "word",
    "we",
    "sequence",
    "word embeddings position embeddings",
    "word embeddings",
    "analysis"
  ],
  "url": "https://aclanthology.org/2024.repl4nlp-1.17/",
  "provenance": {
    "collected_at": "2025-06-05 11:09:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}