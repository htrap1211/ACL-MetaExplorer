{
  "id": "W19-3814",
  "title": "Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution",
  "authors": [
    "Xu, Yinchuan  and\nYang, Junlin"
  ],
  "year": "2019",
  "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
  "abstract": "Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9% F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific task is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCNâ€™s embeddings outperform the original BERT embeddings on the coreference task. Our work significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9% to 80.3%. We participated in the Gender Bias for Natural Language Processing 2019 shared task, and our codes are available online.",
  "keywords": [
    "however fine-tune bert",
    "transformers",
    "bias",
    "end",
    "we",
    "graph",
    "convolutional",
    "syntax",
    "natural",
    "information",
    "natural language processing",
    "the original bert embeddings",
    "processing",
    "bert",
    "existing coreference resolvers"
  ],
  "url": "https://aclanthology.org/W19-3814/",
  "provenance": {
    "collected_at": "2025-06-05 00:59:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}