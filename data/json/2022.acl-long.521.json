{
  "id": "2022.acl-long.521",
  "title": "Memorisation versus Generalisation in Pre-trained Language Models",
  "authors": [
    "T{\\\"a}nzer, Michael  and\nRuder, Sebastian  and\nRei, Marek"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks.",
  "keywords": [
    "language",
    "pre-trained language models",
    "it",
    "generalisation",
    "few-shot learning",
    "capabilities",
    "we",
    "learning",
    "pre",
    "entity",
    "training",
    "shot",
    "that",
    "recognition",
    "limited amounts"
  ],
  "url": "https://aclanthology.org/2022.acl-long.521/",
  "provenance": {
    "collected_at": "2025-06-05 08:31:19",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}