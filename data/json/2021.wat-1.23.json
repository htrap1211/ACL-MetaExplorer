{
  "id": "2021.wat-1.23",
  "title": "NICT}-5{'}s Submission To {WAT} 2021: {MBART} Pre-training And In-Domain Fine Tuning For Indic Languages",
  "authors": [
    "Dabre, Raj  and\nChakrabarty, Abhisek"
  ],
  "year": "2021",
  "venue": "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
  "abstract": "In this paper we describe our submission to the multilingual Indic language translation wtask “MultiIndicMT” under the team name “NICT-5”. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.",
  "keywords": [
    "variety",
    "tuning",
    "language",
    "model",
    "versa",
    "objective",
    "the objective",
    "-",
    "translation quality",
    "fine",
    "we",
    "a variety",
    "pre",
    "furthermore multilingual fine-tuning",
    "training"
  ],
  "url": "https://aclanthology.org/2021.wat-1.23/",
  "provenance": {
    "collected_at": "2025-06-05 08:23:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}