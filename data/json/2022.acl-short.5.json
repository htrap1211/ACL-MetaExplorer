{
  "id": "2022.acl-short.5",
  "title": "On the probability{--}quality paradox in language generation",
  "authors": [
    "Meister, Clara  and\nWiher, Gian  and\nPimentel, Tiago  and\nCotterell, Ryan"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text—covering multiple tasks and common decoding strategies—suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.",
  "keywords": [
    "we",
    "language generation",
    "neural",
    "natural",
    "it",
    "information",
    "natural language",
    "log",
    "i",
    "text",
    "strategies",
    "mode",
    "language",
    "generation",
    "model"
  ],
  "url": "https://aclanthology.org/2022.acl-short.5/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}