{
  "id": "2024.arabicnlp-1.20",
  "title": "A}rab{L}egal{E}val: A Multitask Benchmark for Assessing {A}rabic Legal Knowledge in Large Language Models",
  "authors": [
    "Hijazi, Faris  and\nAlharbi, Somayah  and\nAlHussein, Abdulaziz  and\nShairah, Harethah  and\nAlzahrani, Reem  and\nAlshamlan, Hebah  and\nTurkiyyah, George  and\nKnio, Omar"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "The rapid advancements in Large Language Models (LLMs) have led to significant improvements in various natural language processing tasks. However, the evaluation of LLMs’ legal knowledge, particularly in non English languages such as Arabic, remains under-explored. To address this gap, we introduce ArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal knowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval consists of multiple tasks sourced from Saudi legal documents and synthesized questions. In this work, we aim to analyze the capabilities required to solve legal problems in Arabic and benchmark the performance of state-of-the-art LLMs. We explore the impact of in-context learning on performance and investigate various evaluation methods. Additionally, we explore workflows for automatically generating questions with automatic validation to enhance the dataset’s quality. By releasing ArabLegalEval and our code, we hope to accelerate AI research in the Arabic Legal domain",
  "keywords": [
    "code",
    "validation",
    "we",
    "val",
    "natural",
    "learning",
    "the capabilities",
    "llms",
    "processing",
    "large language models llms",
    "work",
    "knowledge",
    "language",
    "large language models",
    "capabilities"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.20/",
  "provenance": {
    "collected_at": "2025-06-05 11:02:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}