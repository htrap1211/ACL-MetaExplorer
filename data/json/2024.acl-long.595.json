{
  "id": "2024.acl-long.595",
  "title": "I}ndic{G}en{B}ench: A Multilingual Benchmark to Evaluate Generation Capabilities of {LLM}s on {I}ndic Languages",
  "authors": [
    "Singh, Harman  and\nGupta, Nitish  and\nBharadwaj, Shikhar  and\nTewari, Dinesh  and\nTalukdar, Partha"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench â€” the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate stateof-the-art LLMs like GPT-3.5, GPT-4, PaLM2, and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench isavailable at www.github.com/google-researchdatasets/indic-gen-bench",
  "keywords": [
    "variety",
    "families",
    "user-facing generation tasks",
    "llm s",
    "question",
    "multilingual llm evaluation",
    "summarization",
    "we",
    "llm",
    "translation",
    "cross",
    "it",
    "gpt-3",
    "a variety",
    "llms"
  ],
  "url": "https://aclanthology.org/2024.acl-long.595/",
  "provenance": {
    "collected_at": "2025-06-05 10:42:31",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}