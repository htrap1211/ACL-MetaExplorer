{
  "id": "2023.acl-short.155",
  "title": "Revisiting Automated Prompting: Are We Actually Doing Better?",
  "authors": [
    "Zhou, Yulin  and\nZhao, Yiren  and\nShumailov, Ilia  and\nMullins, Robert  and\nGal, Yarin"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates that automation can outperform fine-tuning in certain K-shot learning scenarios. In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompting. Our work suggests that, in addition to fine-tuning, manual prompting should be used as a baseline in this line of research.",
  "keywords": [
    "work",
    "human-led prompting",
    "great few-shot learners",
    "language",
    "learners",
    "human",
    "fine-tuning manual prompting",
    "a few-shot learning",
    "large language models llms",
    "prompting",
    "fine",
    "we",
    "learning",
    "automated prompting",
    "current"
  ],
  "url": "https://aclanthology.org/2023.acl-short.155/",
  "provenance": {
    "collected_at": "2025-06-05 09:50:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}