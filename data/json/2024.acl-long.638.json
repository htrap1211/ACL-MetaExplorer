{
  "id": "2024.acl-long.638",
  "title": "C}haracter{E}val: A {C}hinese Benchmark for Role-Playing Conversational Agent Evaluation",
  "authors": [
    "Tu, Quan  and\nFan, Shilong  and\nTian, Zihang  and\nShen, Tianhao  and\nShang, Shuo  and\nGao, Xin  and\nYan, Rui"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduceCharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike.CharacterEvalemploys a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics inCharacterEval, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments onCharacterEvaldemonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.",
  "keywords": [
    "extraction",
    "chinese role-playing conversation",
    "field",
    "comprehensive rpca assessment",
    "the convenient evaluation",
    "we",
    "dialogue",
    "generative agents",
    "val",
    "rpcas",
    "these subjective metrics incharactereval",
    "a multifaceted evaluation approach",
    "it",
    "role-playing conversational agent evaluation",
    "considerable attention"
  ],
  "url": "https://aclanthology.org/2024.acl-long.638/",
  "provenance": {
    "collected_at": "2025-06-05 10:43:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}