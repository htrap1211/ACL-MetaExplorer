{
  "id": "2023.semeval-1.303",
  "title": "azaad@{BND} at {S}em{E}val-2023 Task 2: How to Go from a Simple Transformer Model to a Better Model to Get Better Results in Natural Language Processing",
  "authors": [
    "Ahmadi, Reza  and\nArefi, Shiva  and\nJafarabad, Mohammad"
  ],
  "year": "2023",
  "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
  "abstract": "In this article, which was prepared for the sameval2023 competition (task number 2), information about the implementation techniques of the transformer model and the use of the pre-trained BERT model in order to identify the named entity (NER) in the English language, has been collected and also the implementation method is explained. Finally, it led to an F1 score of about 57% for Fine-grained and 72% for Coarse-grained in the dev data. In the final test data, F1 score reached 50%.",
  "keywords": [
    "transformer",
    "ner",
    "processing",
    "the transformer model",
    "language",
    "an f1 score",
    "natural",
    "bert",
    "model",
    "it",
    "information",
    "natural language processing",
    "pre",
    "a simple transformer model",
    "entity"
  ],
  "url": "https://aclanthology.org/2023.semeval-1.303/",
  "provenance": {
    "collected_at": "2025-06-05 10:30:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}