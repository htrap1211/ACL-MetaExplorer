{
  "id": "2023.acl-long.105",
  "title": "k{NN}-{TL}: k-Nearest-Neighbor Transfer Learning for Low-Resource Neural Machine Translation",
  "authors": [
    "Liu, Shudong  and\nLiu, Xuebo  and\nWong, Derek F.  and\nLi, Zhaocong  and\nJiao, Wenxiang  and\nChao, Lidia S.  and\nZhang, Min"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Transfer learning has been shown to be an effective technique for enhancing the performance of low-resource neural machine translation (NMT). This is typically achieved through either fine-tuning a child model with a pre-trained parent model, or by utilizing the out- put of the parent model during the training of the child model. However, these methods do not make use of the parent knowledge during the child inference, which may limit the translation performance. In this paper, we propose a k-Nearest-Neighbor Transfer Learning (kNN-TL) approach for low-resource NMT, which leverages the parent knowledge throughout the entire developing process of the child model. Our approach includes a parent-child representation alignment method, which ensures consistency in the output representations between the two models, and a child-aware datastore construction method that improves inference efficiency by selectively distilling the parent datastore based on relevance to the child model. Experimental results on four low-resource translation tasks show that kNN-TL outperforms strong baselines. Extensive analyses further demonstrate the effectiveness of our approach. Code and scripts are freely available athttps://github.com/NLP2CT/kNN-TL.",
  "keywords": [
    "code",
    "inference efficiency",
    "efficiency",
    "we",
    "nlp2ct",
    "training",
    "translation",
    "the translation performance",
    "neural",
    "learning",
    "transfer",
    "alignment",
    "process",
    "knowledge",
    "four low-resource translation tasks"
  ],
  "url": "https://aclanthology.org/2023.acl-long.105/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}