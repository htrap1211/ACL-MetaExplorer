{
  "id": "2024.acl-long.407",
  "title": "SEGO}: Sequential Subgoal Optimization for Mathematical Problem-Solving",
  "authors": [
    "Zhao, Xueliang  and\nHuang, Xinting  and\nBi, Wei  and\nKong, Lingpeng"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Large Language Models (LLMs) have driven substantial progress in artificial intelligence in recent years, exhibiting impressive capabilities across a wide range of tasks, including mathematical problem-solving. Inspired by the success of subgoal-based methods, we propose a novel framework calledSEquential subGoalOptimization (SEGO) to enhance LLMs’ ability to solve mathematical problems. By establishing a connection between the subgoal breakdown process and the probability of solving problems, SEGO aims to identify better subgoals with theoretical guarantees. Addressing the challenge of identifying suitable subgoals in a large solution space, our framework generates problem-specific subgoals and adjusts them according to carefully designed criteria. Incorporating these optimized subgoals into the policy model training leads to significant improvements in problem-solving performance. We validate SEGO’s efficacy through experiments on two benchmarks, GSM8K and MATH, where our approach outperforms existing methods, highlighting the potential of SEGO in AI-driven mathematical problem-solving.",
  "keywords": [
    "we",
    "training",
    "subgoaloptimization",
    "impressive capabilities",
    "llms",
    "sequential subgoal optimization",
    "large language models llms",
    "process",
    "language",
    "model",
    "capabilities",
    "optimization",
    "approach",
    "problem-solving performance",
    "carefully designed criteria"
  ],
  "url": "https://aclanthology.org/2024.acl-long.407/",
  "provenance": {
    "collected_at": "2025-06-05 10:39:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}