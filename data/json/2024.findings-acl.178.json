{
  "id": "2024.findings-acl.178",
  "title": "L}o{RAP}rune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning",
  "authors": [
    "Zhang, Mingyang  and\nChen, Hao  and\nShen, Chunhua  and\nYang, Zhen  and\nOu, Linlin  and\nYu, Xinyi  and\nZhuang, Bohan"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead.To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We subsequently integrate this criterion into an iterative pruning process, effectively removing redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models.At a 50% compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner, achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while also decreasing memory usage by 52.6%.Besides, LoRAPrune also matches semi-structural pruning across multiple LLMs, proving its wide applicability. The code is available at https://github.com/aim-uofa/LoRAPrune.",
  "keywords": [
    "code",
    "end",
    "rap",
    "efficient",
    "rate",
    "series",
    "pruner",
    "we",
    "current",
    "llm",
    "parameter",
    "training",
    "a highly memory-efficient manner",
    "multiple llms",
    "llm-pruner"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.178/",
  "provenance": {
    "collected_at": "2025-06-05 10:51:01",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}