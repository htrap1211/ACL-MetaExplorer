{
  "id": "2020.ngt-1.25",
  "title": "Efficient and High-Quality Neural Machine Translation with {O}pen{NMT",
  "authors": [
    "Klein, Guillaume  and\nZhang, Dakun  and\nChouteau, Cl{\\'e}ment  and\nCrego, Josep  and\nSenellart, Jean"
  ],
  "year": "2020",
  "venue": "Proceedings of the Fourth Workshop on Neural Generation and Translation",
  "abstract": "This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models.",
  "keywords": [
    "transformer",
    "optimizations",
    "transformer models",
    "neural",
    "machine",
    "efficient",
    "dependencies",
    "few dependencies",
    "efficiency",
    "we",
    "additional optimizations",
    "pen",
    "fast",
    "training",
    "translation"
  ],
  "url": "https://aclanthology.org/2020.ngt-1.25/",
  "provenance": {
    "collected_at": "2025-06-05 07:57:15",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}