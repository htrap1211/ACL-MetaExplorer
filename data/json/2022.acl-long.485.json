{
  "id": "2022.acl-long.485",
  "title": "BERT} Learns to Teach: Knowledge Distillation with Meta Learning",
  "authors": [
    "Zhou, Wangchunshu  and\nXu, Canwen  and\nMcAuley, Julian"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e.,learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.",
  "keywords": [
    "alignment",
    "knowledge",
    "hyperparameters",
    "i",
    "feedback",
    "bert",
    "model",
    "the inner-learner and meta-learner",
    "the alignment",
    "inner",
    "we",
    "learning",
    "network",
    "learner",
    "an improved inner-learner experiments"
  ],
  "url": "https://aclanthology.org/2022.acl-long.485/",
  "provenance": {
    "collected_at": "2025-06-05 08:30:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}