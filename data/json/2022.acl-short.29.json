{
  "id": "2022.acl-short.29",
  "title": "Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer",
  "authors": [
    "Lai, Huiyuan  and\nToral, Antonio  and\nNissim, Malvina"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "We exploit the pre-trained seq2seq model mBART for multilingual text style transfer. Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider. Besides, in view of the general scarcity of parallel data, we propose a modular approach for multilingual formality transfer, which consists of two training strategies that target adaptation to both language and task. Our approach achieves competitive performance without monolingual task-specific parallel data and can be applied to other style transfer tasks as well as to other languages.",
  "keywords": [
    "seq2seq",
    "general",
    "language",
    "model",
    "machine",
    "text",
    "the general scarcity",
    "strategies",
    "view",
    "two training strategies",
    "we",
    "transfer",
    "pre",
    "training",
    "that"
  ],
  "url": "https://aclanthology.org/2022.acl-short.29/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}