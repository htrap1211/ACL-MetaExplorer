{
  "id": "2022.repl4nlp-1.16",
  "title": "Detecting Word-Level Adversarial Text Attacks via {SH}apley Additive ex{P}lanations",
  "authors": [
    "Huber, Lukas  and\nK{\\\"u}hn, Marc Alexander  and\nMosca, Edoardo  and\nGroh, Georg"
  ],
  "year": "2022",
  "venue": "Proceedings of the 7th Workshop on Representation Learning for NLP",
  "abstract": "State-of-the-art machine learning models are prone to adversarial attacks”:” Maliciously crafted inputs to fool the model into making a wrong prediction, often with high confidence. While defense strategies have been extensively explored in the computer vision domain, research in natural language processing still lacks techniques to make models resilient to adversarial text inputs. We adapt a technique from computer vision to detect word-level attacks targeting text classifiers. This method relies on training an adversarial detector leveraging Shapley additive explanations and outperforms the current state-of-the-art on two benchmarks. Furthermore, we prove the detector requires only a low amount of training samples and, in some cases, generalizes to different datasets without needing to retrain.",
  "keywords": [
    "resilient",
    "processing",
    "language",
    "natural",
    "model",
    "text",
    "machine",
    "text classifiers",
    "strategies",
    "classifiers",
    "word",
    "we",
    "learning",
    "natural language processing",
    "current"
  ],
  "url": "https://aclanthology.org/2022.repl4nlp-1.16/",
  "provenance": {
    "collected_at": "2025-06-05 08:45:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}