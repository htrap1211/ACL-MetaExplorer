{
  "id": "P19-1147",
  "title": "On the Robustness of Self-Attentive Models",
  "authors": [
    "Hsieh, Yu-Lun  and\nCheng, Minhao  and\nJuan, Da-Cheng  and\nWei, Wei  and\nHsu, Wen-Lian  and\nHsieh, Cho-Jui"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.",
  "keywords": [
    "work",
    "neural",
    "extraction",
    "natural",
    "machine",
    "self-attentive neural networks",
    "self",
    "attention",
    "we",
    "entailment and machine translation",
    "recurrent",
    "the attention",
    "analysis",
    "translation",
    "sentiment"
  ],
  "url": "https://aclanthology.org/P19-1147/",
  "provenance": {
    "collected_at": "2025-06-05 00:33:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}