{
  "id": "P18-1008",
  "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
  "authors": [
    "Chen, Mia Xu  and\nFirat, Orhan  and\nBapna, Ankur  and\nJohnson, Melvin  and\nMacherey, Wolfgang  and\nFoster, George  and\nJones, Llion  and\nSchuster, Mike  and\nShazeer, Noam  and\nParmar, Niki  and\nVaswani, Ashish  and\nUszkoreit, Jakob  and\nKaiser, Lukasz  and\nChen, Zhifeng  and\nWu, Yonghui  and\nHughes, Macduff"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMTâ€™14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
  "keywords": [
    "neural machine translation",
    "machine translation",
    "all",
    "we",
    "convolutional",
    "training",
    "translation",
    "each fundamental seq2seq architecture",
    "neural",
    "properties",
    "sequence",
    "other seq2seq architectures",
    "the classic rnn-based approaches",
    "the convolutional seq2seq model",
    "seq2seq"
  ],
  "url": "https://aclanthology.org/P18-1008/",
  "provenance": {
    "collected_at": "2025-06-05 00:09:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}