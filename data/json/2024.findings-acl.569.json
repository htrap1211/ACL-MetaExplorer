{
  "id": "2024.findings-acl.569",
  "title": "Enhancing Cross Text-Molecule Learning by Self-Augmentation",
  "authors": [
    "Jiang, Yinuo  and\nZhuang, Xiang  and\nDing, Keyan  and\nZhang, Qiang  and\nChen, Huajun"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "The development of Large Language Models (LLMs) has greatly advanced the field of drug discovery, with the belief that natural language can enhance human control over molecule design. However, the scarcity of high-quality labeled data remains a challenge for cross text-molecule learning. Existing datasets are limited due to the difficulty of collecting precise molecule-description pairs. Although recent efforts have utilized pseudo data generated by LLMs for augmentation, the lack of specialized chemistry knowledge of LLMs and the absence of an effective high quality data selector may introduce noise into the annotations, compromising the models’ robustness. To address these challenges, this paper introduces a novel framework that interweaves model fine-tuning and data augmentation to overcome the scarcity of high-quality data. The proposed approach involves an iterative procedure where the model plays dual roles in annotating unlabeled data and sampling a subset of high-quality data until convergence is achieved, enhancing the model’s understanding and adaptability. Additionally, a new dataset called SAPubChem-41 is presented, which comprises meticulously curated high-quality parallel molecule-description pairs designed specifically for fine-tuning purposes. This research provides an important contribution to the field by addressing the need for high-quality datasets and presenting an effective framework for cross text-molecule learning.",
  "keywords": [
    "field",
    "cross",
    "natural",
    "the field",
    "self",
    "learning",
    "natural language",
    "llms",
    "tuning",
    "the belief",
    "text",
    "large language models llms",
    "fine",
    "fine-tuning purposes",
    "knowledge"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.569/",
  "provenance": {
    "collected_at": "2025-06-05 10:56:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}