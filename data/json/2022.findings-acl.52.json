{
  "id": "2022.findings-acl.52",
  "title": "Human Language Modeling",
  "authors": [
    "Soni, Nikita  and\nMatero, Matthew  and\nBalasubramanian, Niranjan  and\nSchwartz, H. Andrew"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate itâ€™s effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. Results on all tasks meet or surpass the current state-of-the-art.",
  "keywords": [
    "transformer",
    "hierarchical",
    "tuning",
    "the language modeling problem",
    "language",
    "natural",
    "model",
    "a large-scale transformer model",
    "it",
    "human",
    "both language modeling perplexity",
    "modeling",
    "perplexity",
    "e",
    "views"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.52/",
  "provenance": {
    "collected_at": "2025-06-05 08:35:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}