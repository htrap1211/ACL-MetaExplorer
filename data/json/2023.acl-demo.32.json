{
  "id": "2023.acl-demo.32",
  "title": "O}pen{RT}: An Open-source Framework for Reasoning Over Tabular Data",
  "authors": [
    "Zhao, Yilun  and\nMi, Boyu  and\nQi, Zhenting  and\nNan, Linyong  and\nGuo, Minghao  and\nCohan, Arman  and\nRadev, Dragomir"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
  "abstract": "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems.",
  "keywords": [
    "generation",
    "text",
    "existing table pre-training models",
    "unified",
    "question",
    "table pre-training methods",
    "we",
    "pre",
    "pen",
    "six table pre-training models",
    "training",
    "a limited number",
    "different baseline systems",
    "fair",
    "performances"
  ],
  "url": "https://aclanthology.org/2023.acl-demo.32/",
  "provenance": {
    "collected_at": "2025-06-05 09:51:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}