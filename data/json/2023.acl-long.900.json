{
  "id": "2023.acl-long.900",
  "title": "Character-Aware Models Improve Visual Text Rendering",
  "authors": [
    "Liu, Rosanne  and\nGarrette, Dan  and\nSaharia, Chitwan  and\nChan, William  and\nRoberts, Adam  and\nNarang, Sharan  and\nBlok, Irina  and\nMical, Rj  and\nNorouzi, Mohammad  and\nConstant, Noah"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a wordâ€™s visual makeup as a series of glyphs. To quantify this effect, we conduct a series of experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Applying our learnings to the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples.",
  "keywords": [
    "series",
    "we",
    "current",
    "image generation models",
    "it",
    "character-blind text encoders",
    "word",
    "visual",
    "current image generation models",
    "text",
    "encoders",
    "accuracy",
    "30 point accuracy gains",
    "a series",
    "generation"
  ],
  "url": "https://aclanthology.org/2023.acl-long.900/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}