{
  "id": "2020.figlang-1.37",
  "title": "A Transformer Approach to Contextual Sarcasm Detection in {T}witter",
  "authors": [
    "Gregory, Hunter  and\nLi, Steven  and\nMohammadi, Pouya  and\nTarn, Natalie  and\nDraelos, Rachel  and\nRudin, Cynthia"
  ],
  "year": "2020",
  "venue": "Proceedings of the Second Workshop on Figurative Language Processing",
  "abstract": "Understanding tone in Twitter posts will be increasingly important as more and more communication moves online. One of the most difficult, yet important tones to detect is sarcasm. In the past, LSTM and transformer architecture models have been used to tackle this problem. We attempt to expand upon this research, implementing LSTM, GRU, and transformer models, and exploring new methods to classify sarcasm in Twitter posts. Among these, the most successful were transformer models, most notably BERT. While we attempted a few other models described in this paper, our most successful model was an ensemble of transformer models including BERT, RoBERTa, XLNet, RoBERTa-large, and ALBERT. This research was performed in conjunction with the sarcasm detection shared task section in the Second Workshop on Figurative Language Processing, co-located with ACL 2020.",
  "keywords": [
    "ensemble",
    "transformer",
    "lstm gru",
    "roberta",
    "processing",
    "transformer models",
    "language",
    "bert",
    "model",
    "a transformer approach",
    "albert",
    "we",
    "lstm",
    "gru",
    "the past lstm"
  ],
  "url": "https://aclanthology.org/2020.figlang-1.37/",
  "provenance": {
    "collected_at": "2025-06-05 07:56:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}