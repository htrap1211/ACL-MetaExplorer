{
  "id": "2023.acl-long.606",
  "title": "Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations",
  "authors": [
    "Hu, Dou  and\nBao, Yinan  and\nWei, Lingwei  and\nZhou, Wei  and\nHu, Songlin"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the modelâ€™s context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT.",
  "keywords": [
    "conversations",
    "we",
    "lstm",
    "generalized and robust representations",
    "training",
    "it",
    "a sequence-based sacl-lstm",
    "learning",
    "sequence",
    "cat",
    "manner",
    "sacl-lstm",
    "model",
    "class",
    "a supervised manner"
  ],
  "url": "https://aclanthology.org/2023.acl-long.606/",
  "provenance": {
    "collected_at": "2025-06-05 09:43:45",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}