{
  "id": "2022.in2writing-1.5",
  "title": "Language Models as Context-sensitive Word Search Engines",
  "authors": [
    "Wiegmann, Matti  and\nV{\\\"o}lske, Michael  and\nStein, Benno  and\nPotthast, Martin"
  ],
  "year": "2022",
  "venue": "Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)",
  "abstract": "Context-sensitive word search engines are writing assistants that support word choice, phrasing, and idiomatic language use by indexing large-scale n-gram collections and implementing a wildcard search. However, search results become unreliable with increasing context size (e.g., n>=5), when observations become sparse. This paper proposes two strategies for word search with larger n, based on masked and conditional language modeling. We build such search engines using BERT and BART and compare their capabilities in answering English context queries with those of the n-gram-based word search engine Netspeak. Our proposed strategies score within 5 percentage points MRR of n-gram collections while answering up to 5 times as many queries.",
  "keywords": [
    "our proposed strategies",
    "language",
    "n",
    "bert",
    "two strategies",
    "strategies",
    "queries",
    "language models",
    "modeling",
    "capabilities",
    "word",
    "we",
    "as many queries",
    "their capabilities",
    "that"
  ],
  "url": "https://aclanthology.org/2022.in2writing-1.5/",
  "provenance": {
    "collected_at": "2025-06-05 08:42:56",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}