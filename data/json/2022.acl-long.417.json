{
  "id": "2022.acl-long.417",
  "title": "RNG}-{KBQA}: Generation Augmented Iterative Ranking for Knowledge Base Question Answering",
  "authors": [
    "Ye, Xi  and\nYavuz, Semih  and\nHashimoto, Kazuma  and\nZhou, Yingbo  and\nXiong, Caiming"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GrailQA and WebQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.",
  "keywords": [
    "the knowledge graph",
    "zero-shot generalization",
    "question",
    "form",
    "we",
    "rng - kbqa generation",
    "graph",
    "kbqa",
    "shot",
    "a generation model",
    "it",
    "a rank-and-generate approach",
    "a tailored generation model",
    "addition rng-kbqa outperforms",
    "i"
  ],
  "url": "https://aclanthology.org/2022.acl-long.417/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}