{
  "id": "2022.acl-long.360",
  "title": "PRIMERA}: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
  "authors": [
    "Xiao, Wen  and\nBeltagy, Iz  and\nCarenini, Giuseppe  and\nCohan, Arman"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.",
  "keywords": [
    "fine-tuning labeled data primera",
    "tuning",
    "transformers",
    "processing",
    "model",
    "it",
    "objective",
    "efficient",
    "efficient encoder-decoder transformers",
    "encoder",
    "decoder",
    "information",
    "fine",
    "multi-document summarization",
    "summarization"
  ],
  "url": "https://aclanthology.org/2022.acl-long.360/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}