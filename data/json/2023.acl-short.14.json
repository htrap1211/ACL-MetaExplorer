{
  "id": "2023.acl-short.14",
  "title": "Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times",
  "authors": [
    "de Varda, Andrea  and\nMarelli, Marco"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters.",
  "keywords": [
    "early semantic integration",
    "semantic",
    "we",
    "neural language models",
    "current",
    "parameter",
    "neural",
    "it",
    "word",
    "processing",
    "early",
    "larger transformer-based language models",
    "work",
    "late",
    "transformer"
  ],
  "url": "https://aclanthology.org/2023.acl-short.14/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}