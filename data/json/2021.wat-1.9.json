{
  "id": "2021.wat-1.9",
  "title": "Rakuten{'}s Participation in {WAT} 2021: Examining the Effectiveness of Pre-trained Models for Multilingual and Multimodal Machine Translation",
  "authors": [
    "Susanto, Raymond Hendy  and\nWang, Dongzhe  and\nYadav, Sunil  and\nJain, Mausam  and\nHtun, Ohnmar"
  ],
  "year": "2021",
  "venue": "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
  "abstract": "This paper introduces our neural machine translation systemsâ€™ participation in the WAT 2021 shared translation tasks (team ID: sakura). We participated in the (i) NICT-SAP, (ii) Japanese-English multimodal translation, (iii) Multilingual Indic, and (iv) Myanmar-English translation tasks. Multilingual approaches such as mBART (Liu et al., 2020) are capable of pre-training a complete, multilingual sequence-to-sequence model through denoising objectives, making it a great starting point for building multilingual translation systems. Our main focus in this work is to investigate the effectiveness of multilingual finetuning on such a multilingual language model on various translation tasks, including low-resource, multimodal, and mixed-domain translation. We further explore a multimodal approach based on universal visual representation (Zhang et al., 2019) and compare its performance against a unimodal approach based on mBART alone.",
  "keywords": [
    "work",
    "objectives",
    "language",
    "various translation tasks",
    "neural",
    "denoising objectives",
    "model",
    "machine",
    "it",
    "we",
    "sequence",
    "visual",
    "pre",
    "et al",
    "multilingual translation systems"
  ],
  "url": "https://aclanthology.org/2021.wat-1.9/",
  "provenance": {
    "collected_at": "2025-06-05 08:23:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}