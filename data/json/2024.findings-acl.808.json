{
  "id": "2024.findings-acl.808",
  "title": "OOP}: Object-Oriented Programming Evaluation Benchmark for Large Language Models",
  "authors": [
    "Wang, Shuai  and\nDing, Liang  and\nShen, Li  and\nLuo, Yong  and\nDu, Bo  and\nTao, Dacheng"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favour of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k metric. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field. Our benchmark and scripts will be publicly released at GitHub.",
  "keywords": [
    "code",
    "field",
    "k",
    "we",
    "current",
    "current evaluation frameworks",
    "all advanced llms",
    "chatgpt",
    "llms",
    "object",
    "metric",
    "oop code generation",
    "our evaluation",
    "general",
    "language"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.808/",
  "provenance": {
    "collected_at": "2025-06-05 10:59:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}