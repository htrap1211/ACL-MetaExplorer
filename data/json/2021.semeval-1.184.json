{
  "id": "2021.semeval-1.184",
  "title": "YNU}-{HPCC} at {S}em{E}val-2021 Task 10: Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing",
  "authors": [
    "Yu, Zhewen  and\nWang, Jin  and\nZhang, Xuejie"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "Data sharing restrictions are common in NLP datasets. The purpose of this task is to develop a model trained in a source domain to make predictions for a target domain with related domain data. To address the issue, the organizers provided the models that fine-tuned a large number of source domain data on pre-trained models and the dev data for participants. But the source domain data was not distributed. This paper describes the provided model to the NER (Name entity recognition) task and the ways to develop the model. As a little data provided, pre-trained models are suitable to solve the cross-domain tasks. The models fine-tuned by large number of another domain could be effective in new domain because the task had no change.",
  "keywords": [
    "transformer",
    "ner",
    "cross",
    "processing",
    "em",
    "nlp",
    "model",
    "nlp datasets",
    "semantic",
    "pre",
    "entity",
    "that",
    "a large number",
    "new domain",
    "a target domain"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.184/",
  "provenance": {
    "collected_at": "2025-06-05 08:22:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}