{
  "id": "2022.acl-long.364",
  "title": "Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data",
  "authors": [
    "Leong, Colin  and\nWhitenack, Daniel"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the worldâ€™s languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch. Preprocessing and training code will be uploaded tohttps://github.com/sil-ai/phone-it-in.",
  "keywords": [
    "code",
    "we",
    "up to 6 f1-score",
    "training",
    "it",
    "systematic inequalities",
    "nlp tasks",
    "ner",
    "language model",
    "text",
    "language models",
    "-",
    "inequalities",
    "work",
    "language"
  ],
  "url": "https://aclanthology.org/2022.acl-long.364/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:13",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}