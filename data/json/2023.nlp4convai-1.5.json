{
  "id": "2023.nlp4convai-1.5",
  "title": "LLM}-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
  "authors": [
    "Lin, Yen-Ting  and\nChen, Yun-Nung"
  ],
  "year": "2023",
  "venue": "Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023)",
  "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",
  "keywords": [
    "multiple llm prompts",
    "decoding strategies",
    "accurate evaluation results",
    "conversations",
    "its effectiveness efficiency",
    "efficiency",
    "we",
    "llm",
    "existing evaluation methods",
    "open-domain conversation systems",
    "suitable llms",
    "unified",
    "a unified evaluation schema",
    "analysis",
    "llms"
  ],
  "url": "https://aclanthology.org/2023.nlp4convai-1.5/",
  "provenance": {
    "collected_at": "2025-06-05 10:25:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}