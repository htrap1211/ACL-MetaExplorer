{
  "id": "2022.acl-long.296",
  "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
  "authors": [
    "Eberle, Oliver  and\nBrandl, Stephanie  and\nPilot, Jonas  and\nS{\\o}gaard, Anders"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pre-trained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts. Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lower-entropy attention vectors are more faithful.",
  "keywords": [
    "transformer",
    "tuning",
    "transformer models",
    "human attention",
    "language",
    "nlp",
    "extraction",
    "human",
    "large-scale pre-trained language models",
    "vectors",
    "similar attention patterns",
    "self",
    "self-attention",
    "large-scale pre-trained self-attention",
    "fine"
  ],
  "url": "https://aclanthology.org/2022.acl-long.296/",
  "provenance": {
    "collected_at": "2025-06-05 08:28:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}