{
  "id": "2022.acl-long.479",
  "title": "What Makes Reading Comprehension Questions Difficult?",
  "authors": [
    "Sugawara, Saku  and\nNangia, Nikita  and\nWarstadt, Alex  and\nBowman, Samuel"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "For a natural language understanding benchmark to be useful in research, it has to consist of examples that are diverse and difficult enough to discriminate among current and near-future state-of-the-art systems. However, we do not yet know how best to select text sources to collect a variety of challenging examples. In this study, we crowdsource multiple-choice reading comprehension questions for passages taken from seven qualitatively distinct sources, analyzing what attributes of passages contribute to the difficulty and question types of the collected examples. To our surprise, we find that passage source, length, and readability measures do not significantly affect question difficulty. Through our manual annotation of seven reasoning types, we observe several trends between passage sources and reasoning types, e.g., logical reasoning is more often required in questions written for technical passages. These results suggest that when creating a new benchmark dataset, selecting a diverse set of passages can help ensure a diverse range of question types, but that passage difficulty need not be a priority.",
  "keywords": [
    "variety",
    "question",
    "we",
    "current",
    "natural",
    "it",
    "a variety",
    "text",
    "language",
    "question types",
    "measures",
    "state",
    "e g logical reasoning",
    "diverse",
    "a priority"
  ],
  "url": "https://aclanthology.org/2022.acl-long.479/",
  "provenance": {
    "collected_at": "2025-06-05 08:30:45",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}