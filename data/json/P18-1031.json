{
  "id": "P18-1031",
  "title": "Universal Language Model Fine-tuning for Text Classification",
  "authors": [
    "Howard, Jeremy  and\nRuder, Sebastian"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
  "keywords": [
    "six text classification tasks",
    "code",
    "tuning",
    "language",
    "nlp",
    "model",
    "text",
    "it",
    "fine",
    "we",
    "learning",
    "transfer",
    "universal language model",
    "a language model",
    "training"
  ],
  "url": "https://aclanthology.org/P18-1031/",
  "provenance": {
    "collected_at": "2025-06-05 00:09:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}