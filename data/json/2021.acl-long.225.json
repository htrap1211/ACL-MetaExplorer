{
  "id": "2021.acl-long.225",
  "title": "Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning",
  "authors": [
    "Park, Cheonbok  and\nTae, Yunwon  and\nKim, TaeHee  and\nYang, Soyoung  and\nKhan, Mohammad Azam  and\nPark, Lucy  and\nChoo, Jaegul"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines.",
  "keywords": [
    "general",
    "knowledge",
    "a transfer learning-based approach",
    "translation",
    "neural",
    "meta-learning unsupervised machine translation",
    "machine",
    "model",
    "it",
    "supervised machine translation",
    "bleu",
    "domain-general knowledge",
    "we",
    "learning",
    "transfer"
  ],
  "url": "https://aclanthology.org/2021.acl-long.225/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}