{
  "id": "2023.acl-industry.55",
  "title": "Weighted Contrastive Learning With False Negative Control to Help Long-tailed Product Classification",
  "authors": [
    "Wang, Tianqi  and\nChen, Lei  and\nZhu, Xiaodan  and\nLee, Younghun  and\nGao, Jing"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
  "abstract": "Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be poor due to scarce supervision, causing many issues in real-life applications. To address IC task’s long-tail issue, K-positive contrastive loss (KCL) is proposed on image classification task and can be applied on the IC task when using text-based contrastive learning, e.g., SimCSE. However, one shortcoming of using KCL has been neglected in previous research: false negative (FN) instances may harm the KCL’s representation learning. To address the FN issue in the KCL, we proposed to re-weight the positive pairs in the KCL loss with a regularization that the sum of weights should be constrained to K+1 as close as possible. After controlling FN instances with the proposed method, IC performance has been further improved and is superior to other LT-addressing methods.",
  "keywords": [
    "a regularization",
    "ic",
    "image classification task",
    "k",
    "we",
    "classification",
    "loss",
    "learning",
    "text",
    "sum",
    "regularization",
    "performances",
    "supervision",
    "the proposed method",
    "fn instances"
  ],
  "url": "https://aclanthology.org/2023.acl-industry.55/",
  "provenance": {
    "collected_at": "2025-06-05 09:52:45",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}