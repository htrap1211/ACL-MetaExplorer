{
  "id": "2021.acl-long.111",
  "title": "Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention",
  "authors": [
    "Ahmad, Wasi  and\nBai, Xiao  and\nLee, Soomin  and\nChang, Kai-Wei"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Natural language processing techniques have demonstrated promising results in keyphrase generation. However, one of the major challenges inneuralkeyphrase generation is processing long documents using deep neural networks. Generally, documents are truncated before given as inputs to neural networks. Consequently, the models may miss essential points conveyed in the target document. To overcome this limitation, we proposeSEG-Net, a neural keyphrase generation model that is composed of two major components, (1) a selector that selects the salient sentences in a document and (2) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses Transformer, a self-attentive architecture, as the basic building block with a novellayer-wisecoverage attention to summarize most of the points discussed in the document. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin.",
  "keywords": [
    "deep",
    "keyphrase generation",
    "deep neural networks",
    "layer",
    "we",
    "seven keyphrase generation",
    "neural",
    "natural",
    "net",
    "self",
    "scientific and web documents",
    "2 an extractor-generator",
    "scientific",
    "generative",
    "processing"
  ],
  "url": "https://aclanthology.org/2021.acl-long.111/",
  "provenance": {
    "collected_at": "2025-06-05 08:00:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}