{
  "id": "2020.bea-1.13",
  "title": "An empirical investigation of neural methods for content scoring of science explanations",
  "authors": [
    "Riordan, Brian  and\nBichler, Sarah  and\nBradford, Allison  and\nKing Chen, Jennifer  and\nWiley, Korah  and\nGerard, Libby  and\nC. Linn, Marcia"
  ],
  "year": "2020",
  "venue": "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications",
  "abstract": "With the widespread adoption of the Next Generation Science Standards (NGSS), science teachers and online learning environments face the challenge of evaluating studentsâ€™ integration of different dimensions of science learning. Recent advances in representation learning in natural language processing have proven effective across many natural language processing tasks, but a rigorous evaluation of the relative merits of these methods for scoring complex constructed response formative assessments has not previously been carried out. We present a detailed empirical investigation of feature-based, recurrent neural network, and pre-trained transformer models on scoring content in real-world formative assessment data. We demonstrate that recent neural methods can rival or exceed the performance of feature-based methods. We also provide evidence that different classes of neural models take advantage of different learning cues, and pre-trained transformer models may be more robust to spurious, dataset-specific learning cues, better reflecting scoring rubrics.",
  "keywords": [
    "a rigorous evaluation",
    "we",
    "neural",
    "natural",
    "science",
    "learning",
    "natural language processing",
    "recurrent",
    "feature-based recurrent neural network",
    "science explanations",
    "processing",
    "pre-trained transformer models",
    "transformer",
    "generation",
    "language"
  ],
  "url": "https://aclanthology.org/2020.bea-1.13/",
  "provenance": {
    "collected_at": "2025-06-05 07:54:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}