{
  "id": "2023.semeval-1.82",
  "title": "T}.{M}. Scanlon at {S}em{E}val-2023 Task 4: Leveraging Pretrained Language Models for Human Value Argument Mining with Contrastive Learning",
  "authors": [
    "Molazadeh Oskuee, Milad  and\nRahgouy, Mostafa  and\nBabaei Giglou, Hamed  and\nD Seals, Cheryl"
  ],
  "year": "2023",
  "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
  "abstract": "Human values are of great concern to social sciences which refer to when people have different beliefs and priorities of what is generally worth striving for and how to do so. This paper presents an approach for human value argument mining using contrastive learning to leverage the isotropy of language models. We fine-tuned DeBERTa-Large in a multi-label classification fashion and achieved an F1 score of 49% for the task, resulting in a rank of 11. Our proposed model provides a valuable tool for analyzing arguments related to human values and highlights the significance of leveraging the isotropy of large language models for identifying human values.",
  "keywords": [
    "priorities",
    "language",
    "social sciences",
    "deberta",
    "an f1 score",
    "model",
    "human",
    "pretrained language models",
    "different beliefs",
    "language models",
    "large language models",
    "sciences",
    "a multi-label classification fashion",
    "beliefs",
    "we"
  ],
  "url": "https://aclanthology.org/2023.semeval-1.82/",
  "provenance": {
    "collected_at": "2025-06-05 10:27:45",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}