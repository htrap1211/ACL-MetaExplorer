{
  "id": "2024.acl-long.845",
  "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
  "authors": [
    "{\\\"U}st{\\\"u}n, Ahmet  and\nAryabumi, Viraat  and\nYong, Zheng  and\nKo, Wei-Yin  and\nD{'}souza, Daniel  and\nOnilude, Gbemileke  and\nBhandari, Neel  and\nSingh, Shivalika  and\nOoi, Hui-Lee  and\nKayid, Amr  and\nVargus, Freddie  and\nBlunsom, Phil  and\nLongpre, Shayne  and\nMuennighoff, Niklas  and\nFadaee, Marzieh  and\nKreutzer, Julia  and\nHooker, Sara"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages —— including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.",
  "keywords": [
    "instruction",
    "work",
    "generative",
    "bias",
    "language",
    "model",
    "extensive new evaluation suites",
    "it",
    "class",
    "human",
    "llms",
    "large language models llms",
    "rich",
    "we",
    "eval"
  ],
  "url": "https://aclanthology.org/2024.acl-long.845/",
  "provenance": {
    "collected_at": "2025-06-05 10:45:56",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}