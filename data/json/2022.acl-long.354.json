{
  "id": "2022.acl-long.354",
  "title": "F}a{VIQ}: {FA}ct Verification from Information-seeking Questions",
  "authors": [
    "Park, Jungsoo  and\nMin, Sewon  and\nKang, Jaewoo  and\nZettlemoyer, Luke  and\nHajishirzi, Hannaneh"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Despite significant interest in developing general purpose fact checking models, it is challenging to construct a large-scale fact verification dataset with realistic real-world claims. Existing claims are either authored by crowdworkers, thereby introducing subtle biases thatare difficult to control for, or manually verified by professional fact checkers, causing them to be expensive and limited in scale. In this paper, we construct a large-scale challenging fact verification dataset called FAVIQ, consisting of 188k claims derived from an existing corpus of ambiguous information-seeking questions. The ambiguities in the questions enable automatically constructing true and false claims that reflect user confusions (e.g., the year of the movie being filmed vs. being released). Claims in FAVIQ are verified to be natural, contain little lexical bias, and require a complete understanding of the evidence for verification. Our experiments show that the state-of-the-art models are far from solving our new task. Moreover, training on our data helps in professional fact-checking, outperforming models trained on the widely used dataset FEVER or in-domain data by up to 17% absolute. Altogether, our data will serve as a challenging benchmark for natural language understanding and support future progress in professional fact checking.",
  "keywords": [
    "bias",
    "we",
    "ambiguities",
    "training",
    "the ambiguities",
    "the movie",
    "natural",
    "it",
    "little lexical bias",
    "information",
    "general purpose fact",
    "subtle biases",
    "biases",
    "general",
    "language"
  ],
  "url": "https://aclanthology.org/2022.acl-long.354/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}