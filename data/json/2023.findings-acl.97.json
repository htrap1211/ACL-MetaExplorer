{
  "id": "2023.findings-acl.97",
  "title": "SERENGETI}: Massively Multilingual Language Models for {A}frica",
  "authors": [
    "Adebara, Ife  and\nElmadany, AbdelRahim  and\nAbdul-Mageed, Muhammad  and\nAlcoba Inciarte, Alcides"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research. Anonymous link",
  "keywords": [
    "language",
    "natural",
    "model",
    "varieties",
    "zero-shot settings",
    "information",
    "generalizable",
    "us",
    "existing language models",
    "we",
    "valuable generalizable linguistic information",
    "language varieties",
    "shot",
    "massively multilingual language model",
    "that"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.97/",
  "provenance": {
    "collected_at": "2025-06-05 09:54:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}