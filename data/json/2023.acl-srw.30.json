{
  "id": "2023.acl-srw.30",
  "title": "Probing for Hyperbole in Pre-Trained Language Models",
  "authors": [
    "Schneidermann, Nina  and\nHershcovich, Daniel  and\nPedersen, Bolette"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
  "abstract": "Hyperbole is a common figure of speech, which is under-explored in NLP research. In this study, we conduct edge and minimal description length (MDL) probing experiments on three pre-trained language models (PLMs) in an attempt to explore the extent to which hyperbolic information is encoded in these models. We use both word-in-context and sentence-level representations as model inputs as a basis for comparison. We also annotate 63 hyperbole sentences from the HYPO dataset according to an operational taxonomy to conduct an error analysis to explore the encoding of different hyperbole categories. Our results show that hyperbole is to a limited extent encoded in PLMs, and mostly in the final layers. They also indicate that hyperbolic information may be better encoded by the sentence-level representations, which, due to the pragmatic nature of hyperbole, may therefore provide a more accurate and informative representation in PLMs. Finally, the inter-annotator agreement for our annotations, a Cohenâ€™s Kappa of 0.339, suggest that the taxonomy categories may not be intuitive and need revision or simplification.",
  "keywords": [
    "pre-trained language models hyperbole",
    "we",
    "edge",
    "different hyperbole categories",
    "information",
    "word",
    "analysis",
    "categories",
    "the taxonomy categories",
    "nlp research",
    "language",
    "nlp",
    "model",
    "pre",
    "attempt"
  ],
  "url": "https://aclanthology.org/2023.acl-srw.30/",
  "provenance": {
    "collected_at": "2025-06-05 09:51:44",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}