{
  "id": "2024.acl-long.322",
  "title": "Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning",
  "authors": [
    "Kim, Yeachan  and\nKim, Junho  and\nLee, SangKeun"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Parameter-efficient fine-tuning (PEFT) has enabled the efficient optimization of cumbersome language models in real-world settings. However, as datasets in such environments often contain noisy labels that adversely affect performance, PEFT methods are inevitably exposed to noisy labels. Despite this challenge, the adaptability of PEFT to noisy environments remains underexplored. To bridge this gap, we investigate various PEFT methods under noisy labels. Interestingly, our findings reveal that PEFT has difficulty in memorizing noisy labels due to its inherently limited capacity, resulting in robustness. However, we also find that such limited capacity simultaneously makes PEFT more vulnerable to interference of noisy labels, impeding the learning of clean samples. To address this issue, we propose Clean Routing (CleaR), a novel routing-based PEFT approach that adaptively activates PEFT modules. In CleaR, PEFT modules are preferentially exposed to clean data while bypassing the noisy ones, thereby minimizing the noisy influence. To verify the efficacy of CleaR, we perform extensive experiments on diverse configurations of noisy labels. The results convincingly demonstrate that CleaR leads to substantially improved performance in noisy environments",
  "keywords": [
    "efficient",
    "generalized",
    "we",
    "parameter",
    "cumbersome language models",
    "learning",
    "vulnerable",
    "tuning",
    "the efficient optimization",
    "fine",
    "language",
    "optimization",
    "interference",
    "approach",
    "the learning"
  ],
  "url": "https://aclanthology.org/2024.acl-long.322/",
  "provenance": {
    "collected_at": "2025-06-05 10:38:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}