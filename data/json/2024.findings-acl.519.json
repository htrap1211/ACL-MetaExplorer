{
  "id": "2024.findings-acl.519",
  "title": "Teaching Large Language Models an Unseen Language on the Fly",
  "authors": [
    "Zhang, Chen  and\nLiu, Xiao  and\nLin, Jiuheng  and\nFeng, Yansong"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones, for which there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce DiPMT++, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and 5K parallel sentences only, DiPMT++ significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. We also validate the effectiveness of our framework on Kalamang, another unseen language. Furthermore, we demonstrate the practical utility of DiPMT++ in aiding humans in translating completely unseen languages, which could contribute to the preservation of linguistic diversity.",
  "keywords": [
    "existing large language models",
    "32 bleu",
    "language",
    "translation",
    "large language models",
    "bleu",
    "question",
    "gpt-4",
    "zhuang-to-chinese translation",
    "16 bleu",
    "no llms",
    "we",
    "learning",
    "parameter",
    "training"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.519/",
  "provenance": {
    "collected_at": "2025-06-05 10:55:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}