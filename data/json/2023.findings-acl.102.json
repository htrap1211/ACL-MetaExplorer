{
  "id": "2023.findings-acl.102",
  "title": "General-to-Specific Transfer Labeling for Domain Adaptable Keyphrase Generation",
  "authors": [
    "Meng, Rui  and\nWang, Tong  and\nYuan, Xingdi  and\nZhou, Yingbo  and\nHe, Daqing"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Training keyphrase generation (KPG) models require a large amount of annotated data, which can be prohibitively expensive and often limited to specific domains. In this study, we first demonstrate that large distribution shifts among different domains severely hinder the transferability of KPG models. We then propose a three-stage pipeline, which gradually guides KPG modelsâ€™ learning focus from general syntactical features to domain-related semantics, in a data-efficient manner. With domain-general phrase pre-training, we pre-train Sequence-to-Sequence models with generic phrase annotations that are widely available on the web, which enables the models to generate phrases in a wide range of domains. The resulting model is then applied in the Transfer Labeling stage to produce domain-specific pseudo keyphrases, which help adapt models to a new domain. Finally, we fine-tune the model with limited data with true labels to fully adapt it to the target domain. Our experiment results show that the proposed process can produce good quality keyphrases in new domains and achieve consistent improvements after adaptation with limited in-domain annotated data. All code and datasets are available athttps://github.com/memray/OpenNMT-kpg-release.",
  "keywords": [
    "code",
    "efficient",
    "general-to-specific transfer labeling",
    "we",
    "training",
    "semantics",
    "it",
    "sequence",
    "transfer",
    "manner",
    "generic phrase annotations",
    "generic",
    "fine",
    "domain-general phrase",
    "domain-related semantics"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.102/",
  "provenance": {
    "collected_at": "2025-06-05 09:54:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}