{
  "id": "2023.clinicalnlp-1.15",
  "title": "Training Models on Oversampled Data and a Novel Multi-class Annotation Scheme for Dementia Detection",
  "authors": [
    "Abdelhalim, Nadine  and\nAbdelhalim, Ingy  and\nBatista-Navarro, Riza"
  ],
  "year": "2023",
  "venue": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
  "abstract": "This work introduces a novel three-class annotation scheme for text-based dementia classification in patients, based on their recorded visit interactions. Multiple models were developed utilising BERT, RoBERTa and DistilBERT. Two approaches were employed to improve the representation of dementia samples: oversampling the underrepresented data points in the original Pitt dataset and combining the Pitt with the Holland and Kempler datasets. The DistilBERT models trained on either an oversampled Pitt dataset or the combined dataset performed best in classifying the dementia class. Specifically, the model trained on the oversampled Pitt dataset and the one trained on the combined dataset obtained state-of-the-art performance with 98.8% overall accuracy and 98.6% macro-averaged F1-score, respectively. The modelsâ€™ outputs were manually inspected through saliency highlighting, using Local Interpretable Model-agnostic Explanations (LIME), to provide a better understanding of its predictions.",
  "keywords": [
    "work",
    "roberta",
    "98 8 overall accuracy",
    "bert",
    "the distilbert models",
    "text",
    "class",
    "patients",
    "model",
    "saliency",
    "bert roberta",
    "distilbert",
    "accuracy",
    "training",
    "classification"
  ],
  "url": "https://aclanthology.org/2023.clinicalnlp-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 10:23:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}