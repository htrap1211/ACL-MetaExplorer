{
  "id": "2020.ngt-1.26",
  "title": "E}dinburgh{'}s Submissions to the 2020 Machine Translation Efficiency Task",
  "authors": [
    "Bogoychev, Nikolay  and\nGrundkiewicz, Roman  and\nAji, Alham Fikri  and\nBehnke, Maximiliana  and\nHeafield, Kenneth  and\nKashyap, Sidharth  and\nFarsarakis, Emmanouil-Ioannis  and\nChudyk, Mateusz"
  ],
  "year": "2020",
  "venue": "Proceedings of the Fourth Workshop on Neural Generation and Translation",
  "abstract": "We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On GPUs, we used 16-bit floating-point tensor cores. On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.",
  "keywords": [
    "embeddings",
    "variety",
    "generation",
    "neural",
    "model",
    "machine",
    "efficiency",
    "we",
    "a variety",
    "recurrent",
    "time",
    "core",
    "log",
    "training",
    "translation"
  ],
  "url": "https://aclanthology.org/2020.ngt-1.26/",
  "provenance": {
    "collected_at": "2025-06-05 07:57:15",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}