{
  "id": "2022.acl-long.509",
  "title": "Adversarial Authorship Attribution for Deobfuscation",
  "authors": [
    "Zhai, Wanyue  and\nRusert, Jonathan  and\nShafiq, Zubair  and\nSrinivasan, Padmini"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent advances in natural language processing have enabled powerful privacy-invasive authorship attribution. To counter authorship attribution, researchers have proposed a variety of rule-based and learning-based text obfuscation approaches. However, existing authorship obfuscation approaches do not consider the adversarial threat model. Specifically, they are not evaluated against adversarially trained authorship attributors that are aware of potential obfuscation. To fill this gap, we investigate the problem of adversarial authorship attribution for deobfuscation. We show that adversarially trained authorship attributors are able to degrade the effectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate the effectiveness of adversarial training when the attributor makes incorrect assumptions about whether and which obfuscator was used. While there is a a clear degradation in attribution accuracy, it is noteworthy that this degradation is still at or above the attribution accuracy of the attributor that is not adversarially trained at all. Our results motivate the need to develop authorship obfuscation approaches that are resistant to deobfuscation.",
  "keywords": [
    "variety",
    "processing",
    "language",
    "natural",
    "model",
    "attribution accuracy",
    "text",
    "it",
    "we",
    "learning",
    "a variety",
    "natural language processing",
    "the attribution accuracy",
    "training",
    "accuracy"
  ],
  "url": "https://aclanthology.org/2022.acl-long.509/",
  "provenance": {
    "collected_at": "2025-06-05 08:31:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}