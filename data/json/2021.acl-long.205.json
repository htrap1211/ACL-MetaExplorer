{
  "id": "2021.acl-long.205",
  "title": "N-ary Constituent Tree Parsing with Recursive Semi-{M}arkov Model",
  "authors": [
    "Xin, Xin  and\nLi, Jinlong  and\nTan, Zeqi"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction. The limitation is that the hidden nodes break the sibling relations of the n-ary node’s children. Consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored. To solve this limitation, we propose a novel graph-based framework, which is called “recursive semi-Markov model”. The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent. In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation. Through experiments, the proposed framework obtains the F1 of 95.92% and 92.50% on the datasets of PTB and CTB 5.1 respectively. Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1.",
  "keywords": [
    "parsing",
    "the f1",
    "whose average f1",
    "the dependencies",
    "we",
    "graph",
    "dependencies",
    "sequence",
    "manner",
    "processing",
    "model",
    "graph-based constituent parsing",
    "modeling",
    "this manner",
    "pre"
  ],
  "url": "https://aclanthology.org/2021.acl-long.205/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}