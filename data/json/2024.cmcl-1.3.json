{
  "id": "2024.cmcl-1.3",
  "title": "Locally Biased Transformers Better Align with Human Reading Times",
  "authors": [
    "De Varda, Andrea  and\nMarelli, Marco"
  ],
  "year": "2024",
  "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
  "abstract": "Recent psycholinguistic theories emphasize the interdependence between linguistic expectations and memory limitations in human language processing. We modify the self-attention mechanism of a transformer model to simulate a lossy context representation, biasing the modelâ€™s predictions to give additional weight to the local linguistic context. We show that surprisal estimates from our locally-biased model generally provide a better fit to human psychometric data, underscoring the sensitivity of the human parser to local linguistic information.",
  "keywords": [
    "transformer",
    "transformers",
    "align",
    "processing",
    "language",
    "model",
    "human",
    "fit",
    "theories",
    "recent psycholinguistic theories",
    "self",
    "information",
    "the self-attention mechanism",
    "attention",
    "we"
  ],
  "url": "https://aclanthology.org/2024.cmcl-1.3/",
  "provenance": {
    "collected_at": "2025-06-05 11:05:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}