{
  "id": "2022.findings-acl.271",
  "title": "M}eta{W}eighting: Learning to Weight Tasks in Multi-Task Learning",
  "authors": [
    "Mao, Yuren  and\nWang, Zekai  and\nLiu, Weiwei  and\nLin, Xuemin  and\nXie, Pengtao"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it. However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization loss. It degenerates MTLâ€™s performance. To address this issue, the present paper proposes a novel task weighting algorithm, which automatically weights the tasks via a learning-to-learn paradigm, referred to as MetaWeighting. Extensive experiments are conducted to validate the superiority of our proposed method in multi-task text classification.",
  "keywords": [
    "it",
    "generalization loss",
    "text",
    "-",
    "multi-task text classification",
    "loss",
    "generalization",
    "learning",
    "training",
    "classification",
    "existing task weighting methods",
    "mtl s performance",
    "this issue",
    "eta",
    "multi"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.271/",
  "provenance": {
    "collected_at": "2025-06-05 08:38:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}