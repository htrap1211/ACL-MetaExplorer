{
  "id": "2024.conda-1.1",
  "title": "Evaluating {C}hinese Large Language Models on Discipline Knowledge Acquisition via Memorization and Robustness Assessment",
  "authors": [
    "Liu, Chuang  and\nJin, Renren  and\nSteedman, Mark  and\nXiong, Deyi"
  ],
  "year": "2024",
  "venue": "Proceedings of the 1st Workshop on Data Contamination (CONDA)",
  "abstract": "Chinese LLMs demonstrate impressive performance on NLP tasks, particularly on discipline knowledge benchmarks, with some results approaching those of GPT-4. Previous research has viewed these advancements as potential outcomes of data contamination or leakage, prompting efforts to create new detection methods and address evaluation issues in LLM benchmarks. However, there has been a lack of comprehensive assessment of the evolution of Chinese LLMs. To address this gap, this paper offers a thorough investigation of Chinese LLMs on discipline knowledge evaluation, delving into the advancements of various LLMs, including a group of related models and others. Specifically, we have conducted six assessments ranging from knowledge memorization to comprehension for robustness, encompassing tasks like predicting incomplete questions and options, identifying behaviors by the contaminational fine-tuning, and answering rephrased questions. Experimental findings indicate a positive correlation between the release time of LLMs and their memorization capabilities, but they struggle with variations in original question-options pairs. Additionally, our findings suggest that question descriptions have a more significant impact on LLMsâ€™ performance.",
  "keywords": [
    "question",
    "we",
    "llm",
    "gpt-4 previous research",
    "llm benchmarks",
    "llms performance",
    "nlp tasks",
    "address evaluation issues",
    "llms",
    "tuning",
    "chinese llms",
    "their memorization capabilities",
    "fine",
    "discipline knowledge evaluation delving",
    "various llms"
  ],
  "url": "https://aclanthology.org/2024.conda-1.1/",
  "provenance": {
    "collected_at": "2025-06-05 11:05:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}