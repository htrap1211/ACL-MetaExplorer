{
  "id": "2024.findings-acl.644",
  "title": "Data Contamination Calibration for Black-box {LLM}s",
  "authors": [
    "Ye, Wentao  and\nHu, Jiaqi  and\nLi, Liyao  and\nWang, Haobo  and\nChen, Gang  and\nZhao, Junbo"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) — from machine learning community — by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.",
  "keywords": [
    "series",
    "we",
    "current",
    "llm",
    "training",
    "machine learning community",
    "llms",
    "i",
    "black-box llm",
    "work",
    "a series",
    "language",
    "machine",
    "large language models",
    "black"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.644/",
  "provenance": {
    "collected_at": "2025-06-05 10:57:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}