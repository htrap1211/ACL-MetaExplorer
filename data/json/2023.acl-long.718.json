{
  "id": "2023.acl-long.718",
  "title": "U}ni{S}umm and {S}umm{Z}oo: Unified Model and Diverse Benchmark for Few-Shot Summarization",
  "authors": [
    "Chen, Yulong  and\nLiu, Yang  and\nXu, Ruochen  and\nYang, Ziyi  and\nZhu, Chenguang  and\nZeng, Michael  and\nZhang, Yue"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose UniSumm, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark SummZoo. It consists of 8 summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that UniSumm outperforms strong baselines by a large margin across all sub-tasks in SummZoo under both automatic and human evaluations and achieves comparable results in human evaluation compared with a GPT-3.5 model.",
  "keywords": [
    "multiple summarization tasks",
    "end",
    "few-shot summarization systems",
    "summarization",
    "we",
    "current",
    "shot",
    "training",
    "human evaluation",
    "it",
    "gpt-3",
    "unified",
    "u",
    "analysis",
    "few-shot summarizers"
  ],
  "url": "https://aclanthology.org/2023.acl-long.718/",
  "provenance": {
    "collected_at": "2025-06-05 09:45:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}