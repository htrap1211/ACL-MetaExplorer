{
  "id": "2021.acl-short.84",
  "title": "Domain-Adaptive Pretraining Methods for Dialogue Understanding",
  "authors": [
    "Wu, Han  and\nXu, Kun  and\nSong, Linfeng  and\nJin, Lifeng  and\nZhang, Haisong  and\nSong, Linqi"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "Language models like BERT and SpanBERT pretrained on open-domain data have obtained impressive gains on various NLP tasks. In this paper, we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domain-adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances.",
  "keywords": [
    "objectives",
    "language",
    "domain-adaptive pretraining objectives",
    "nlp",
    "bert",
    "objective",
    "language models",
    "proper objectives",
    "particular three objectives",
    "various nlp tasks",
    "we",
    "spanbert",
    "dialogue",
    "a novel objective",
    "pretraining"
  ],
  "url": "https://aclanthology.org/2021.acl-short.84/",
  "provenance": {
    "collected_at": "2025-06-05 08:08:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}