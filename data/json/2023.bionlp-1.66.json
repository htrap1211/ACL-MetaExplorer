{
  "id": "2023.bionlp-1.66",
  "title": "LHS}712{EE} at {B}io{L}ay{S}umm 2023: Using {BART} and {LED} to summarize biomedical research articles",
  "authors": [
    "Liu, Quancheng  and\nRen, Xiheng  and\nVydiswaran, V.G.Vinod"
  ],
  "year": "2023",
  "venue": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
  "abstract": "As part of our participation in BioLaySumm 2023, we explored the use of large language models (LLMs) to automatically generate concise and readable summaries of biomedical research articles. We utilized pre-trained LLMs to fine-tune our summarization models on two provided datasets, and adapt them to the shared task within the constraints of training time and computational power. Our final models achieved very high relevance and factuality scores on the test set, and ranked among the top five models in the overall performance.",
  "keywords": [
    "summaries",
    "language",
    "large language models",
    "pre-trained llms",
    "our summarization models",
    "umm",
    "fine",
    "summarization",
    "we",
    "pre",
    "concise and readable summaries",
    "time",
    "training",
    "llms",
    "fine-tune"
  ],
  "url": "https://aclanthology.org/2023.bionlp-1.66/",
  "provenance": {
    "collected_at": "2025-06-05 10:23:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}