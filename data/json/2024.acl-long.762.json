{
  "id": "2024.acl-long.762",
  "title": "Language Models are {H}omer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
  "authors": [
    "Bhardwaj, Rishabh  and\nDo, Duc Anh  and\nPoria, Soujanya"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We propose RESTA to perform LLM realignment towards safety, which gets compromised due to downstream task fine-tuning. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in parameter-efficient and full fine-tuning, respectively, while maintaining most of the modelâ€™s performance on the task. We release the source codes at: https://github.com/declare-lab/resta.",
  "keywords": [
    "code",
    "problem-solving capabilities",
    "efficient",
    "we",
    "generalizability",
    "llm",
    "parameter",
    "instruction",
    "it",
    "the generalizability",
    "llm realignment",
    "task fine-tuning resta",
    "core",
    "tuning",
    "vector"
  ],
  "url": "https://aclanthology.org/2024.acl-long.762/",
  "provenance": {
    "collected_at": "2025-06-05 10:44:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}