{
  "id": "2024.findings-acl.537",
  "title": "T}ext{B}ind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
  "authors": [
    "Li, Huayang  and\nLi, Siheng  and\nCai, Deng  and\nWang, Longyue  and\nLiu, Lemao  and\nWatanabe, Taro  and\nYang, Yujiu  and\nShi, Shuming"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering LLMs with multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. To accommodate interleaved image-text inputs and outputs, we devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models. Extensive quantitative and qualitative experiments demonstrate that MIM trained on TextBind achieves remarkable generation capability in multimodal conversations compared to recent baselines.",
  "keywords": [
    "ext",
    "conversations",
    "field",
    "we",
    "generalizability",
    "instruction",
    "multimodal conversations",
    "natural",
    "the field",
    "it",
    "decoder",
    "decoder models",
    "remarkable generation capability",
    "a language model",
    "llms"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.537/",
  "provenance": {
    "collected_at": "2025-06-05 10:55:56",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}