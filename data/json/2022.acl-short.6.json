{
  "id": "2022.acl-short.6",
  "title": "Disentangled Knowledge Transfer for {OOD} Intent Discovery with Unified Contrastive Learning",
  "authors": [
    "Mou, Yutao  and\nHe, Keqing  and\nWu, Yanan  and\nZeng, Zhiyuan  and\nXu, Hong  and\nJiang, Huixing  and\nWu, Wei  and\nXu, Weiran"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Discovering Out-of-Domain(OOD) intents is essential for developing new skills in a task-oriented dialogue system. The key challenge is how to transfer prior IND knowledge to OOD clustering. Different from existing work based on shared intent representation, we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning framework. We aim to bridge the gap between IND pre-training and OOD clustering. Experiments and analysis on two benchmark datasets show the effectiveness of our method.",
  "keywords": [
    "work",
    "knowledge",
    "unified",
    "we",
    "learning",
    "transfer",
    "dialogue",
    "unified contrastive learning",
    "pre",
    "a task-oriented dialogue system",
    "analysis",
    "training",
    "the effectiveness",
    "multi",
    "ind"
  ],
  "url": "https://aclanthology.org/2022.acl-short.6/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:31",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}