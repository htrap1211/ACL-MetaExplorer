{
  "id": "2023.findings-acl.866",
  "title": "On the Difference of {BERT}-style and {CLIP}-style Text Encoders",
  "authors": [
    "Chen, Zhihong  and\nChen, Guiming  and\nDiao, Shizhe  and\nWan, Xiang  and\nWang, Benyou"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Masked language modeling (MLM) has been one of the most popular pretraining recipes in natural language processing,e.g., BERT, one of the representative models. Recently, contrastive language-image pretraining (CLIP) has also attracted attention, especially its vision models that achieve excellent performance on a broad range of vision tasks. However, few studies are dedicated to studying the text encoders learned by CLIP. In this paper, we analyze the difference betweenBERT-styleandCLIP-styletext encoders from three experiments: (i) general text understanding, (ii) vision-centric text understanding, and (iii) text-to-image generation. Experimental analyses show that although CLIP-style text encoders underperform BERT-style ones for general text understanding tasks, they are equipped with a unique ability,i.e.,synesthesia, for the cross-modal association, which is more similar to the senses of humans.",
  "keywords": [
    "cross",
    "few studies",
    "general",
    "processing",
    "bert-style ones",
    "i",
    "language",
    "generation",
    "natural",
    "bert -style",
    "bert",
    "studies",
    "text",
    "modeling",
    "betweenbert"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.866/",
  "provenance": {
    "collected_at": "2025-06-05 10:20:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}