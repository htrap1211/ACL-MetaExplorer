{
  "id": "2023.acl-long.695",
  "title": "Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks",
  "authors": [
    "Tang, Yun  and\nSun, Anna  and\nInaguma, Hirofumi  and\nChen, Xinyue  and\nDong, Ning  and\nMa, Xutai  and\nTomasello, Paden  and\nPino, Juan"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-to-text tasks. The new method leverages AED’s strength in non-monotonic sequence to sequence learning while retaining Transducer’s streaming property. In the proposed framework, Transducer and AED share the same speech encoder. The predictor in Transducer is replaced by the decoder in the AED model, and the outputs of the decoder are conditioned on the speech inputs instead of outputs from an unconditioned language model. The proposed solution ensures that the model is optimized by covering all possible read/write scenarios and creates a matched environment for streaming applications. We evaluate the proposed approach on the MuST-C dataset and the findings demonstrate that TAED performs significantly better than Transducer for offline automatic speech recognition (ASR) and speech-to-text translation (ST) tasks. In the streaming case, TAED outperforms Transducer in the ASR task and one ST direction while comparable results are achieved in another translation direction.",
  "keywords": [
    "the decoder",
    "we",
    "translation",
    "based encoder-decoder taed",
    "based encoder-decoder aed",
    "another translation direction",
    "decoder",
    "sequence",
    "attention based encoder-decoder modeling",
    "learning",
    "text",
    "an unconditioned language model",
    "language",
    "model",
    "modeling"
  ],
  "url": "https://aclanthology.org/2023.acl-long.695/",
  "provenance": {
    "collected_at": "2025-06-05 09:45:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}