{
  "id": "2023.bea-1.49",
  "title": "Rating Short {L}2 Essays on the {CEFR} Scale with {GPT}-4",
  "authors": [
    "Yancey, Kevin P.  and\nLaflair, Geoffrey  and\nVerardi, Anthony  and\nBurstein, Jill"
  ],
  "year": "2023",
  "venue": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
  "abstract": "Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes language assessments. While automated scoring approaches are mature and have been around for decades, human scoring is still considered the gold standard, despite its high costs and well-known issues such as human rater fatigue and bias. The recent introduction of large language models (LLMs) brings new opportunities for automated scoring. In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-takerâ€™s first language (L1).",
  "keywords": [
    "opportunities",
    "we",
    "gpt-3",
    "gpt -4 essay scoring",
    "new opportunities",
    "llms",
    "l2 english learners",
    "gpt-4",
    "proficiency",
    "language",
    "learners",
    "human",
    "large language models",
    "gpt",
    "evaluation"
  ],
  "url": "https://aclanthology.org/2023.bea-1.49/",
  "provenance": {
    "collected_at": "2025-06-05 10:21:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}