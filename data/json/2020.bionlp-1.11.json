{
  "id": "2020.bionlp-1.11",
  "title": "Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity",
  "authors": [
    "Wang, Yuxia  and\nLiu, Fei  and\nVerspoor, Karin  and\nBaldwin, Timothy"
  ],
  "year": "2020",
  "venue": "Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing",
  "abstract": "In this paper, we apply pre-trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low-resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS 1 demonstrate substantial improvements, validating the utility of the proposed methods.",
  "keywords": [
    "clinical semantic textual similarity",
    "domain-specific fine-tuning",
    "tuning",
    "general",
    "language",
    "pre-trained language models",
    "bert",
    "model",
    "the general domain",
    "strategies",
    "fine",
    "semantic",
    "we",
    "pre",
    "that"
  ],
  "url": "https://aclanthology.org/2020.bionlp-1.11/",
  "provenance": {
    "collected_at": "2025-06-05 07:54:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}