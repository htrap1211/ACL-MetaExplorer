{
  "id": "2023.acl-short.1",
  "title": "Should you marginalize over possible tokenizations?",
  "authors": [
    "Chirkova, Nadezhda  and\nKruszewski, Germ{\\'a}n  and\nRozen, Jos  and\nDymetman, Marc"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words.",
  "keywords": [
    "the marginal probabilities",
    "end",
    "we",
    "tokenizations",
    "it",
    "token",
    "sequence",
    "log",
    "autoregressive language models lms",
    "language",
    "model",
    "possible tokenizations",
    "all tokenizations",
    "us",
    "probabilities"
  ],
  "url": "https://aclanthology.org/2023.acl-short.1/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}