{
  "id": "2024.acl-long.197",
  "title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
  "authors": [
    "Zhang, Wenqi  and\nShen, Yongliang  and\nWu, Linjuan  and\nPeng, Qiuying  and\nWang, Jun  and\nZhuang, Yueting  and\nLu, Weiming"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM’s response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM’s intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.",
  "keywords": [
    "feedback",
    "their discrepancies",
    "large language model",
    "series",
    "stubborn biases",
    "we",
    "llm s intrinsic reflection",
    "llm",
    "translation",
    "different llms",
    "it",
    "generality",
    "extensive attention",
    "self",
    "llm s response"
  ],
  "url": "https://aclanthology.org/2024.acl-long.197/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:59",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}