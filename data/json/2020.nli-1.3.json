{
  "id": "2020.nli-1.3",
  "title": "Examination and Extension of Strategies for Improving Personalized Language Modeling via Interpolation",
  "authors": [
    "Shao, Liqun  and\nMantravadi, Sahitya  and\nManzini, Tom  and\nBuendia, Alejandro  and\nKnoertzer, Manon  and\nSrinivasan, Soundar  and\nQuirk, Chris"
  ],
  "year": "2020",
  "venue": "Proceedings of the First Workshop on Natural Language Interfaces",
  "abstract": "In this paper, we detail novel strategies for interpolating personalized language models and methods to handle out-of-vocabulary (OOV) tokens to improve personalized language models. Using publicly available data from Reddit, we demonstrate improvements in offline metrics at the user level by interpolating a global LSTM-based authoring model with a user-personalized n-gram model. By optimizing this approach with a back-off to uniform OOV penalty and the interpolation coefficient, we observe that over 80% of users receive a lift in perplexity, with an average of 5.4% in perplexity lift per user. In doing this research we extend previous work in building NLIs and improve the robustness of metrics for downstream tasks.",
  "keywords": [
    "work",
    "language",
    "offline metrics",
    "model",
    "personalized language models",
    "metrics",
    "strategies",
    "perplexity",
    "novel strategies",
    "we",
    "lstm",
    "perplexity lift",
    "coefficient",
    "the interpolation coefficient",
    "users"
  ],
  "url": "https://aclanthology.org/2020.nli-1.3/",
  "provenance": {
    "collected_at": "2025-06-05 07:57:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}