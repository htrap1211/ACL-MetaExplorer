{
  "id": "2021.semeval-1.126",
  "title": "S}koltech{NLP} at {S}em{E}val-2021 Task 5: Leveraging Sentence-level Pre-training for Toxic Span Detection",
  "authors": [
    "Dale, David  and\nMarkov, Igor  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "This work describes the participation of the Skoltech NLP group team (Sk) in the Toxic Spans Detection task at SemEval-2021. The goal of the task is to identify the most toxic fragments of a given sentence, which is a binary sequence tagging problem. We show that fine-tuning a RoBERTa model for this problem is a strong baseline. This baseline can be further improved by pre-training the RoBERTa model on a large dataset labeled for toxicity at the sentence level. While our solution scored among the top 20% participating models, it is only 2 points below the best result. This suggests the viability of our approach.",
  "keywords": [
    "work",
    "roberta",
    "nlp",
    "the roberta model",
    "model",
    "it",
    "-",
    "tagging",
    "fine",
    "we",
    "sequence",
    "pre",
    "koltech nlp",
    "training",
    "a roberta model"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.126/",
  "provenance": {
    "collected_at": "2025-06-05 08:21:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}