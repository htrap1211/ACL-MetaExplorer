{
  "id": "2024.acl-short.21",
  "title": "Fine-Tuning Pre-Trained Language Models with Gaze Supervision",
  "authors": [
    "Deng, Shuwen  and\nPrasse, Paul  and\nReich, David  and\nScheffer, Tobias  and\nJ{\\\"a}ger, Lena"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Human gaze data provide cognitive information that reflect human language comprehension and has been effectively integrated into a variety of natural language processing (NLP) tasks, demonstrating improved performance over corresponding plain text-based models. In this work, we propose to integrate a gaze module into pre-trained language models (LMs) at the fine-tuning stage to improve their capabilities to learn representations that are grounded in human language processing. This is done by extending the conventional purely text-based fine-tuning objective with an auxiliary loss to exploit cognitive signals. The gaze module is only included during training, retaining compatibility with existing pre-trained LM-based pipelines. We evaluate the proposed approach using two distinct pre-trained LMs on the GLUE benchmark and observe that the proposed model improves performance compared to both standard fine-tuning and traditional text augmentation baselines.",
  "keywords": [
    "variety",
    "work",
    "fine-tuning pre-trained language models",
    "tuning",
    "processing",
    "language",
    "nlp",
    "natural",
    "model",
    "text",
    "human",
    "objective",
    "loss",
    "information",
    "capabilities"
  ],
  "url": "https://aclanthology.org/2024.acl-short.21/",
  "provenance": {
    "collected_at": "2025-06-05 10:46:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}