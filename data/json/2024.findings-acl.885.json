{
  "id": "2024.findings-acl.885",
  "title": "B}loom{VQA}: Assessing Hierarchical Multi-modal Comprehension",
  "authors": [
    "Gong, Yunye  and\nShrestha, Robik  and\nClaypoole, Jared  and\nCogswell, Michael  and\nRay, Arijit  and\nKanan, Christopher  and\nDivakaran, Ajay"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloomâ€™s Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demonstrates improved accuracy over all comprehension levels and also shows a tendency of bypassing visual inputs especially for higher-level tasks. Current models also show consistency patterns misaligned with human comprehension in various scenarios, demonstrating the need for improvement based on theoretically-grounded criteria. The dataset can be accessed at https://huggingface.co/datasets/ygong/BloomVQA.",
  "keywords": [
    "improved accuracy",
    "comprehensive evaluation",
    "gpt-4v",
    "earlier models gpt-4v",
    "we",
    "graph",
    "current",
    "picture stories",
    "large vision-language models",
    "vqa accuracy",
    "visual",
    "analysis",
    "bloomvqa",
    "drop",
    "stories"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.885/",
  "provenance": {
    "collected_at": "2025-06-05 11:00:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}