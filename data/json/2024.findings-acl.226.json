{
  "id": "2024.findings-acl.226",
  "title": "Data-Centric Explainable Debiasing for Improving Fairness in Pre-trained Language Models",
  "authors": [
    "Li, Yingji  and\nDu, Mengnan  and\nSong, Rui  and\nWang, Xin  and\nWang, Ying"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Human-like social bias of pre-trained language models (PLMs) on downstream tasks have attracted increasing attention. The potential flaws in the training data are the main factor that causes unfairness in PLMs. Existing data-centric debiasing strategies mainly leverage explicit bias words (defined as sensitive attribute words specific to demographic groups) for counterfactual data augmentation to balance the training data. However, they lack consideration of implicit bias words potentially associated with explicit bias words in complex distribution data, which indirectly harms the fairness of PLMs. To this end, we propose a **Data**-Centric **Debias**ing method (named Data-Debias), which uses an explainability method to search for implicit bias words to assist in debiasing PLMs. Specifically, we compute the feature attributions of all tokens using the Integrated Gradients method, and then treat the tokens that have a large impact on the modelâ€™s decision as implicit bias words. To make the search results more precise, we iteratively train a biased model to amplify the bias with each iteration. Finally, we use the implicit bias words searched in the last iteration to assist in debiasing PLMs. Extensive experimental results on multiple PLMs debiasing on three different classification tasks demonstrate that Data-Debias achieves state-of-the-art debiasing performance and strong generalization while maintaining predictive abilities.",
  "keywords": [
    "the bias",
    "bias",
    "three different classification tasks",
    "end",
    "the implicit bias words",
    "implicit bias words",
    "data-debias",
    "we",
    "training",
    "classification",
    "ing",
    "a biased model",
    "increasing attention",
    "pre-trained language models",
    "pre-trained language models plms"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.226/",
  "provenance": {
    "collected_at": "2025-06-05 10:51:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}