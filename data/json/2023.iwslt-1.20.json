{
  "id": "2023.iwslt-1.20",
  "title": "CMU}{'}s {IWSLT} 2023 Simultaneous Speech Translation System",
  "authors": [
    "Yan, Brian  and\nShi, Jiatong  and\nMaiti, Soumi  and\nChen, William  and\nLi, Xinjian  and\nPeng, Yifan  and\nArora, Siddhant  and\nWatanabe, Shinji"
  ],
  "year": "2023",
  "venue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
  "abstract": "This paper describes CMUâ€™s submission to the IWSLT 2023 simultaneous speech translation shared task for translating English speech to both German text and speech in a streaming fashion. We first build offline speech-to-text (ST) models using the joint CTC/attention framework. These models also use WavLM front-end features and mBART decoder initialization. We adapt our offline ST models for simultaneous speech-to-text translation (SST) by 1) incrementally encoding chunks of input speech, re-computing encoder states for each new chunk and 2) incrementally decoding output text, pruning beam search hypotheses to 1-best after processing each chunk. We then build text-to-speech (TTS) models using the VITS framework and achieve simultaneous speech-to-speech translation (SS2ST) by cascading our SST and TTS models.",
  "keywords": [
    "end",
    "-computing encoder states",
    "text",
    "encoder",
    "decoder",
    "attention",
    "we",
    "mbart decoder initialization",
    "translation",
    "speech",
    "states",
    "fashion",
    "sst",
    "new",
    "task"
  ],
  "url": "https://aclanthology.org/2023.iwslt-1.20/",
  "provenance": {
    "collected_at": "2025-06-05 10:24:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}