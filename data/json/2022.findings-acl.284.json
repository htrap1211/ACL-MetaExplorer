{
  "id": "2022.findings-acl.284",
  "title": "Improving the Adversarial Robustness of {NLP} Models by Information Bottleneck",
  "authors": [
    "Zhang, Cenyuan  and\nZhou, Xiang  and\nWan, Yixin  and\nZheng, Xiaoqing  and\nChang, Kai-Wei  and\nHsieh, Cho-Jui"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Existing studies have demonstrated that adversarial examples can be directly attributed to the presence of non-robust features, which are highly predictive, but can be easily manipulated by adversaries to fool NLP models. In this study, we explore the feasibility of capturing task-specific robust features, while eliminating the non-robust ones by using the information bottleneck theory. Through extensive experiments, we show that the models trained with our information bottleneck-based method are able to achieve a significant improvement in robust accuracy, exceeding performances of all the previously reported defense methods while suffering almost no performance drop in clean accuracy on SST-2, AGNEWS and IMDB datasets.",
  "keywords": [
    "robust accuracy exceeding performances",
    "clean accuracy",
    "nlp",
    "studies",
    "adversaries",
    "drop",
    "information",
    "we",
    "nlp models",
    "existing studies",
    "accuracy",
    "theory",
    "performances",
    "the presence",
    "task"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.284/",
  "provenance": {
    "collected_at": "2025-06-05 08:38:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}