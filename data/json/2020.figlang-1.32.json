{
  "id": "2020.figlang-1.32",
  "title": "Go Figure! Multi-task transformer-based architecture for metaphor detection using idioms: {ETS} team in 2020 metaphor shared task",
  "authors": [
    "Chen, Xianyang  and\nLeong, Chee Wee (Ben)  and\nFlor, Michael  and\nBeigman Klebanov, Beata"
  ],
  "year": "2020",
  "venue": "Proceedings of the Second Workshop on Figurative Language Processing",
  "abstract": "This paper describes the ETS entry to the 2020 Metaphor Detection shared task. Our contribution consists of a sequence of experiments using BERT, starting with a baseline, strengthening it by spell-correcting the TOEFL corpus, followed by a multi-task learning setting, where one of the tasks is the token-level metaphor classification as per the shared task, while the other is meant to provide additional training that we hypothesized to be relevant to the main task. In one case, out-of-domain data manually annotated for metaphor is used for the auxiliary task; in the other case, in-domain data automatically annotated for idioms is used for the auxiliary task. Both multi-task experiments yield promising results.",
  "keywords": [
    "transformer",
    "the token-level metaphor classification",
    "bert",
    "it",
    "token",
    "we",
    "sequence",
    "learning",
    "multi-task transformer-based architecture",
    "classification",
    "training",
    "that",
    "promising",
    "idioms",
    "a baseline"
  ],
  "url": "https://aclanthology.org/2020.figlang-1.32/",
  "provenance": {
    "collected_at": "2025-06-05 07:55:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}