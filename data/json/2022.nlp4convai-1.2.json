{
  "id": "2022.nlp4convai-1.2",
  "title": "Are Pre-trained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection",
  "authors": [
    "Zhang, Jianguo  and\nHashimoto, Kazuma  and\nWan, Yao  and\nLiu, Zhiwei  and\nLiu, Ye  and\nXiong, Caiming  and\nYu, Philip"
  ],
  "year": "2022",
  "venue": "Proceedings of the 4th Workshop on NLP for Conversational AI",
  "abstract": "Pre-trained Transformer-based models were reported to be robust in intent classification. In this work, we first point out the importance of in-domain out-of-scope detection in few-shot intent recognition tasks and then illustrate the vulnerability of pre-trained Transformer-based models against samples that are in-domain but out-of-scope (ID-OOS). We construct two new datasets, and empirically show that pre-trained models do not perform well on both ID-OOS examples and general out-of-scope examples, especially on fine-grained few-shot intent detection tasks.",
  "keywords": [
    "work",
    "transformer",
    "a missing ingredient",
    "intent classification",
    "pre-trained transformer-based models",
    "transformers",
    "general",
    "oos",
    "we",
    "ingredient",
    "evaluation",
    "pre",
    "few-shot intent recognition tasks",
    "vulnerability",
    "shot"
  ],
  "url": "https://aclanthology.org/2022.nlp4convai-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 08:45:13",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}