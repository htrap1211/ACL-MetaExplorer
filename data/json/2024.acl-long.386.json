{
  "id": "2024.acl-long.386",
  "title": "C}hi{M}ed-{GPT}: A {C}hinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences",
  "authors": [
    "Tian, Yuanhe  and\nGan, Ruyi  and\nSong, Yan  and\nZhang, Jiaxing  and\nZhang, Yongdong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate ChiMed-GPTâ€™s superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting ChiMed-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain.",
  "keywords": [
    "extraction",
    "question",
    "we",
    "dialogue",
    "current",
    "llm",
    "training",
    "better alignment",
    "the discrepancies",
    "rlhf evaluations",
    "natural",
    "rlhf",
    "it",
    "patients",
    "dialogue generation"
  ],
  "url": "https://aclanthology.org/2024.acl-long.386/",
  "provenance": {
    "collected_at": "2025-06-05 10:39:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}