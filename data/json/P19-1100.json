{
  "id": "P19-1100",
  "title": "Searching for Effective Neural Extractive Summarization: What Works and What{'}s Next",
  "authors": [
    "Zhong, Ming  and\nLiu, Pengfei  and\nWang, Danqing  and\nQiu, Xipeng  and\nHuang, Xuanjing"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "The recent years have seen remarkable success in the use of deep neural networks on text summarization. However, there is no clear understanding of why they perform so well, or how they might be improved. In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Besides, we find an effective way to improve the current framework and achieve the state-of-the-art result on CNN/DailyMail by a large margin based on our observations and analysis. Hopefully, our work could provide more hints for future research on extractive summarization.",
  "keywords": [
    "deep",
    "work",
    "knowledge",
    "neural",
    "cnn",
    "model",
    "text",
    "extractive summarization",
    "text summarization",
    "effective neural extractive summarization",
    "deep neural networks",
    "neural extractive summarization systems",
    "summarization",
    "we",
    "cnn dailymail"
  ],
  "url": "https://aclanthology.org/P19-1100/",
  "provenance": {
    "collected_at": "2025-06-05 00:32:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}