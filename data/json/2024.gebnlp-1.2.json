{
  "id": "2024.gebnlp-1.2",
  "title": "Do {PLM}s and Annotators Share the Same Gender Bias? Definition, Dataset, and Framework of Contextualized Gender Bias",
  "authors": [
    "Zhu, Shucheng  and\nDu, Bingjie  and\nZhao, Jishun  and\nLiu, Ying  and\nLiu, Pengyuan"
  ],
  "year": "2024",
  "venue": "Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
  "abstract": "Pre-trained language models (PLMs) have achieved success in various of natural language processing (NLP) tasks. However, PLMs also introduce some disquieting safety problems, such as gender bias. Gender bias is an extremely complex issue, because different individuals may hold disparate opinions on whether the same sentence expresses harmful bias, especially those seemingly neutral or positive. This paper first defines the concept of contextualized gender bias (CGB), which makes it easy to measure implicit gender bias in both PLMs and annotators. We then construct CGBDataset, which contains 20k natural sentences with gendered words, from Chinese news. Similar to the task of masked language models, gendered words are masked for PLMs and annotators to judge whether a male word or a female word is more suitable. Then, we introduce CGBFrame to measure the gender bias of annotators. By comparing the results measured by PLMs and annotators, we find that though there are differences on the choices made by PLMs and annotators, they show significant consistency in general.",
  "keywords": [
    "bias",
    "we",
    "natural",
    "masked language models",
    "it",
    "word",
    "processing",
    "contextualized gender bias cgb",
    "harmful bias",
    "gender bias gender bias",
    "implicit gender bias",
    "general",
    "language",
    "nlp",
    "disquieting"
  ],
  "url": "https://aclanthology.org/2024.gebnlp-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 11:06:01",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}