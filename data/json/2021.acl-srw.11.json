{
  "id": "2021.acl-srw.11",
  "title": "``{I}{'}ve Seen Things You People Wouldn{'}t Believe'': Hallucinating Entities in {G}uess{W}hat?!",
  "authors": [
    "Testoni, Alberto  and\nBernardi, Raffaella"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
  "abstract": "Natural language generation systems have witnessed important progress in the last years, but they are shown to generate tokens that are unrelated to the source input. This problem affects computational models in many NLP tasks, and it is particularly unpleasant in multimodal systems. In this work, we assess the rate of object hallucination in multimodal conversational agents playing the GuessWhat?! referential game. Better visual processing has been shown to mitigate this issue in image captioning; hence, we adapt to the GuessWhat?! task the best visual processing models at disposal, and propose two new models to play the Questioner agent. We show that the new models generate few hallucinations compared to other renowned models available in the literature. Moreover, their hallucinations are less severe (affect task-accuracy less) and are more human-like. We also analyse where hallucinations tend to occur more often through the dialogue: hallucinations are less frequent in earlier turns, cause a cascade hallucination effect, and are often preceded by negative answers, which have been shown to be harder to ground.",
  "keywords": [
    "rate",
    "the dialogue hallucinations",
    "we",
    "dialogue",
    "natural",
    "task-accuracy",
    "it",
    "many nlp tasks",
    "the questioner agent",
    "hallucinating entities",
    "visual",
    "processing",
    "i",
    "object",
    "accuracy"
  ],
  "url": "https://aclanthology.org/2021.acl-srw.11/",
  "provenance": {
    "collected_at": "2025-06-05 08:09:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}