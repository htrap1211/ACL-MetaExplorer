{
  "id": "2021.acl-long.248",
  "title": "Few-{NERD}: A Few-shot Named Entity Recognition Dataset",
  "authors": [
    "Ding, Ning  and\nXu, Guangwei  and\nChen, Yulin  and\nWang, Xiaobin  and\nHan, Xu  and\nXie, Pengjun  and\nZheng, Haitao  and\nLiu, Zhiyuan"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of the two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. The Few-NERD dataset and the baselines will be publicly available to facilitate the research on this problem.",
  "keywords": [
    "these strategies",
    "we",
    "current",
    "shot",
    "few-nerd",
    "analysis",
    "ner",
    "nerd",
    "the generalization capability",
    "strategies",
    "generalization",
    "hierarchy",
    "a hierarchy",
    "few- nerd",
    "the few-shot setting"
  ],
  "url": "https://aclanthology.org/2021.acl-long.248/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}