{
  "id": "2022.acl-demo.17",
  "title": "Dynatask: A Framework for Creating Dynamic {AI} Benchmark Tasks",
  "authors": [
    "Thrush, Tristan  and\nTirumala, Kushal  and\nGupta, Anmol  and\nBartolo, Max  and\nRodriguez, Pedro  and\nKane, Tariq  and\nGaviria Rojas, William  and\nMattson, Peter  and\nWilliams, Adina  and\nKiela, Douwe"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
  "abstract": "We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available athttps://dynabench.org/and the full library can be found athttps://github.com/facebookresearch/dynabench.",
  "keywords": [
    "knowledge",
    "custom nlp tasks",
    "nlp",
    "model",
    "human",
    "we",
    "evaluation",
    "that",
    "effort",
    "users",
    "the relevant web interfaces",
    "custom",
    "state",
    "the-art",
    "task"
  ],
  "url": "https://aclanthology.org/2022.acl-demo.17/",
  "provenance": {
    "collected_at": "2025-06-05 08:34:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}