{
  "id": "2022.repl4nlp-1.13",
  "title": "ANNA}: Enhanced Language Representation for Question Answering",
  "authors": [
    "Jun, Changwook  and\nJang, Hansol  and\nSim, Myoseop  and\nKim, Hyun  and\nChoi, Jooyoung  and\nMin, Kyungkoo  and\nBae, Kyunghoon"
  ],
  "year": "2022",
  "venue": "Proceedings of the 7th Workshop on Representation Learning for NLP",
  "abstract": "Pre-trained language models have brought significant improvements in performance in a variety of natural language processing tasks. Most existing models performing state-of-the-art results have shown their approaches in the separate perspectives of data processing, pre-training tasks, neural network modeling, or fine-tuning. In this paper, we demonstrate how the approaches affect performance individually, and that the language model performs the best results on a specific question answering task when those approaches are jointly considered in pre-training models. In particular, we propose an extended pre-training task, and a new neighbor-aware mechanism that attends neighboring tokens more to capture the richness of context for pre-training language modeling. Our best model achieves new state-of-the-art results of 95.7% F1 and 90.6% EM on SQuAD 1.1 and also outperforms existing pre-trained language models such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmark.",
  "keywords": [
    "variety",
    "roberta albert electra",
    "roberta",
    "natural language processing tasks",
    "question",
    "we",
    "training",
    "fine-tuning",
    "95 7 f1",
    "neural",
    "pre-trained language models",
    "natural",
    "albert",
    "existing pre-trained language models",
    "pre-training models"
  ],
  "url": "https://aclanthology.org/2022.repl4nlp-1.13/",
  "provenance": {
    "collected_at": "2025-06-05 08:45:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}