{
  "id": "P18-1150",
  "title": "A Graph-to-Sequence Model for {AMR}-to-Text Generation",
  "authors": [
    "Song, Linfeng  and\nZhang, Yue  and\nWang, Zhiguo  and\nGildea, Daniel"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.",
  "keywords": [
    "a novel lstm structure",
    "generation",
    "model",
    "neural",
    "text",
    "semantics",
    "a sequence lstm",
    "graph-level semantics",
    "non-local semantic information",
    "information",
    "semantic",
    "sequence",
    "we",
    "lstm",
    "graph"
  ],
  "url": "https://aclanthology.org/P18-1150/",
  "provenance": {
    "collected_at": "2025-06-05 00:13:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}