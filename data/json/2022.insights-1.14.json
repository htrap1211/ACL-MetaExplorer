{
  "id": "2022.insights-1.14",
  "title": "Clustering Examples in Multi-Dataset Benchmarks with Item Response Theory",
  "authors": [
    "Rodriguez, Pedro  and\nHtut, Phu Mon  and\nLalor, John  and\nSedoc, Jo{\\~a}o"
  ],
  "year": "2022",
  "venue": "Proceedings of the Third Workshop on Insights from Negative Results in NLP",
  "abstract": "In natural language processing, multi-dataset benchmarks for common tasks (e.g., SuperGLUE for natural language inference and MRQA for question answering) have risen in importance. Invariably, tasks and individual examples vary in difficulty. Recent analysis methods infer properties of examples such as difficulty. In particular, Item Response Theory (IRT) jointly infers example and model properties from the output of benchmark tasks (i.e., scores for each model-example pair). Therefore, it seems sensible that methods like IRT should be able to detect differences between datasets in a task. This work shows that current IRT models are not as good at identifying differences as we would expect, explain why this is difficult, and outline future directions that incorporate more (textual) signal from examples.",
  "keywords": [
    "work",
    "processing",
    "answering",
    "language",
    "natural",
    "model",
    "it",
    "question answering",
    "properties",
    "question",
    "clustering examples",
    "we",
    "current",
    "model properties",
    "analysis"
  ],
  "url": "https://aclanthology.org/2022.insights-1.14/",
  "provenance": {
    "collected_at": "2025-06-05 08:43:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}