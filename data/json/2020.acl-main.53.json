{
  "id": "2020.acl-main.53",
  "title": "Efficient Dialogue State Tracking by Selectively Overwriting Memory",
  "authors": [
    "Kim, Sungdong  and\nYang, Sohee  and\nKim, Gyuwan  and\nLee, Sang-Woo"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.",
  "keywords": [
    "dialogue state",
    "the decoder",
    "inefficient",
    "efficient",
    "we",
    "dialogue",
    "the dialogue state",
    "current",
    "training",
    "dialogue state tracking model",
    "it",
    "scalability and generalization issues",
    "decoder",
    "efficient dialogue state tracking",
    "the predefined ontology-based approaches"
  ],
  "url": "https://aclanthology.org/2020.acl-main.53/",
  "provenance": {
    "collected_at": "2025-06-05 07:42:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}