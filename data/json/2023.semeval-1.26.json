{
  "id": "2023.semeval-1.26",
  "title": "HULAT} at {S}em{E}val-2023 Task 10: Data Augmentation for Pre-trained Transformers Applied to the Detection of Sexism in Social Media",
  "authors": [
    "Segura-Bedmar, Isabel"
  ],
  "year": "2023",
  "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
  "abstract": "This paper describes our participation in SemEval-2023 Task 10, whose goal is the detection of sexism in social media. We explore some of the most popular transformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study different data augmentation techniques to increase the training dataset. During the development phase, our best results were obtained by using RoBERTa and data augmentation for tasks B and C. However, the use of synthetic data does not improve the results for task C. We participated in the three subtasks. Our approach still has much room for improvement, especially in the two fine-grained classifications. All our code is available in the repositoryhttps://github.com/isegura/hulat_edos.",
  "keywords": [
    "transformer",
    "code",
    "transformers",
    "roberta",
    "bert",
    "roberta and data augmentation",
    "bert distilbert roberta",
    "we",
    "distilbert",
    "pre",
    "the two fine-grained classifications",
    "training",
    "classifications",
    "pre-trained transformers",
    "hulat"
  ],
  "url": "https://aclanthology.org/2023.semeval-1.26/",
  "provenance": {
    "collected_at": "2025-06-05 10:26:59",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}