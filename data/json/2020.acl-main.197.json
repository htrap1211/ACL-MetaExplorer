{
  "id": "2020.acl-main.197",
  "title": "SMART}: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
  "authors": [
    "Jiang, Haoming  and\nHe, Pengcheng  and\nChen, Weizhu  and\nLiu, Xiaodong  and\nGao, Jianfeng  and\nZhao, Tuo"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.",
  "keywords": [
    "two important ingredients",
    "efficient",
    "pre-trained natural language models",
    "we",
    "training",
    "natural",
    "1 smoothness-inducing regularization",
    "it",
    "ingredients",
    "learning",
    "transfer",
    "natural language processing",
    "nlp tasks",
    "manner",
    "tuning"
  ],
  "url": "https://aclanthology.org/2020.acl-main.197/",
  "provenance": {
    "collected_at": "2025-06-05 07:44:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}