{
  "id": "2022.dravidianlangtech-1.14",
  "title": "GJG}@{T}amil{NLP}-{ACL}2022: Emotion Analysis and Classification in {T}amil using Transformers",
  "authors": [
    "Prasad, Janvi  and\nPrasad, Gaurang  and\nC, Gunavathi"
  ],
  "year": "2022",
  "venue": "Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages",
  "abstract": "This paper describes the systems built by our team for the “Emotion Analysis in Tamil” shared task at the Second Workshop on Speech and Language Technologies for Dravidian Languages at ACL 2022. There were two multi-class classification sub-tasks as a part of this shared task. The dataset for sub-task A contained 11 types of emotions while sub-task B was more fine-grained with 31 emotions. We fine-tuned an XLM-RoBERTa and DeBERTA base model for each sub-task. For sub-task A, the XLM-RoBERTa model achieved an accuracy of 0.46 and the DeBERTa model achieved an accuracy of 0.45. We had the best classification performance out of 11 teams for sub-task A. For sub-task B, the XLM-RoBERTa model’s accuracy was 0.33 and the DeBERTa model had an accuracy of 0.26. We ranked 2nd out of 7 teams for sub-task B.",
  "keywords": [
    "transformers",
    "roberta",
    "speech and language technologies",
    "language",
    "deberta",
    "nlp",
    "model",
    "class",
    "the deberta model",
    "-",
    "nlp - acl",
    "two multi-class classification sub",
    "an accuracy",
    "technologies",
    "b"
  ],
  "url": "https://aclanthology.org/2022.dravidianlangtech-1.14/",
  "provenance": {
    "collected_at": "2025-06-05 08:41:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}