{
  "id": "2024.acl-long.119",
  "title": "T}ool{S}word: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
  "authors": [
    "Ye, Junjie  and\nLi, Sixian  and\nLi, Guanyu  and\nHuang, Caishuang  and\nGao, Songyang  and\nWu, Yilong  and\nZhang, Qi  and\nGui, Tao  and\nHuang, Xuanjing"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we presentToolSword, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassingmaliciousqueriesandjailbreakattacksin the input stage,noisymisdirectionandriskycuesin the execution stage, andharmfulfeedbackanderrorconflictsin the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data will be released upon acceptance of the paper.",
  "keywords": [
    "feedback",
    "we",
    "current",
    "it",
    "queries",
    "word",
    "learning",
    "llms",
    "large language models llms",
    "language",
    "studies",
    "large language models",
    "harmful queries",
    "further studies",
    "approach"
  ],
  "url": "https://aclanthology.org/2024.acl-long.119/",
  "provenance": {
    "collected_at": "2025-06-05 10:35:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}