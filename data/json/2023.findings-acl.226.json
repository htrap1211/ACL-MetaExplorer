{
  "id": "2023.findings-acl.226",
  "title": "Non-Repeatable Experiments and Non-Reproducible Results: The Reproducibility Crisis in Human Evaluation in {NLP",
  "authors": [
    "Belz, Anya  and\nThomson, Craig  and\nReiter, Ehud  and\nMille, Simon"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Human evaluation is widely regarded as the litmus test of quality in NLP. A basic requirementof all evaluations, but in particular where they are used for meta-evaluation, is that they should support the same conclusions if repeated. However, the reproducibility of human evaluations is virtually never queried, let alone formally tested, in NLP which means that their repeatability and the reproducibility of their results is currently an open question. This focused contribution reports our review of human evaluation experiments reported in NLP papers over the past five years which we assessed in terms oftheir ability to be rerun. Overall, we estimatethat just 5% of human evaluations are repeatable in the sense that (i) there are no prohibitivebarriers to repetition, and (ii) sufficient information about experimental design is publicly available for rerunning them. Our estimate goesup to about 20% when author help is sought. We complement this investigation with a survey of results concerning the reproducibilityof human evaluations where those are repeatable in the first place. Here we find worryinglylow degrees of reproducibility, both in terms ofsimilarity of scores and of findings supportedby them. We summarise what insights can begleaned so far regarding how to make humanevaluations in NLP more repeatable and morereproducible.",
  "keywords": [
    "nlp papers",
    "question",
    "we",
    "human evaluation",
    "information",
    "sufficient",
    "nlp human evaluation",
    "i",
    "prohibitivebarriers",
    "all evaluations",
    "human evaluations",
    "human evaluation experiments",
    "no prohibitivebarriers",
    "nlp",
    "human"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.226/",
  "provenance": {
    "collected_at": "2025-06-05 09:56:23",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}