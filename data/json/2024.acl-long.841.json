{
  "id": "2024.acl-long.841",
  "title": "OLM}o: Accelerating the Science of Language Models",
  "authors": [
    "Groeneveld, Dirk  and\nBeltagy, Iz  and\nWalsh, Evan  and\nBhagia, Akshita  and\nKinney, Rodney  and\nTafjord, Oyvind  and\nJha, Ananya  and\nIvison, Hamish  and\nMagnusson, Ian  and\nWang, Yizhong  and\nArora, Shane  and\nAtkinson, David  and\nAuthur, Russell  and\nChandu, Khyathi  and\nCohan, Arman  and\nDumas, Jennifer  and\nElazar, Yanai  and\nGu, Yuling  and\nHessel, Jack  and\nKhot, Tushar  and\nMerrill, William  and\nMorrison, Jacob  and\nMuennighoff, Niklas  and\nNaik, Aakanksha  and\nNam, Crystal  and\nPeters, Matthew  and\nPyatkin, Valentina  and\nRavichander, Abhilasha  and\nSchwenk, Dustin  and\nShah, Saurabh  and\nSmith, William  and\nStrubell, Emma  and\nSubramani, Nishant  and\nWortsman, Mitchell  and\nDasigi, Pradeep  and\nLambert, Nathan  and\nRichardson, Kyle  and\nZettlemoyer, Luke  and\nDodge, Jesse  and\nLo, Kyle  and\nSoldaini, Luca  and\nSmith, Noah  and\nHajishirzi, Hannaneh"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
  "keywords": [
    "code",
    "proprietary interfaces",
    "the scientific study",
    "language",
    "end",
    "nlp",
    "the science",
    "model",
    "it",
    "both nlp research",
    "training and evaluation code",
    "language models",
    "science",
    "we",
    "scientific"
  ],
  "url": "https://aclanthology.org/2024.acl-long.841/",
  "provenance": {
    "collected_at": "2025-06-05 10:45:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}