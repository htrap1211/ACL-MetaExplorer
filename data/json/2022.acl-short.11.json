{
  "id": "2022.acl-short.11",
  "title": "Does {BERT} Know that the {IS}-A Relation Is Transitive?",
  "authors": [
    "Lin, Ruixi  and\nNg, Hwee Tou"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models. One such question is: can a black-box model make logically consistent predictions for transitive relations? Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context. However, to what extent BERT captures the transitive nature of some lexical relations is unclear. From a probing perspective, we examine WordNet word senses and the IS-A relation, which is a transitive relation. That is, for senses A, B, and C, A is-a B and B is-a C entail A is-a C. We aim to quantify how much BERT agrees with the transitive property of IS-A relations, via a minimalist probing setting. Our investigation reveals that BERTâ€™s predictions do not fully obey the transitivity property of the IS-A relation.",
  "keywords": [
    "deep",
    "question",
    "b",
    "semantic",
    "we",
    "bert s predictions",
    "natural",
    "c",
    "word",
    "learning",
    "a",
    "lexico-semantic clues",
    "processing",
    "bert",
    "pre-trained bert"
  ],
  "url": "https://aclanthology.org/2022.acl-short.11/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}