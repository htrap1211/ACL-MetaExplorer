{
  "id": "2020.acl-main.524",
  "title": "Multi-Cell Compositional {LSTM} for {NER} Domain Adaptation",
  "authors": [
    "Jia, Chen  and\nZhang, Yue"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Cross-domain NER is a challenging yet practical problem. Entity mentions can be highly different across domains. However, the correlations between entity types can be relatively more stable across domains. We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state. With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level. Theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer. Empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results.",
  "keywords": [
    "cross",
    "ner",
    "a series",
    "knowledge",
    "it",
    "series",
    "we",
    "learning",
    "transfer",
    "lstm",
    "multi-cell compositional lstm",
    "entity",
    "shot",
    "practical",
    "distinct"
  ],
  "url": "https://aclanthology.org/2020.acl-main.524/",
  "provenance": {
    "collected_at": "2025-06-05 07:49:13",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}