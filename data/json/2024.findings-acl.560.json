{
  "id": "2024.findings-acl.560",
  "title": "Critical Learning Periods: Leveraging Early Training Dynamics for Efficient Data Pruning",
  "authors": [
    "Chimoto, Everlyn Asiko  and\nGala, Jay  and\nAhia, Orevaoghene  and\nKreutzer, Julia  and\nBassett, Bruce A.  and\nHooker, Sara"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Neural Machine Translation models are extremely data and compute-hungry. However, not all datapoints contribute equally to model training and generalization. Data pruning to remove the low-value data points has the benefit of drastically reducing the compute budget without significantdrop in model performance. In this paper, we propose a new data pruning technique: CheckpointsAcross Time (CAT ), that leverages early model training dynamics to identify the most relevantdata points for model performance. We benchmark CAT against several data pruning techniquesincluding COMET-QE, LASER and LaBSE. We find that CAT outperforms the benchmarks onIndo-European languages on multiple test sets. When applied to English-German, English-Frenchand English-Swahili translation tasks, CAT achieves comparable performance to using the fulldataset, while pruning up to 50% of training data. We inspect the data points that CAT selectsand find that it tends to favour longer sentences and sentences with unique or rare words.",
  "keywords": [
    "efficient",
    "we",
    "fulldataset",
    "training",
    "translation",
    "neural",
    "it",
    "learning",
    "the fulldataset",
    "cat",
    "early",
    "generalization",
    "training and generalization data",
    "machine",
    "model"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.560/",
  "provenance": {
    "collected_at": "2025-06-05 10:56:15",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}