{
  "id": "2021.acl-long.367",
  "title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction",
  "authors": [
    "Xu, Lu  and\nChia, Yew Ken  and\nBing, Lidong"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term. Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation. Thus, it can make predictions with the semantics of whole spans, ensuring better sentiment consistency. To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly. Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our span-level approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions.",
  "keywords": [
    "end",
    "extraction",
    "the semantics",
    "efficiency",
    "we",
    "semantics",
    "it",
    "word",
    "analysis",
    "manner",
    "ate",
    "sentiment",
    "computational efficiency",
    "multiple words",
    "the aspect term extraction"
  ],
  "url": "https://aclanthology.org/2021.acl-long.367/",
  "provenance": {
    "collected_at": "2025-06-05 08:04:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}