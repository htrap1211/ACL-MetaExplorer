{
  "id": "2023.bea-1.21",
  "title": "Analyzing Bias in Large Language Model Solutions for Assisted Writing Feedback Tools: Lessons from the Feedback Prize Competition Series",
  "authors": [
    "Baffour, Perpetual  and\nSaxberg, Tor  and\nCrossley, Scott"
  ],
  "year": "2023",
  "venue": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
  "abstract": "This paper analyzes winning solutions from the Feedback Prize competition series hosted from 2021-2022. The competition sought to improve Assisted Writing Feedback Tools (AWFTs) by crowdsourcing Large Language Model (LLM) solutions for evaluating student writing. The winning models are freely available for incorporation into educational applications, but the models need to be assessed for performance and other factors. This study reports the performance accuracy of Feedback Prize-winning models based on demographic factors such as student race/ethnicity, economic disadvantage, and English Language Learner status. Two competitions are analyzed. The first, which focused on identifying discourse elements, demonstrated minimal bias based on studentsâ€™ demographic factors. However, the second competition, which aimed to predict discourse effectiveness, exhibited moderate bias.",
  "keywords": [
    "large language model solutions",
    "accuracy",
    "bias",
    "language",
    "moderate bias",
    "feedback",
    "model",
    "large language model",
    "series",
    "the performance accuracy",
    "learner",
    "english language learner status",
    "llm solutions",
    "llm",
    "minimal bias"
  ],
  "url": "https://aclanthology.org/2023.bea-1.21/",
  "provenance": {
    "collected_at": "2025-06-05 10:21:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}