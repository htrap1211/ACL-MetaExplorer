{
  "id": "2024.findings-acl.116",
  "title": "M}ol{TC}: Towards Molecular Relational Modeling In Language Models",
  "authors": [
    "Fang, Junfeng  and\nZhang, Shuai  and\nWu, Chang  and\nYang, Zhengyi  and\nLiu, Zhiyuan  and\nLi, Sihang  and\nWang, Kun  and\nDu, Wenjie  and\nWang, Xiang"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the issue of insufficient data exploitation, as it hinders the sharing of interaction mechanism learned across various datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for molecular interaction modeling following Chain-of-Thought (CoT) theory, termed MolTC, which effectively integrate graphical information of two molecules in pair. To train this integrated framework efficiently, we introduce a *multi-hierarchical CoT theory* to refine its training paradigm, and conduct a comprehensive *Molecular Interactive Instructions* dataset for the development of biochemical LLMs involving MRL.Our experiments,conducted across various datasets involving over 4,000,000 molecular pairs, exhibit the superiority of our method over current GNN and LLM-based baselines. Code is available at https://github.com/MangoKiller/MolTC.",
  "keywords": [
    "code",
    "efficient and effective mrl",
    "efficient",
    "chain",
    "biochemical llms",
    "we",
    "a multi-hierarchical cot theory",
    "current",
    "training",
    "repositories",
    "cot",
    "it",
    "a unified framework",
    "unified",
    "their vast knowledge repositories"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.116/",
  "provenance": {
    "collected_at": "2025-06-05 10:50:11",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}