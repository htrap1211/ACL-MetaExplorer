{
  "id": "2022.acl-long.19",
  "title": "Long-range Sequence Modeling with Predictable Sparse Attention",
  "authors": [
    "Zhuang, Yimeng  and\nZhang, Jing  and\nTu, Mei"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling, but it suffers from quadratic complexity in time and memory usage. Due to the sparsity of the attention matrix, much computation is redundant. Therefore, in this paper, we design an efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling. We provide a brand-new perspective for constructing sparse attention matrix, i.e. making the sparse attention matrix predictable. Two core sub-modules are: (1) A fast Fourier transform based hidden state cross module, which captures and poolsL2semantic combinations inùí™(LlogL)time complexity. (2) A sparse attention matrix estimation module, which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module. By reparameterization and gradient truncation, FSAT successfully learned the index of dominant elements. The overall complexity about the sequence length is reduced fromùí™(L2)toùí™(LlogL). Extensive experiments (natural language, vision, and math) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.",
  "keywords": [
    "sparse attention matrix",
    "transformer fsat",
    "efficient",
    "the standard multi-head attention",
    "an attention matrix",
    "we",
    "the attention matrix",
    "cross",
    "natural",
    "it",
    "self",
    "fourier",
    "dependencies",
    "global context dependencies",
    "sequence"
  ],
  "url": "https://aclanthology.org/2022.acl-long.19/",
  "provenance": {
    "collected_at": "2025-06-05 08:24:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}