{
  "id": "2024.arabicnlp-1.52",
  "title": "CUET}{\\_}sstm at {A}r{AIE}val Shared Task: Unimodal (Text) Propagandistic Technique Detection Using Transformer-Based Model",
  "authors": [
    "Labib, Momtazul  and\nRahman, Samia  and\nMurad, Hasan  and\nDas, Udoy"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "In recent days, propaganda has started to influence public opinion increasingly as social media usage continues to grow. Our research has been part of the first challenge, Unimodal (Text) Propagandistic Technique Detection of ArAIEval shared task at the ArabicNLP 2024 conference, co-located with ACL 2024, identifying specific Arabic text spans using twenty-three propaganda techniques. We have augmented underrepresented techniques in the provided dataset using synonym replacement and have evaluated various machine learning (RF, SVM, MNB), deep learning (BiLSTM), and transformer-based models (bert-base-arabic, Marefa-NER, AraBERT) with transfer learning. Our comparative study has shown that the transformer model “bert-base-arabic” has outperformed other models. Evaluating the test set, it has achieved the micro-F1 score of 0.2995 which is the highest. This result has secured our team “CUET_sstm” first place among all participants in task 1 of the ArAIEval.",
  "keywords": [
    "deep",
    "the araieval",
    "bert-base-arabic marefa-ner arabert",
    "we",
    "rf svm mnb",
    "val",
    "svm",
    "it",
    "bilstm",
    "arabert",
    "learning",
    "araieval shared task",
    "transfer",
    "ner",
    "bert"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.52/",
  "provenance": {
    "collected_at": "2025-06-05 11:02:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}