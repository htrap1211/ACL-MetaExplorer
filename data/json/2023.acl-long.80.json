{
  "id": "2023.acl-long.80",
  "title": "On the Efficacy of Sampling Adapters",
  "authors": [
    "Meister, Clara  and\nPimentel, Tiago  and\nMalagutti, Luca  and\nWilcox, Ethan  and\nCotterell, Ryan"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Sampling-based decoding strategies are widely employed for generating text from probabilistic models, yet standard ancestral sampling often results in text that is degenerate or incoherent. To alleviate this issue, various modifications to a modelâ€™s sampling distribution, such as top-p or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the token-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of distribution quality (such as perplexity), we find that several precision-emphasizing measures indeed indicate that sampling adapters can lead to probability distributions more aligned with the true distribution. Further, these measures correlate with higher sequence-level quality scores, specifically, Mauve.",
  "keywords": [
    "precision",
    "rate",
    "question",
    "we",
    "token",
    "a unified framework",
    "unified",
    "sequence",
    "several precision-emphasizing measures",
    "text",
    "metrics",
    "sampling-based decoding strategies",
    "its precision rate",
    "strategies",
    "perplexity"
  ],
  "url": "https://aclanthology.org/2023.acl-long.80/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}