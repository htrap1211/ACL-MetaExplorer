{
  "id": "2024.findings-acl.419",
  "title": "LEIA}: Facilitating Cross-lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation",
  "authors": [
    "Yamada, Ikuya  and\nRi, Ryokan"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages.",
  "keywords": [
    "cross",
    "tuning",
    "knowledge",
    "language",
    "model",
    "llms",
    "language models",
    "the efficiency",
    "7b-parameter llms",
    "modeling",
    "question",
    "efficiency",
    "we",
    "transfer",
    "english-based large language models"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.419/",
  "provenance": {
    "collected_at": "2025-06-05 10:54:19",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}