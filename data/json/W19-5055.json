{
  "id": "W19-5055",
  "title": "Saama Research at {MEDIQA} 2019: Pre-trained {B}io{BERT} with Attention Visualisation for Medical Natural Language Inference",
  "authors": [
    "Kanakarajan, Kamal raj  and\nRamamoorthy, Suriyadeepan  and\nArchana, Vaidheeswaran  and\nChatterjee, Soham  and\nSankarasubbu, Malaikannan"
  ],
  "year": "2019",
  "venue": "Proceedings of the 18th BioNLP Workshop and Shared Task",
  "abstract": "Natural Language inference is the task of identifying relation between two sentences as entailment, contradiction or neutrality. MedNLI is a biomedical flavour of NLI for clinical domain. This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI. The proposed model, BERT pre-trained on PMC, PubMed and fine-tuned on MIMICIII v1.4, achieves state of the art results on MedNLI (83.45%) and an accuracy of 78.5% in MEDIQA challenge. The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz.",
  "keywords": [
    "transformer",
    "a visualization tool",
    "language",
    "natural",
    "bert",
    "model",
    "bertviz",
    "mediqa challenge",
    "transformer bert",
    "mediqa",
    "encoder",
    "bidirectional encoder representation",
    "an accuracy",
    "the attention patterns",
    "attention"
  ],
  "url": "https://aclanthology.org/W19-5055/",
  "provenance": {
    "collected_at": "2025-06-05 00:57:11",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}