{
  "id": "W18-3403",
  "title": "Multi-task learning for historical text normalization: Size matters",
  "authors": [
    "Bollmann, Marcel  and\nS{\\o}gaard, Anders  and\nBingel, Joachim"
  ],
  "year": "2018",
  "venue": "Proceedings of the Workshop on Deep Learning Approaches for Low-Resource {NLP}",
  "abstract": "Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that multi-task learning can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from multi-task learning. We explore the benefits of multi-task learning across 10 different datasets, representing different languages and periods. Our main finding—contrary to what has been observed for other NLP tasks—is that multi-task learning mainly works when target task data is very scarce.",
  "keywords": [
    "work",
    "normalization",
    "language",
    "nlp",
    "high variance",
    "text",
    "it",
    "we",
    "learning",
    "other nlp tasks",
    "variance",
    "that",
    "multi",
    "related problems",
    "task"
  ],
  "url": "https://aclanthology.org/W18-3403/",
  "provenance": {
    "collected_at": "2025-06-05 00:08:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}