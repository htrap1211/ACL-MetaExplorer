{
  "id": "2021.acl-long.265",
  "title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment",
  "authors": [
    "Chi, Zewen  and\nDong, Li  and\nZheng, Bo  and\nHuang, Shaohan  and\nMao, Xian-Ling  and\nHuang, Heyan  and\nWei, Furu"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-label word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks. The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align.",
  "keywords": [
    "code",
    "alignments",
    "rate",
    "question",
    "we",
    "denoising word alignment",
    "training",
    "cross",
    "the cross-lingual language models",
    "a pretrained word aligner",
    "aligner",
    "masked language modeling",
    "self",
    "token",
    "word"
  ],
  "url": "https://aclanthology.org/2021.acl-long.265/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}