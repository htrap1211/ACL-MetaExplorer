{
  "id": "W19-5045",
  "title": "KU}{\\_}ai at {MEDIQA} 2019: Domain-specific Pre-training and Transfer Learning for Medical {NLI",
  "authors": [
    "Cengiz, Cemil  and\nSert, Ula{\\c{s}}  and\nYuret, Deniz"
  ],
  "year": "2019",
  "venue": "Proceedings of the 18th BioNLP Workshop and Shared Task",
  "abstract": "In this paper, we describe our system and results submitted for the Natural Language Inference (NLI) track of the MEDIQA 2019 Shared Task. As KU_ai team, we used BERT as our baseline model and pre-processed the MedNLI dataset to mitigate the negative impact of de-identification artifacts. Moreover, we investigated different pre-training and transfer learning approaches to improve the performance. We show that pre-training the language model on rich biomedical corpora has a significant effect in teaching the model domain-specific language. In addition, training the model on large NLI datasets such as MultiNLI and SNLI helps in learning task-specific reasoning. Finally, we ensembled our highest-performing models, and achieved 84.7% accuracy on the unseen test dataset and ranked 10th out of 17 teams in the official results.",
  "keywords": [
    "the language model",
    "accuracy",
    "language",
    "natural",
    "bert",
    "model",
    "84 7 accuracy",
    "mediqa",
    "rich",
    "we",
    "learning",
    "transfer",
    "pre",
    "training",
    "identification"
  ],
  "url": "https://aclanthology.org/W19-5045/",
  "provenance": {
    "collected_at": "2025-06-05 00:56:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}