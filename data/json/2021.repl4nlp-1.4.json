{
  "id": "2021.repl4nlp-1.4",
  "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
  "authors": [
    "Goyal, Naman  and\nDu, Jingfei  and\nOtt, Myle  and\nAnantharaman, Giri  and\nConneau, Alexis"
  ],
  "year": "2021",
  "venue": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
  "abstract": "Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed and outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.",
  "keywords": [
    "work",
    "cross",
    "code",
    "cross-lingual language model",
    "accuracy",
    "transformers",
    "roberta",
    "language",
    "model",
    "we",
    "the roberta-large model",
    "larger-scale transformers",
    "the effectiveness",
    "99 more languages",
    "this"
  ],
  "url": "https://aclanthology.org/2021.repl4nlp-1.4/",
  "provenance": {
    "collected_at": "2025-06-05 08:19:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}