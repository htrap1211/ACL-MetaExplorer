{
  "id": "P18-2022",
  "title": "SNAG}: Spoken Narratives and Gaze Dataset",
  "authors": [
    "Vaidyanathan, Preethi  and\nPrud{'}hommeaux, Emily T.  and\nPelz, Jeff B.  and\nAlm, Cecilia O."
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Humans rely on multiple sensory modalities when examining and reasoning over images. In this paper, we describe a new multimodal dataset that consists of gaze measurements and spoken descriptions collected in parallel during an image inspection task. The task was performed by multiple participants on 100 general-domain images showing everyday objects and activities. We demonstrate the usefulness of the dataset by applying an existing visual-linguistic data fusion framework in order to label important image regions with appropriate linguistic labels.",
  "keywords": [
    "general",
    "multiple sensory modalities",
    "modalities",
    "we",
    "visual",
    "activities",
    "fusion",
    "100 general-domain images",
    "that",
    "usefulness",
    "gaze measurements",
    "new",
    "task",
    "multiple participants",
    "images"
  ],
  "url": "https://aclanthology.org/P18-2022/",
  "provenance": {
    "collected_at": "2025-06-05 00:16:42",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}