{
  "id": "2021.gem-1.10",
  "title": "The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics",
  "authors": [
    "Gehrmann, Sebastian  and\nAdewumi, Tosin  and\nAggarwal, Karmanya  and\nAmmanamanchi, Pawan Sasanka  and\nAremu, Anuoluwapo  and\nBosselut, Antoine  and\nChandu, Khyathi Raghavi  and\nClinciu, Miruna-Adriana  and\nDas, Dipanjan  and\nDhole, Kaustubh  and\nDu, Wanyu  and\nDurmus, Esin  and\nDu{\\v{s}}ek, Ond{\\v{r}}ej  and\nEmezue, Chris Chinenye  and\nGangal, Varun  and\nGarbacea, Cristina  and\nHashimoto, Tatsunori  and\nHou, Yufang  and\nJernite, Yacine  and\nJhamtani, Harsh  and\nJi, Yangfeng  and\nJolly, Shailza  and\nKale, Mihir  and\nKumar, Dhruv  and\nLadhak, Faisal  and\nMadaan, Aman  and\nMaddela, Mounica  and\nMahajan, Khyati  and\nMahamood, Saad  and\nMajumder, Bodhisattwa Prasad  and\nMartins, Pedro Henrique  and\nMcMillan-Major, Angelina  and\nMille, Simon  and\nvan Miltenburg, Emiel  and\nNadeem, Moin  and\nNarayan, Shashi  and\nNikolaev, Vitaly  and\nNiyongabo Rubungo, Andre  and\nOsei, Salomey  and\nParikh, Ankur  and\nPerez-Beltrachini, Laura  and\nRao, Niranjan Ramesh  and\nRaunak, Vikas  and\nRodriguez, Juan Diego  and\nSanthanam, Sashank  and\nSedoc, Jo{\\~a}o  and\nSellam, Thibault  and\nShaikh, Samira  and\nShimorina, Anastasia  and\nSobrevilla Cabezudo, Marco Antonio  and\nStrobelt, Hendrik  and\nSubramani, Nishant  and\nXu, Wei  and\nYang, Diyi  and\nYerukola, Akhila  and\nZhou, Jiawei"
  ],
  "year": "2021",
  "venue": "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
  "abstract": "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
  "keywords": [
    "language",
    "generation",
    "natural",
    "automated metrics datasets",
    "opportunities",
    "it",
    "well-established but flawed metrics",
    "human",
    "metrics",
    "strategies",
    "its evaluation",
    "regular",
    "human evaluation standards",
    "we",
    "natural language generation"
  ],
  "url": "https://aclanthology.org/2021.gem-1.10/",
  "provenance": {
    "collected_at": "2025-06-05 08:17:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}