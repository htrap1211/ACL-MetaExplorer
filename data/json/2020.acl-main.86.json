{
  "id": "2020.acl-main.86",
  "title": "Dynamic Sampling Strategies for Multi-Task Reading Comprehension",
  "authors": [
    "Gottumukkala, Ananth  and\nDua, Dheeru  and\nSingh, Sameer  and\nGardner, Matt"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community. Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up. We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task modelâ€™s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning. We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level. Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.",
  "keywords": [
    "we",
    "current",
    "prior multi-task sampling strategies",
    "learning",
    "strategies",
    "-",
    "generalization",
    "work",
    "general",
    "model",
    "dynamic sampling strategies",
    "time",
    "batch",
    "general reading comprehension systems",
    "multi"
  ],
  "url": "https://aclanthology.org/2020.acl-main.86/",
  "provenance": {
    "collected_at": "2025-06-05 07:43:23",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}