{
  "id": "2023.acl-short.108",
  "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting",
  "authors": [
    "Fatemi, Zahra  and\nXing, Chen  and\nLiu, Wenhao  and\nXiong, Caimming"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the modelâ€™s downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.",
  "keywords": [
    "work",
    "prompts",
    "general",
    "bias",
    "prompt",
    "language",
    "pre-trained language models",
    "nlp",
    "model",
    "studies",
    "gender-related prompts",
    "gender bias",
    "-",
    "information",
    "general nlp tasks"
  ],
  "url": "https://aclanthology.org/2023.acl-short.108/",
  "provenance": {
    "collected_at": "2025-06-05 09:49:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}