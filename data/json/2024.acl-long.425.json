{
  "id": "2024.acl-long.425",
  "title": "Synthesizing Text-to-{SQL} Data from Weak and Strong {LLM}s",
  "authors": [
    "Yang, Jiaxi  and\nHui, Binyuan  and\nYang, Min  and\nYang, Jian  and\nLin, Junyang  and\nZhou, Chang"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.",
  "keywords": [
    "instruction",
    "the domain generalization",
    "instruction tuning",
    "tuning",
    "language",
    "weak and strong llm",
    "model",
    "text",
    "open-source llms",
    "information",
    "generalization",
    "we",
    "learning",
    "llm",
    "llms"
  ],
  "url": "https://aclanthology.org/2024.acl-long.425/",
  "provenance": {
    "collected_at": "2025-06-05 10:40:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}