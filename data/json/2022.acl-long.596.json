{
  "id": "2022.acl-long.596",
  "title": "End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding",
  "authors": [
    "Li, Mengze  and\nWang, Tianbao  and\nZhang, Haoyu  and\nZhang, Shengyu  and\nZhao, Zhou  and\nMiao, Jiaxu  and\nZhang, Wenqiao  and\nTan, Wenming  and\nWang, Jin  and\nWang, Peng  and\nPu, Shiliang  and\nWu, Fei"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Natural language spatial video grounding aims to detect the relevant objects in video frames with descriptive sentences as the query. In spite of the great advances, most existing methods rely on dense video frame annotations, which require a tremendous amount of human effort. To achieve effective grounding under a limited annotation budget, we investigate one-shot video grounding and learn to ground natural language in all video frames with solely one frame labeled, in an end-to-end manner. One major challenge of end-to-end one-shot video grounding is the existence of videos frames that are either irrelevant to the language query or the labeled frame. Another challenge relates to the limited supervision, which might result in ineffective representation learning. To address these challenges, we designed an end-to-end model via Information Tree for One-Shot video grounding (IT-OS). Its key module, the information tree, can eliminate the interference of irrelevant frames based on branch search and branch cropping techniques. In addition, several self-supervised tasks are proposed based on the information tree to improve the representation learning under insufficient labeling. Experiments on the benchmark dataset demonstrate the effectiveness of our model.",
  "keywords": [
    "end",
    "we",
    "shot",
    "natural",
    "it",
    "self",
    "information",
    "learning",
    "natural language",
    "manner",
    "insufficient",
    "insufficient labeling experiments",
    "language",
    "model",
    "human"
  ],
  "url": "https://aclanthology.org/2022.acl-long.596/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:19",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}