{
  "id": "2023.acl-long.72",
  "title": "Consistency Regularization Training for Compositional Generalization",
  "authors": [
    "Yin, Yongjing  and\nZeng, Jiali  and\nLi, Yafu  and\nMeng, Fandong  and\nZhou, Jie  and\nZhang, Yue"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Existing neural models have difficulty generalizing to unseen combinations of seen components. To achieve compositional generalization, models are required to consistently interpret (sub)expressions across contexts. Without modifying model architectures, we improve the capability of Transformer on compositional generalization through consistency regularization training, which promotes representation consistency across samples and prediction consistency for a single sample. Experimental results on semantic parsing and machine translation benchmarks empirically demonstrate the effectiveness and generality of our method. In addition, we find that the prediction consistency scores on in-distribution validation sets can be an alternative for evaluating models during training, when commonly-used metrics are not informative.",
  "keywords": [
    "transformer",
    "parsing",
    "validation",
    "neural",
    "model",
    "machine",
    "generality",
    "metrics",
    "consistency regularization training",
    "compositional generalization models",
    "regularization",
    "generalization",
    "commonly-used metrics",
    "semantic",
    "we"
  ],
  "url": "https://aclanthology.org/2023.acl-long.72/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}