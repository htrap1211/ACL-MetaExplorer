{
  "id": "2023.iwslt-1.35",
  "title": "The {HW}-{TSC}{'}s Simultaneous Speech-to-Text Translation System for {IWSLT} 2023 Evaluation",
  "authors": [
    "Guo, Jiaxin  and\nWei, Daimeng  and\nWu, Zhanglin  and\nLi, Zongyao  and\nRao, Zhiqiang  and\nWang, Minghan  and\nShang, Hengchao  and\nChen, Xiaoyu  and\nYu, Zhengzhe  and\nLi, Shaojun  and\nXie, Yuhao  and\nLei, Lizhi  and\nYang, Hao"
  ],
  "year": "2023",
  "venue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
  "abstract": "In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Text Translation competition. Our participation involves three language directions: English-German, English-Chinese, and English-Japanese. Our proposed solution is a cascaded incremental decoding system that comprises an ASR model and an MT model. The ASR model is based on the U2++ architecture and can handle both streaming and offline speech scenarios with ease. Meanwhile, the MT model adopts the Deep-Transformer architecture. To improve performance, we explore methods to generate a confident partial target text output that guides the next MT incremental decoding process. In our experiments, we demonstrate that our simultaneous strategies achieve low latency while maintaining a loss of no more than 2 BLEU points when compared to offline systems.",
  "keywords": [
    "our simultaneous strategies",
    "deep",
    "transformer",
    "process",
    "language",
    "model",
    "text",
    "strategies",
    "iwslt 2023 evaluation",
    "bleu",
    "loss",
    "we",
    "evaluation",
    "the deep-transformer architecture",
    "translation"
  ],
  "url": "https://aclanthology.org/2023.iwslt-1.35/",
  "provenance": {
    "collected_at": "2025-06-05 10:25:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}