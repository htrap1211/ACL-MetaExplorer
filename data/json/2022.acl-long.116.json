{
  "id": "2022.acl-long.116",
  "title": "Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation",
  "authors": [
    "Choi, Dongha  and\nChoi, HongSeok  and\nLee, Hyunju"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to boost their performance on downstream tasks in specific domains, such as biomedical or scientific domains. Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs. However, these pre-training methods require considerable in-domain data and training resources and a longer training time. Moreover, the training must be re-performed whenever a new PLM emerges. In this study, we propose a domain knowledge transferring (DoKTra) framework for PLMs without additional in-domain pretraining. Specifically, we extract the domain knowledge from an existing in-domain pretrained language model and transfer it to other PLMs by applying knowledge distillation. In particular, we employ activation boundary distillation, which focuses on the activation of hidden neurons. We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities, and thus aid the distillation. By applying the proposed DoKTra framework to downstream tasks in the biomedical, clinical, and financial domains, our student models can retain a high percentage of teacher performance and even outperform the teachers in certain tasks. Our code is available athttps://github.com/DMCB-GIST/DoKTra.",
  "keywords": [
    "code",
    "activation boundary distillation",
    "we",
    "training",
    "it",
    "the activation",
    "scientific",
    "pre-trained language model",
    "reliable output probabilities",
    "activation",
    "calibrated activation boundary distillation",
    "knowledge",
    "language",
    "model",
    "pretrained language models"
  ],
  "url": "https://aclanthology.org/2022.acl-long.116/",
  "provenance": {
    "collected_at": "2025-06-05 08:25:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}