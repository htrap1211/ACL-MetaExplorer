{
  "id": "2024.acl-long.716",
  "title": "Visualization Recommendation with Prompt-based Reprogramming of Large Language Models",
  "authors": [
    "Li, Xinhang  and\nZhou, Jingbo  and\nChen, Wei  and\nXu, Derong  and\nXu, Tong  and\nChen, Enhong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Visualization recommendations, which aim to automatically match proper visual charts for specific data tables, can significantly simplify the data analysis process. Traditional approaches in this domain have primarily relied on rule-based or machine learning-based methodologies. These methods often demand extensive manual maintenance and yet fail to fully comprehend the tabular data, leading to unsatisfactory performance. Recently, Large Language Models (LLMs) have emerged as powerful tools, exhibiting strong reasoning capabilities. This advancement suggests their substantial promise in addressing visualization recommendation challenges. However, effectively harnessing LLMs to discern and rationalize patterns in tabular data, and consequently deduce the essential information for chart generation, remains an unresolved challenge. To this end, we introduce a novel Hierarchical Table Prompt-based reprogramming framework, named HTP. This framework aims to integrate multi-dimensional tabular data into LLMs through a strategically crafted prompt learning method while keeping the LLMsâ€™ backbone and weights unaltered. The HTP framework uniquely incorporates a four-level prompt structure, encompassing general, instance, cluster, and column levels. This multi-level approach is engineered to provide a comprehensive understanding of both general distribution and multifaceted fine-grained features of tabular data, before inputting the tabular data into the frozen LLM. Our empirical studies confirm that the HTP framework achieves state-of-the-art performance, marking an advancement in the field of data visualization and analysis. The code and data will be made publicly available upon acceptance.",
  "keywords": [
    "code",
    "end",
    "our empirical studies",
    "field",
    "the llms",
    "we",
    "llm",
    "visualization recommendation",
    "cluster",
    "the field",
    "the frozen llm",
    "information",
    "learning",
    "visual",
    "prompt-based reprogramming"
  ],
  "url": "https://aclanthology.org/2024.acl-long.716/",
  "provenance": {
    "collected_at": "2025-06-05 10:44:10",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}