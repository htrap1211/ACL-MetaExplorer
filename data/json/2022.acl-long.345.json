{
  "id": "2022.acl-long.345",
  "title": "A Comparative Study of Faithfulness Metrics for Model Interpretability Methods",
  "authors": [
    "Chan, Chun Sik  and\nKong, Huanqi  and\nGuanqing, Liang"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Interpretable methods to reveal the internal reasoning processes behind machine learning models have attracted increasing attention in recent years. To quantify the extent to which the identified interpretations truly reflect the intrinsic decision-making mechanisms, various faithfulness evaluation metrics have been proposed. However, we find that different faithfulness metrics show conflicting preferences when comparing different interpretations. Motivated by this observation, we aim to conduct a comprehensive and comparative study of the widely adopted faithfulness metrics. In particular, we introduce two assessment dimensions, namely diagnosticity and complexity. Diagnosticity refers to the degree to which the faithfulness metric favors relatively faithful interpretations over randomly generated ones, and complexity is measured by the average number of model forward passes. According to the experimental results, we find that sufficiency and comprehensiveness metrics have higher diagnosticity and lower complexity than the other faithfulness metrics.",
  "keywords": [
    "faithfulness metrics",
    "increasing attention",
    "comprehensiveness metrics",
    "the other faithfulness metrics",
    "model",
    "machine",
    "randomly generated ones",
    "metric",
    "various faithfulness evaluation metrics",
    "different faithfulness metrics",
    "machine learning models",
    "metrics",
    "the identified interpretations",
    "sufficiency",
    "attention"
  ],
  "url": "https://aclanthology.org/2022.acl-long.345/",
  "provenance": {
    "collected_at": "2025-06-05 08:28:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}