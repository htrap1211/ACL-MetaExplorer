{
  "id": "2024.acl-long.620",
  "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
  "authors": [
    "Singh, Shivalika  and\nVargus, Freddie  and\nD{'}souza, Daniel  and\nKarlsson, B{\\\"o}rje F.  and\nMahendiran, Abinaya  and\nKo, Wei-Yin  and\nShandilya, Herumb  and\nPatel, Jay  and\nMataciunas, Deividas  and\nO{'}Mahony, Laura  and\nZhang, Mike  and\nHettiarachchi, Ramith  and\nWilson, Joseph  and\nMachado, Marina  and\nMoura, Luisa  and\nKrzemi{\\'n}ski, Dominik  and\nFadaei, Hakimeh  and\nErgun, Irem  and\nOkoh, Ifeoma  and\nAlaagib, Aisha  and\nMudannayake, Oshan  and\nAlyafeai, Zaid  and\nChien, Vu  and\nRuder, Sebastian  and\nGuthikonda, Surya  and\nAlghamdi, Emad  and\nGehrmann, Sebastian  and\nMuennighoff, Niklas  and\nBartolo, Max  and\nKreutzer, Julia  and\n{\\\"U}st{\\\"u}n, Ahmet  and\nFadaee, Marzieh  and\nHooker, Sara"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and augmenting existing datasets across 114 languages. In total, we contribute three key resources: we develop and open-source the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as an important framework for future research collaborations that aim to bridge gaps in resources.",
  "keywords": [
    "we",
    "natural language processing nlp",
    "instruction",
    "natural",
    "many recent achievements",
    "tuning",
    "processing",
    "a large language model",
    "fine",
    "countries",
    "work",
    "the fine-tuning",
    "language",
    "nlp",
    "119 countries"
  ],
  "url": "https://aclanthology.org/2024.acl-long.620/",
  "provenance": {
    "collected_at": "2025-06-05 10:42:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}