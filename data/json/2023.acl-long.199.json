{
  "id": "2023.acl-long.199",
  "title": "DAMP}: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue",
  "authors": [
    "Held, William  and\nHidey, Christopher  and\nLiu, Fei  and\nZhu, Eric  and\nGoel, Rahul  and\nYang, Diyi  and\nShah, Rushin"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust multilingual semantic parsing is critical as codeswitching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that contrastive alignment pretraining improvesbothEnglish performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters.",
  "keywords": [
    "the zero-shot performance",
    "work",
    "alignment",
    "mbert transfer performance",
    "parsing",
    "internal semantic parsing engines",
    "mbert",
    "transfer efficiency",
    "optimization",
    "efficiency",
    "that contrastive alignment",
    "semantic",
    "we",
    "semantic parsing",
    "hyperparameter-free adversarial alignment"
  ],
  "url": "https://aclanthology.org/2023.acl-long.199/",
  "provenance": {
    "collected_at": "2025-06-05 09:38:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}