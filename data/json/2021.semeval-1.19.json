{
  "id": "2021.semeval-1.19",
  "title": "R}e{CAM}@{IITK} at {S}em{E}val-2021 Task 4: {BERT} and {ALBERT} based Ensemble for Abstract Word Prediction",
  "authors": [
    "Mittal, Abhishek  and\nModi, Ashutosh"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "This paper describes our system for Task 4 of SemEval-2021: Reading Comprehension of Abstract Meaning (ReCAM). We participated in all subtasks where the main goal was to predict an abstract word missing from a statement. We fine-tuned the pre-trained masked language models namely BERT and ALBERT and used an Ensemble of these as our submitted system on Subtask 1 (ReCAM-Imperceptibility) and Subtask 2 (ReCAM-Nonspecificity). For Subtask 3 (ReCAM-Intersection), we submitted the ALBERT model as it gives the best results. We tried multiple approaches and found that Masked Language Modeling(MLM) based approach works the best.",
  "keywords": [
    "ensemble",
    "language",
    "bert",
    "model",
    "it",
    "albert",
    "modeling",
    "the albert model",
    "word",
    "we",
    "pre",
    "an ensemble",
    "language modeling",
    "approach",
    "reading"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.19/",
  "provenance": {
    "collected_at": "2025-06-05 08:20:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}