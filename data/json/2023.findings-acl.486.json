{
  "id": "2023.findings-acl.486",
  "title": "Joint Generator-Ranker Learning for Natural Language Generation",
  "authors": [
    "Shen, Weizhou  and\nGong, Yeyun  and\nShen, Yelong  and\nWang, Song  and\nQuan, Xiaojun  and\nDuan, Nan  and\nChen, Weizhu"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. By iteratively updating the generator and the ranker, JGR can effectively harmonize their learning and enhance their quality jointly. We evaluate JGR on various text generation tasks and demonstrate that it surpasses existing methods on four public datasets across three common generation scenarios. Our code and models are publicly available athttps://github.com/microsoft/ProphetNet/tree/master/JGR.",
  "keywords": [
    "code",
    "feedback",
    "joint generator-ranker learning",
    "we",
    "text generation",
    "training",
    "the generator",
    "natural",
    "it",
    "generate-then-rank",
    "loss",
    "learning",
    "natural language generation",
    "various text generation tasks",
    "text"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.486/",
  "provenance": {
    "collected_at": "2025-06-05 10:15:13",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}