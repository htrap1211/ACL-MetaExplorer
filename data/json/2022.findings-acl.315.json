{
  "id": "2022.findings-acl.315",
  "title": "Interpreting the Robustness of Neural {NLP} Models to Textual Perturbations",
  "authors": [
    "Zhang, Yunxiang  and\nPan, Liangming  and\nTan, Samson  and\nKan, Min-Yen"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data. However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We further give a causal justification for the learnability metric. We conduct extensive experiments with four prominent NLP models — TextRNN, BERT, RoBERTa and XLNet — over eight types of textual perturbations on three datasets. We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesis.",
  "keywords": [
    "work",
    "roberta",
    "processing",
    "language",
    "neural",
    "nlp",
    "natural",
    "model",
    "metric",
    "it",
    "neural nlp models",
    "bert",
    "we",
    "textrnn",
    "time"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.315/",
  "provenance": {
    "collected_at": "2025-06-05 08:39:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}