{
  "id": "2023.acl-long.502",
  "title": "M}ix{CE}: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies",
  "authors": [
    "Zhang, Shiyue  and\nWu, Shijie  and\nIrsoy, Ozan  and\nLu, Steven  and\nBansal, Mohit  and\nDredze, Mark  and\nRosenberg, David"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model. Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies. We evaluate models trained with this objective on synthetic data settings (where P is known) and real data, and show that the resulting models yield better generated text without complex decoding strategies.",
  "keywords": [
    "cross",
    "p",
    "i",
    "q",
    "language",
    "complex decoding strategies",
    "model",
    "text",
    "human",
    "objective",
    "entropies",
    "this objective",
    "better generated text",
    "strategies",
    "-"
  ],
  "url": "https://aclanthology.org/2023.acl-long.502/",
  "provenance": {
    "collected_at": "2025-06-05 09:42:13",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}