{
  "id": "2023.acl-short.32",
  "title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages",
  "authors": [
    "Karoui, Yasmine  and\nLebret, R{\\'e}mi  and\nForoutan Eghlidi, Negar  and\nAberer, Karl"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available athttps://github.com/Yasminekaroui/CliCoTea.",
  "keywords": [
    "variety",
    "code",
    "efficient",
    "yasminekaroui clicotea",
    "machine translation",
    "we",
    "shot",
    "training",
    "translation",
    "cross",
    "natural",
    "image queries",
    "queries",
    "token",
    "retrieval"
  ],
  "url": "https://aclanthology.org/2023.acl-short.32/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}