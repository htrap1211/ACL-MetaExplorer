{
  "id": "2024.acl-long.516",
  "title": "Are {LLM}-based Evaluators Confusing {NLG} Quality Criteria?",
  "authors": [
    "Hu, Xinyu  and\nGao, Mingqi  and\nHu, Sen  and\nZhang, Yang  and\nChen, Yicheng  and\nXu, Teng  and\nWan, Xiaojun"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.",
  "keywords": [
    "we",
    "the classification system",
    "confusion",
    "nlg evaluation",
    "classification",
    "different llms",
    "analysis",
    "llms",
    "previous studies",
    "llm-based evaluation",
    "work",
    "hierarchical",
    "studies",
    "human",
    "the evaluation behaviors"
  ],
  "url": "https://aclanthology.org/2024.acl-long.516/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}