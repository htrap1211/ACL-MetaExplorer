{
  "id": "P18-1143",
  "title": "Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data",
  "authors": [
    "Pratapa, Adithya  and\nBhat, Gayatri  and\nChoudhury, Monojit  and\nSitaram, Sunayana  and\nDandapat, Sandipan  and\nBali, Kalika"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.",
  "keywords": [
    "code",
    "language",
    "randomly generated cm data",
    "model",
    "it",
    "modeling",
    "perplexity",
    "the perplexity",
    "we",
    "valid",
    "rnn",
    "an rnn-based language model",
    "language modeling",
    "training",
    "theory"
  ],
  "url": "https://aclanthology.org/P18-1143/",
  "provenance": {
    "collected_at": "2025-06-05 00:12:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}