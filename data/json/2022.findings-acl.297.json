{
  "id": "2022.findings-acl.297",
  "title": "Prior Knowledge and Memory Enriched Transformer for Sign Language Translation",
  "authors": [
    "Jin, Tao  and\nZhao, Zhou  and\nZhang, Meng  and\nZeng, Xingshan"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "This paper attacks the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax). However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them. Based on this concern, we propose a novel method called Prior knowledge and memory Enriched Transformer (PET) for SLT, which incorporates the auxiliary information into vanilla transformer. Concretely, we develop gated interactive multi-head attention which associates the multimodal representation and global signing style with adaptive gated functions. One Part-of-Speech (POS) sequence generator relies on the associated information to predict the global syntactic structure, which is thereafter leveraged to guide the sentence generation. Besides, considering that the visual-textual context information, and additional auxiliary knowledge of a word may appear in more than one video, we design a multi-stream memory structure to obtain higher-quality translations, which stores the detailed correspondence between a word and its various relevant information, leading to a more comprehensive understanding for each word. We conduct extensive empirical studies on RWTH-PHOENIX-Weather-2014 dataset with both signer-dependent and signer-independent conditions. The quantitative and qualitative experimental results comprehensively reveal the effectiveness of PET.",
  "keywords": [
    "sign language translation slt",
    "all",
    "we",
    "vanilla transformer",
    "syntax",
    "gated interactive multi-head attention",
    "translation",
    "decoder",
    "information",
    "word",
    "sequence",
    "visual",
    "i",
    "pos",
    "extensive empirical studies"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.297/",
  "provenance": {
    "collected_at": "2025-06-05 08:38:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}