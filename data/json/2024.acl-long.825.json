{
  "id": "2024.acl-long.825",
  "title": "Iterative Forward Tuning Boosts In-Context Learning in Language Models",
  "authors": [
    "Yang, Jiaxi  and\nHui, Binyuan  and\nYang, Min  and\nWang, Bailin  and\nLi, Bowen  and\nLi, Binhua  and\nHuang, Fei  and\nLi, Yongbin"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.",
  "keywords": [
    "deep",
    "specific prompt engineering",
    "we",
    "current",
    "training",
    "enhanced understanding capabilities",
    "information",
    "a unique attention mechanism",
    "learning",
    "llms",
    "processing",
    "prompt",
    "i",
    "language models",
    "process"
  ],
  "url": "https://aclanthology.org/2024.acl-long.825/",
  "provenance": {
    "collected_at": "2025-06-05 10:45:39",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}