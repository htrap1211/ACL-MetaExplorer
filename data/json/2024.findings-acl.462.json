{
  "id": "2024.findings-acl.462",
  "title": "E}-{EVAL}: A Comprehensive {C}hinese K-12 Education Evaluation Benchmark for Large Language Models",
  "authors": [
    "Hou, Jinchang  and\nAo, Chang  and\nWu, Haihong  and\nKong, Xiangtao  and\nZheng, Zhigang  and\nTang, Daijia  and\nLi, Chengming  and\nHu, Xiping  and\nXu, Ruifeng  and\nNi, Shiwen  and\nYang, Min"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "The rapid development of Large Language Models (LLMs) has led to their increasing utilization in Chinese K-12 education. Despite the growing integration of LLMs and education, the absence of a dedicated benchmark for evaluating LLMs within this domain presents a pressing concern. Consequently, there is an urgent need for a comprehensive natural language processing benchmark to precisely assess the capabilities of various LLMs in Chinese K-12 education. In response, we introduce E-EVAL, the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects. Through meticulous evaluation, we find that Chinese-dominant models often outperform English-dominant ones, with many exceeding GPT 4.0. However, most struggle with complex subjects like mathematics. Additionally, our analysis indicates that most Chinese-dominant LLMs do not achieve higher scores at the primary school level compared to the middle school level, highlighting the nuanced relationship between proficiency in higher-order and lower-order knowledge domains. Furthermore, experimental results highlight the effectiveness of the Chain of Thought (CoT) technique in scientific subjects and Few-shot prompting in liberal arts. Through E-EVAL, we aim to conduct a rigorous analysis delineating the strengths and limitations of LLMs in educational applications, thereby contributing significantly to the advancement of Chinese K-12 education and LLMs.",
  "keywords": [
    "meticulous evaluation",
    "chain",
    "we",
    "shot",
    "many exceeding gpt",
    "most chinese-dominant llms",
    "natural",
    "scientific subjects",
    "cot",
    "e",
    "the capabilities",
    "scientific",
    "analysis",
    "llms",
    "processing"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.462/",
  "provenance": {
    "collected_at": "2025-06-05 10:54:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}