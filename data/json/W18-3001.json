{
  "id": "W18-3001",
  "title": "Corpus Specificity in {LSA} and Word2vec: The Role of Out-of-Domain Documents",
  "authors": [
    "Altszyler, Edgar  and\nSigman, Mariano  and\nFern{\\'a}ndez Slezak, Diego"
  ],
  "year": "2018",
  "venue": "Proceedings of the Third Workshop on Representation Learning for {NLP}",
  "abstract": "Despite the popularity of word embeddings, the precise way by which they acquire semantic relations between words remain unclear. In the present article, we investigate whether LSA and word2vec capacity to identify relevant semantic relations increases with corpus size. One intuitive hypothesis is that the capacity to identify relevant associations should increase as the amount of data increases. However, if corpus size grows in topics which are not specific to the domain of interest, signal to noise ratio may weaken. Here we investigate the effect of corpus specificity and size in word-embeddings, and for this, we study two ways for progressive elimination of documents: the elimination of random documents vs. the elimination of documents unrelated to a specific task. We show that word2vec can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus. On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of dimensionality, can increase LSA word-representation quality while speeding up the processing time. From a cognitive-modeling point of view, we point out that LSAâ€™s word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas word2vec does.",
  "keywords": [
    "lsa and word2vec capacity",
    "dimensionality",
    "semantic",
    "we",
    "training",
    "it",
    "word",
    "processing",
    "-",
    "ratio",
    "word embeddings",
    "embeddings",
    "knowledge",
    "random",
    "word-embeddings"
  ],
  "url": "https://aclanthology.org/W18-3001/",
  "provenance": {
    "collected_at": "2025-06-05 00:24:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}