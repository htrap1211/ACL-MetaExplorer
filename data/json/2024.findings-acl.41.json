{
  "id": "2024.findings-acl.41",
  "title": "Do {LVLM}s Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning",
  "authors": [
    "Huang, Kung-Hsiang  and\nZhou, Mingyang  and\nChan, Hou Pong  and\nFung, Yi  and\nWang, Zhenhailong  and\nZhang, Lingyu  and\nChang, Shih-Fu  and\nJi, Heng"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Advances in large vision-language models (LVLMs) have led to significant progress in generating natural language descriptions for visual contents. These powerful models are known for producing texts that are factually inconsistent with the visual input. While some efforts mitigate such inconsistencies in natural image captioning, the factuality of generated captions for structured visuals, such as charts, has not received as much scrutiny. This work introduces a comprehensive typology of factual errors in generated chart captions. A large-scale human annotation effort provides insight into the error patterns in captions generated by various models, ultimately forming the foundation of a dataset, CHOCOLATE. Our analysis reveals that even advanced models like GPT-4V frequently produce captions laced with factual inaccuracies. To combat this, we establish the task of Chart Caption Factual Error Correction and introduce CHARTVE, a visual entailment model that outperforms current LVLMs in evaluating caption factuality. Furthermore, we propose C2TFEC, an interpretable two-stage framework that excels at correcting factual errors. This work inaugurates a new domain in factual error correction for chart captions, presenting a novel evaluation metric, and demonstrating an effective approach to ensuring the factuality of generated chart captions. The code and data as well as the continuously updated benchmark can be found at: https://khuangaf.github.io/CHOCOLATE/.",
  "keywords": [
    "code",
    "inaccuracies",
    "gpt-4v",
    "we",
    "current",
    "such inconsistencies",
    "natural",
    "generated captions",
    "large vision-language models",
    "visual",
    "analysis",
    "s",
    "metric",
    "factual inaccuracies",
    "generated"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.41/",
  "provenance": {
    "collected_at": "2025-06-05 10:49:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}