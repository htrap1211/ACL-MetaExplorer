{
  "id": "2024.acl-long.534",
  "title": "MERA}: A Comprehensive {LLM} Evaluation in {R}ussian",
  "authors": [
    "Fenogenova, Alena  and\nChervyakov, Artem  and\nMartynov, Nikita  and\nKozlova, Anastasia  and\nTikhonova, Maria  and\nAkhmetgareeva, Albina  and\nEmelyanov, Anton  and\nShevelev, Denis  and\nLebedev, Pavel  and\nSinev, Leonid  and\nIsaeva, Ulyana  and\nKolomeytseva, Katerina  and\nMoskovskiy, Daniil  and\nGoncharova, Elizaveta  and\nSavushkin, Nikita  and\nMikhailova, Polina  and\nMinaeva, Anastasia  and\nDimitrov, Denis  and\nPanchenko, Alexander  and\nMarkov, Sergey"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). However, despite researchers’ attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce a new instruction benchmark, MERA, oriented towards the FMs’ performance on the Russian language. The benchmark encompasses 21 evaluation tasks for generative models covering 10 skills and is supplied with private answer scoring to prevent data leakage. The paper introduces a methodology to evaluate FMs and LMs in fixed zero- and few-shot instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential ethical concerns and drawbacks.",
  "keywords": [
    "code",
    "generative models",
    "researchers attention",
    "we",
    "llm",
    "shot",
    "other modalities",
    "instruction",
    "a comprehensive llm evaluation",
    "answer",
    "the capabilities limitations",
    "21 evaluation tasks",
    "generative",
    "an evaluation methodology",
    "language"
  ],
  "url": "https://aclanthology.org/2024.acl-long.534/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}