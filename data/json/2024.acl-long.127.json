{
  "id": "2024.acl-long.127",
  "title": "T}axo{LL}a{MA}: {W}ord{N}et-based Model for Solving Multiple Lexical Semantic Tasks",
  "authors": [
    "Moskvoretskii, Viktor  and\nNeminova, Ekaterina  and\nLobanova, Alina  and\nPanchenko, Alexander  and\nNikishina, Irina"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the “all-in-one” model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. TaxoLLaMA achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online (code: https://github.com/VityaVitalich/TaxoLLaMA)",
  "keywords": [
    "code",
    "tuning",
    "knowledge",
    "few-shot",
    "model",
    "it",
    "capabilities",
    "wordnet",
    "semantic",
    "we",
    "no fine-tuning",
    "the capabilities",
    "ord",
    "pre",
    "lexical-semantic knowledge"
  ],
  "url": "https://aclanthology.org/2024.acl-long.127/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}