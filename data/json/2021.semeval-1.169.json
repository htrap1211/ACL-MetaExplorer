{
  "id": "2021.semeval-1.169",
  "title": "D}uluth{NLP} at {S}em{E}val-2021 Task 7: Fine-Tuning {R}o{BERT}a Model for Humor Detection and Offense Rating",
  "authors": [
    "Akrah, Samuel"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "This paper presents the DuluthNLP submission to Task 7 of the SemEval 2021 competition on Detecting and Rating Humor and Offense. In it, we explain the approach used to train the model together with the process of fine-tuning our model in getting the results. We focus on humor detection, rating, and of-fense rating, representing three out of the four subtasks that were provided. We show that optimizing hyper-parameters for learning rate, batch size and number of epochs can increase the accuracy and F1 score for humor detection",
  "keywords": [
    "the accuracy",
    "accuracy",
    "tuning",
    "process",
    "hyper",
    "the duluthnlp submission",
    "nlp",
    "bert",
    "model",
    "it",
    "rate",
    "-",
    "fine-tuning r o bert",
    "duluthnlp",
    "batch"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.169/",
  "provenance": {
    "collected_at": "2025-06-05 08:22:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}