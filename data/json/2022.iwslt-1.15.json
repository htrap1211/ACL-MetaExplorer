{
  "id": "2022.iwslt-1.15",
  "title": "The {USTC}-{NELSLIP} Offline Speech Translation Systems for {IWSLT} 2022",
  "authors": [
    "Zhang, Weitai  and\nYe, Zhongyi  and\nTang, Haitao  and\nLi, Xiaoxi  and\nZhou, Xinyuan  and\nYang, Jing  and\nCui, Jianwei  and\nDeng, Pan  and\nShi, Mohan  and\nSong, Yifan  and\nLiu, Dan  and\nLiu, Junhua  and\nDai, Lirong"
  ],
  "year": "2022",
  "venue": "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)",
  "abstract": "This paper describes USTC-NELSLIP’s submissions to the IWSLT 2022 Offline Speech Translation task, including speech translation of talks from English to German, English to Chinese and English to Japanese. We describe both cascaded architectures and end-to-end models which can directly translate source speech into target text. In the cascaded condition, we investigate the effectiveness of different model architectures with robust training and achieve 2.72 BLEU improvements over last year’s optimal system on MuST-C English-German test set. In the end-to-end condition, we build models based on Transformer and Conformer architectures, achieving 2.26 BLEU improvements over last year’s optimal end-to-end system. The end-to-end system has obtained promising results, but it is still lagging behind our cascaded models.",
  "keywords": [
    "transformer",
    "end",
    "2 72 bleu improvements",
    "model",
    "text",
    "it",
    "bleu",
    "we",
    "speech translation",
    "2 26 bleu improvements",
    "training",
    "translation",
    "speech",
    "promising",
    "the effectiveness"
  ],
  "url": "https://aclanthology.org/2022.iwslt-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 08:43:39",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}