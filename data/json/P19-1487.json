{
  "id": "P19-1487",
  "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
  "authors": [
    "Rajani, Nazneen Fatema  and\nMcCann, Bryan  and\nXiong, Caiming  and\nSocher, Richard"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.",
  "keywords": [
    "deep learning models",
    "the challenging commonsenseqa task",
    "deep",
    "knowledge",
    "language",
    "commonsenseqa",
    "natural",
    "human",
    "language models",
    "information",
    "e",
    "form",
    "we",
    "learning",
    "transfer"
  ],
  "url": "https://aclanthology.org/P19-1487/",
  "provenance": {
    "collected_at": "2025-06-05 00:42:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}