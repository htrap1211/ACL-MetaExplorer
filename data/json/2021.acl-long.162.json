{
  "id": "2021.acl-long.162",
  "title": "Weight Distillation: Transferring the Knowledge in Neural Network Parameters",
  "authors": [
    "Lin, Ye  and\nLi, Yanyang  and\nWang, Ziyang  and\nLi, Bei  and\nDu, Quan  and\nXiao, Tong  and\nZhu, Jingbo"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Knowledge distillation has been proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the large neural networks, e.g., parameters. Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive BLEU performance. When fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points.",
  "keywords": [
    "knowledge",
    "neural",
    "machine",
    "the large neural networks",
    "it",
    "competitive bleu performance",
    "model",
    "a large neural network",
    "bleu",
    "-",
    "wmt14 en-de machine translation",
    "the small neural network",
    "network",
    "en",
    "we"
  ],
  "url": "https://aclanthology.org/2021.acl-long.162/",
  "provenance": {
    "collected_at": "2025-06-05 08:01:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}