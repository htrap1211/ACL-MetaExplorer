{
  "id": "2021.acl-short.48",
  "title": "On Orthogonality Constraints for Transformers",
  "authors": [
    "Zhang, Aston  and\nChan, Alvin  and\nTay, Yi  and\nFu, Jie  and\nWang, Shuohang  and\nZhang, Shuai  and\nShao, Huajie  and\nYao, Shuochao  and\nLee, Roy Ka-Wei"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. For example, on the large-scale WMT’16 En→De benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (Vaswani et al., 2017) increases the BLEU from 28.4 to 29.6, coming close to the 29.7 BLEU achieved by the very competitive dynamic convolution (Wu et al., 2019).",
  "keywords": [
    "the bleu",
    "transformer",
    "transformers",
    "processing",
    "convolutional neural networks",
    "language",
    "generation",
    "al",
    "neural",
    "natural",
    "ten machine translation tasks",
    "model",
    "machine",
    "convolution",
    "bleu"
  ],
  "url": "https://aclanthology.org/2021.acl-short.48/",
  "provenance": {
    "collected_at": "2025-06-05 08:07:45",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}