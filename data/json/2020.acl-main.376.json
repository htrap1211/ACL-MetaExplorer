{
  "id": "2020.acl-main.376",
  "title": "Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing",
  "authors": [
    "Fern{\\'a}ndez-Gonz{\\'a}lez, Daniel  and\nG{\\'o}mez-Rodr{\\'i}guez, Carlos"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)â€™s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers. Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.",
  "keywords": [
    "parsing",
    "model",
    "deterministic attention mechanisms",
    "attention",
    "sequence",
    "we",
    "accuracy",
    "the best accuracy",
    "constituent",
    "date",
    "approach",
    "transition",
    "state",
    "the-art",
    "sequences"
  ],
  "url": "https://aclanthology.org/2020.acl-main.376/",
  "provenance": {
    "collected_at": "2025-06-05 07:47:15",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}