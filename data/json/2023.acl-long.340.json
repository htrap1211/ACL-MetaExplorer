{
  "id": "2023.acl-long.340",
  "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
  "authors": [
    "Sharma, Mandar  and\nMuralidhar, Nikhil  and\nRamakrishnan, Naren"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information-theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data.",
  "keywords": [
    "field",
    "we",
    "llm",
    "training",
    "the field",
    "it",
    "information",
    "learning",
    "linguistic proficiency",
    "core",
    "llms",
    "their linguistic capabilities",
    "work",
    "proficiency",
    "knowledge"
  ],
  "url": "https://aclanthology.org/2023.acl-long.340/",
  "provenance": {
    "collected_at": "2025-06-05 09:40:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}