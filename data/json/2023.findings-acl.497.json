{
  "id": "2023.findings-acl.497",
  "title": "Benchmarking Diverse-Modal Entity Linking with Generative Models",
  "authors": [
    "Wang, Sijia  and\nLi, Alexander Hanbo  and\nZhu, Henghui  and\nZhang, Sheng  and\nPerera, Pramuditha  and\nHang, Chung-Wei  and\nMa, Jie  and\nWang, William Yang  and\nWang, Zhiguo  and\nCastelli, Vittorio  and\nXiang, Bing  and\nNg, Patrick"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training GDMM with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenge of DMEL, facilitating future researches on this task.",
  "keywords": [
    "we",
    "training",
    "it",
    "inference fine-tuning gdmm",
    "unified",
    "decoder",
    "rich",
    "visual",
    "generative",
    "a unified model",
    "text",
    "generative models entities",
    "all three modalities",
    "model",
    "entities"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.497/",
  "provenance": {
    "collected_at": "2025-06-05 10:15:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}