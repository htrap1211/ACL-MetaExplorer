{
  "id": "2020.acl-main.315",
  "title": "Enhancing Pre-trained {C}hinese Character Representation with Word-aligned Attention",
  "authors": [
    "Li, Yanzeng  and\nYu, Bowen  and\nMengge, Xue  and\nLiu, Tingwen"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Most Chinese pre-trained models take character as the basic unit and learn representation according to characterâ€™s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models. Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion. As a result, word and character information are explicitly integrated at the fine-tuning procedure. Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.",
  "keywords": [
    "tuning",
    "language",
    "a novel word-aligned attention",
    "nlp",
    "bert",
    "bert-wwm",
    "semantics",
    "the semantics",
    "information",
    "propagation",
    "ernie",
    "bert ernie",
    "attention",
    "word",
    "we"
  ],
  "url": "https://aclanthology.org/2020.acl-main.315/",
  "provenance": {
    "collected_at": "2025-06-05 07:46:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}