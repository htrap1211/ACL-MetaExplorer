{
  "id": "2022.acl-long.82",
  "title": "FORTAP}: Using Formulas for Numerical-Reasoning-Aware Table Pretraining",
  "authors": [
    "Cheng, Zhoujun  and\nDong, Haoyu  and\nJia, Ran  and\nWu, Pengfei  and\nHan, Shi  and\nCheng, Fan  and\nZhang, Dongmei"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is a valuable supervision for numerical reasoning in tables. Considering large amounts of spreadsheets available on the web, we propose FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining. Two novel self-supervised pretraining objectives are derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP). While our proposed objectives are generic for encoders, to better capture spreadsheet table layouts and structures, FORTAP is built upon TUTA, the first transformer-based method for spreadsheet table pretraining with tree attention. FORTAP outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction, question answering, and cell type classification, showing the great potential of leveraging formulas for table pretraining.",
  "keywords": [
    "objectives",
    "our proposed objectives",
    "question",
    "we",
    "classification",
    "tree attention fortap",
    "self",
    "rich",
    "answering",
    "generic",
    "the first transformer-based method",
    "encoders",
    "reference",
    "transformer",
    "language"
  ],
  "url": "https://aclanthology.org/2022.acl-long.82/",
  "provenance": {
    "collected_at": "2025-06-05 08:25:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}