{
  "id": "2023.acl-short.109",
  "title": "Class-Incremental Learning based on Label Generation",
  "authors": [
    "Shao, Yijia  and\nGuo, Yiduo  and\nZhao, Dongyan  and\nLiu, Bing"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.",
  "keywords": [
    "generation",
    "language",
    "pre-trained language models",
    "it",
    "class",
    "semantics",
    "the generation",
    "generalizable",
    "we",
    "learning",
    "the generalizable representations",
    "label generation",
    "pre",
    "label semantics experimental results",
    "that"
  ],
  "url": "https://aclanthology.org/2023.acl-short.109/",
  "provenance": {
    "collected_at": "2025-06-05 09:49:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}