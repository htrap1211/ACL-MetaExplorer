{
  "id": "2021.acl-long.23",
  "title": "Multi-Head Highly Parallelized {LSTM} Decoder for Neural Machine Translation",
  "authors": [
    "Xu, Hongfei  and\nLiu, Qiuhui  and\nvan Genabith, Josef  and\nXiong, Deyi  and\nZhang, Meng"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network isO(n2), increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.",
  "keywords": [
    "the standard lstm",
    "the current lstm state",
    "hplstms",
    "neural machine translation",
    "bleu",
    "the self-attention network",
    "we",
    "lstm",
    "current",
    "full lstm context modelling",
    "training",
    "translation",
    "bag",
    "neural",
    "several small hplstms"
  ],
  "url": "https://aclanthology.org/2021.acl-long.23/",
  "provenance": {
    "collected_at": "2025-06-05 07:59:44",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}