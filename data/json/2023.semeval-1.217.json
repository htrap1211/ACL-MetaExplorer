{
  "id": "2023.semeval-1.217",
  "title": "LCT}-1 at {S}em{E}val-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification",
  "authors": [
    "Chernyshev, Konstantin  and\nGaranina, Ekaterina  and\nBayram, Duygu  and\nZheng, Qiankun  and\nEdman, Lukas"
  ],
  "year": "2023",
  "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
  "abstract": "Misogyny and sexism are growing problems in social media. Advances have been made in online sexism detection but the systems are often uninterpretable. SemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at increasing explainability of the sexism detection, and our team participated in all the proposed subtasks. Our system is based on further domain-adaptive pre-training. Building on the Transformer-based models with the domain adaptation, we compare fine-tuning with multi-task learning and show that each subtask requires a different system configuration. In our experiments, multi-task learning performs on par with standard fine-tuning for sexism detection and noticeably better for coarse-grained sexism classification, while fine-tuning is preferable for fine-grained classification.",
  "keywords": [
    "coarse-grained sexism classification",
    "transformer",
    "tuning",
    "standard fine-tuning",
    "fine-grained classification",
    "par",
    "further domain-adaptive pre-training building",
    "classification misogyny",
    "the transformer-based models",
    "fine",
    "we",
    "learning",
    "pre",
    "training",
    "classification"
  ],
  "url": "https://aclanthology.org/2023.semeval-1.217/",
  "provenance": {
    "collected_at": "2025-06-05 10:29:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}