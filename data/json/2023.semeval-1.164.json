{
  "id": "2023.semeval-1.164",
  "title": "Silp{\\_}nlp at {S}em{E}val-2023 Task 2: Cross-lingual Knowledge Transfer for Mono-lingual Learning",
  "authors": [
    "Singh, Sumit  and\nTiwary, Uma"
  ],
  "year": "2023",
  "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
  "abstract": "Our team silp_nlp participated in SemEval2023 Task 2: MultiCoNER II. Our work made systems for 11 mono-lingual tracks. For leveraging the advantage of all track knowledge we chose transformer-based pretrained models, which have strong cross-lingual transferability. Hence our model trained in two stages, the first stage for multi-lingual learning from all tracks and the second for fine-tuning individual tracks. Our work highlights that the knowledge of all tracks can be transferred to an individual track if the baseline language model has crosslingual features. Our system positioned itself in the top 10 for 4 tracks by scoring 0.7432 macro F1 score for the Hindi track ( 7th rank ) and 0.7322 macro F1 score for the Bangla track ( 9th rank ).",
  "keywords": [
    "work",
    "cross",
    "transformer",
    "knowledge",
    "language",
    "nlp",
    "model",
    "the baseline language model",
    "multiconer",
    "silp_nlp",
    "we",
    "transfer",
    "learning",
    "transformer-based pretrained models",
    "all tracks"
  ],
  "url": "https://aclanthology.org/2023.semeval-1.164/",
  "provenance": {
    "collected_at": "2025-06-05 10:28:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}