{
  "id": "2024.arabicnlp-1.95",
  "title": "MGKM} at {S}tance{E}val2024 Fine-Tuning Large Language Models for {A}rabic Stance Detection",
  "authors": [
    "Alghaslan, Mamoun  and\nAlmutairy, Khaled"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "Social media platforms have become essential in daily life, enabling users to express their opinions and stances on various topics. Stance detection, which identifies the viewpoint expressed in text toward a target, has predominantly focused on English. MAWQIF is the pioneering Arabic dataset for target-specific stance detection, consisting of 4,121 tweets annotated with stance, sentiment, and sarcasm. The original dataset, benchmarked on four BERT-based models, achieved a best macro-F1 score of 78.89, indicating significant room for improvement. This study evaluates the effectiveness of three Large Language Models (LLMs) in detecting target-specific stances in MAWQIF. The LLMs assessed are ChatGPT-3.5-turbo, Meta-Llama-3-8B-Instruct, and Falcon-7B-Instruct. Performance was measured using both zero-shot and full fine-tuning approaches. Our findings demonstrate that fine-tuning substantially enhances the stance detection capabilities of LLMs in Arabic tweets. Notably, GPT-3.5-Turbo achieved the highest performance with a macro-F1 score of 82.93, underscoring the potential of fine-tuned LLMs for language-specific applications.",
  "keywords": [
    "a macro-f1 score",
    "fine-tuned llms",
    "a best macro-f1 score",
    "the llms",
    "shot",
    "fine-tuning",
    "four bert-based models",
    "llms",
    "tuning",
    "bert",
    "text",
    "fine",
    "viewpoint",
    "sentiment",
    "instruct"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.95/",
  "provenance": {
    "collected_at": "2025-06-05 11:03:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}