{
  "id": "2021.acl-short.56",
  "title": "Exploration and Exploitation: Two Ways to Improve {C}hinese Spelling Correction Models",
  "authors": [
    "Li, Chong  and\nZhang, Cenyuan  and\nZheng, Xiaoqing  and\nHuang, Xuanjing"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving state-of-the-art performance for CSC task.",
  "keywords": [
    "the generated adversarial examples",
    "the pre-training strategy",
    "neural",
    "neural networks",
    "model",
    "both the generalization",
    "generalization",
    "sequence",
    "we",
    "learning",
    "pre",
    "a task-specific pre-training strategy",
    "confusion",
    "training",
    "csc models"
  ],
  "url": "https://aclanthology.org/2021.acl-short.56/",
  "provenance": {
    "collected_at": "2025-06-05 08:07:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}