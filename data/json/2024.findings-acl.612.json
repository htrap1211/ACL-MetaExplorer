{
  "id": "2024.findings-acl.612",
  "title": "Debiasing Large Language Models with Structured Knowledge",
  "authors": [
    "Ma, Congda  and\nZhao, Tianyu  and\nOkumura, Manabu"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Due to biases inherently present in data for pre-training, current pre-trained Large Language Models (LLMs) also ubiquitously manifest the same phenomena. Since the bias influences the output from the LLMs across various tasks, the widespread deployment of the LLMs is hampered. We propose a simple method that utilizes structured knowledge to alleviate this issue, aiming to reduce the bias embedded within the LLMs and ensuring they have an encompassing perspective when used in applications. Experimental results indicated that our method has good debiasing ability when applied to existing both autoregressive and masked language models. Additionally, it could ensure that the performances of LLMs on downstream tasks remain uncompromised.Our method outperforms state-of-the-art (SOTA) baselines in the debiasing ability. Importantly, our method obviates the need for training from scratch, thus offering enhanced scalability and cost-effectiveness.",
  "keywords": [
    "the bias",
    "knowledge",
    "bias",
    "language",
    "it",
    "large language models",
    "the llms",
    "good debiasing ability",
    "we",
    "the debiasing ability",
    "pre",
    "current",
    "biases",
    "training",
    "llms"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.612/",
  "provenance": {
    "collected_at": "2025-06-05 10:56:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}