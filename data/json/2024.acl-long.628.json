{
  "id": "2024.acl-long.628",
  "title": "Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques",
  "authors": [
    "Yeginbergen, Anar  and\nOronoz, Maite  and\nAgerri, Rodrigo"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent research on sequence labelling has been exploring different strategies to mitigate the lack of manually annotated data for the large majority of the world languages. Among others, the most successful approaches have been based on (i) the crosslingual transfer capabilities of multilingual pre-trained language models (model-transfer), (ii) data translation and label projection (data-transfer) and (iii), prompt-based learning by reusing the mask objective to exploit the few-shot capabilities of pre-trained language models (few-shot). Previous work seems to conclude that model-transfer outperform data-transfer methods and that few-shot techniques based on prompting are superior to updating the modelâ€™s weights via fine-tuning. In this paper we empirically demonstrate that, for Argument Mining, a sequence labelling task which requires the detection of long and complex discourse structures, previous insights on crosslingual transfer or few-shot learning do not apply. Contrary to previous work, we show that for Argument Mining data-transfer obtains better results than model-transfer and that fine-tuning outperforms few-shot methods. Regarding the former, the domain of the dataset used for data-transfer seems to be a deciding factor, while, for few-shot, the type of task (length and complexity of the sequence spans) and sampling method proves to be crucial.",
  "keywords": [
    "few-shot learning",
    "we",
    "shot",
    "translation",
    "the crosslingual transfer capabilities",
    "fine-tuning",
    "cross",
    "pre-trained language models",
    "sequence",
    "learning",
    "transfer",
    "the mask objective",
    "few-shot techniques",
    "few-shot previous work",
    "different strategies"
  ],
  "url": "https://aclanthology.org/2024.acl-long.628/",
  "provenance": {
    "collected_at": "2025-06-05 10:42:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}