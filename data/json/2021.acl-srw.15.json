{
  "id": "2021.acl-srw.15",
  "title": "Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings",
  "authors": [
    "Inoue, Seiichi  and\nAida, Taichi  and\nKomachi, Mamoru  and\nAsai, Manabu"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
  "abstract": "In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed model exhibits a superior performance over the CSTM in terms of perplexity and convergence speed. Furthermore, extrinsic experimental results show that the proposed model is useful for a document classification task when compared with the baseline model. We qualitatively show that the latent coordinates obtained by training the proposed model are better than those of the baseline model.",
  "keywords": [
    "embeddings",
    "model",
    "text",
    "semantics",
    "the semantics",
    "a document classification task",
    "modeling",
    "pre-trained word embeddings",
    "topic",
    "perplexity",
    "perplexity and convergence speed",
    "latent",
    "train",
    "word",
    "we"
  ],
  "url": "https://aclanthology.org/2021.acl-srw.15/",
  "provenance": {
    "collected_at": "2025-06-05 08:09:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}