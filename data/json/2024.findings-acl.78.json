{
  "id": "2024.findings-acl.78",
  "title": "F}low{VQA}: Mapping Multimodal Logic in Visual Question Answering with Flowcharts",
  "authors": [
    "Singh, Shubhankar  and\nChaurasia, Purvi  and\nVarun, Yerram  and\nPandya, Pranshu  and\nGupta, Vatsal  and\nGupta, Vivek  and\nRoth, Dan"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language models in reasoning with flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and human-verified flowchart images from three distinct content sources, along with 22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks, including information localization, decision-making, and logical progression. We conduct a thorough baseline evaluation on a suite of both open-source and proprietary multimodal language models using various strategies, followed by an analysis of directional bias. The results underscore the benchmarkâ€™s potential as a vital tool for advancing the field of multimodal modeling, providing a focused and challenging environment for enhancing model performance in visual and logical reasoning tasks.",
  "keywords": [
    "bias",
    "field",
    "question",
    "we",
    "a thorough baseline evaluation",
    "answer",
    "the field",
    "information",
    "the capabilities",
    "visual",
    "flowvqa",
    "analysis",
    "strategies",
    "directional bias",
    "language"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.78/",
  "provenance": {
    "collected_at": "2025-06-05 10:49:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}