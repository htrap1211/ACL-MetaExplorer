{
  "id": "2024.findings-acl.542",
  "title": "Large Language Models Can Learn Representation in Natural Language",
  "authors": [
    "Guo, Yiduo  and\nLiang, Yaobo  and\nZhao, Dongyan  and\nDuan, Nan"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is obtaining accurate natural language representations for each entity to aid in retriever precision. In this paper, we propose the Natural Language Representation Optimization Problem, which aims to refine entity descriptions for improved retrieval and LLM utilization. We introduce the Learning to Represent with Natural Language method, which utilizes LLMs to optimize entity representations consisting of text patterns based on environmental feedback. We iteratively prompt LLMs to enhance or adjust patterns based on entity samples and evaluate their effectiveness through environmental feedback. Our method successfully learns human-understandable representations for classification tasks (e.g., instructions and documents) and API call tasks (e.g., APIbench and Virtual Home), significantly improving GPT-4â€™s task performance.",
  "keywords": [
    "precision",
    "feedback",
    "we",
    "llm",
    "retriever precision",
    "classification",
    "classification tasks",
    "natural",
    "relevant entities",
    "retrieval",
    "learning",
    "natural language",
    "llms",
    "text",
    "multiple entities"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.542/",
  "provenance": {
    "collected_at": "2025-06-05 10:56:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}