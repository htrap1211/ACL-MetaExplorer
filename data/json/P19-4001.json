{
  "id": "P19-4001",
  "title": "Latent Structure Models for Natural Language Processing",
  "authors": [
    "Martins, Andr{\\'e} F. T.  and\nMihaylova, Tsvetomila  and\nNangia, Nikita  and\nNiculae, Vlad"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts",
  "abstract": "Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines. They are appealing for two main reasons: they allow incorporating structural bias during training, leading to more accurate models; and they allow discovering hidden linguistic structure, which provides better interpretability. This tutorial will cover recent advances in discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: gradient approximation, reinforcement learning, and end-to-end differentiable methods. We highlight connections among all these methods, enumerating their strengths and weaknesses. The models we present and analyze have been applied to a wide variety of NLP tasks, including sentiment analysis, natural language inference, language modeling, machine translation, and semantic parsing. Examples and evaluation will be covered throughout. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem.",
  "keywords": [
    "variety",
    "bias",
    "parsing",
    "end",
    "structural bias",
    "a practitioner",
    "semantic",
    "we",
    "nlp pipelines",
    "a wide variety",
    "training",
    "translation",
    "natural",
    "latent",
    "learning"
  ],
  "url": "https://aclanthology.org/P19-4001/",
  "provenance": {
    "collected_at": "2025-06-05 00:50:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}