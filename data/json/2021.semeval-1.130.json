{
  "id": "2021.semeval-1.130",
  "title": "Y}oung{S}heldon at {S}em{E}val-2021 Task 5: Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective",
  "authors": [
    "Sharma, Mayukh  and\nKandasamy, Ilanthenral  and\nVasantha, W.b."
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "In this paper, we describe our system used for SemEval 2021 Task 5: Toxic Spans Detection. Our proposed system approaches the problem as a token classification task. We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence. We fine-tuned Pre-trained Language Models (PLMs) for identifying the toxic words. For fine-tuning, we stacked the classification layer on top of the PLM features of each word to classify if it is toxic or not. PLMs are pre-trained using different objectives and their performance may differ on downstream tasks. We, therefore, compare the performance of BERT, ELECTRA, RoBERTa, XLM-RoBERTa, T5, XLNet, and MPNet for identifying toxic spans within a sentence. Our best performing system used RoBERTa. It performed well, achieving an F1 score of 0.6841 and secured a rank of 16 on the official leaderboard.",
  "keywords": [
    "top",
    "roberta",
    "objectives",
    "layer",
    "we",
    "classification",
    "fine-tuning",
    "it",
    "token",
    "token classification objective",
    "word",
    "tuning",
    "bert",
    "objective",
    "fine"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.130/",
  "provenance": {
    "collected_at": "2025-06-05 08:21:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}