{
  "id": "2024.repl4nlp-1.5",
  "title": "Prior Knowledge-Guided Adversarial Training",
  "authors": [
    "Pereira, Lis  and\nCheng, Fei  and\nShe, Wan Jou  and\nAsahara, Masayuki  and\nKobayashi, Ichiro"
  ],
  "year": "2024",
  "venue": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
  "abstract": "We introduce a simple yet effective Prior Knowledge-Guided ADVersarial Training (PKG-ADV) algorithm to improve adversarial training for natural language understanding. Our method simply utilizes task-specific label distribution to guide the training process. By prioritizing the use of prior knowledge of labels, we aim to generate more informative adversarial perturbations. We apply our model to several challenging temporal reasoning tasks. Our method enables a more reliable and controllable data training process than relying on randomized adversarial perturbation. Albeit simple, our method achieved significant improvements in these tasks. To facilitate further research, we will release the code and models.",
  "keywords": [
    "code",
    "process",
    "knowledge",
    "language",
    "natural",
    "model",
    "pkg",
    "we",
    "natural language",
    "training",
    "prior knowledge-guided adversarial training",
    "task-specific label distribution",
    "prior knowledge",
    "reliable",
    "task"
  ],
  "url": "https://aclanthology.org/2024.repl4nlp-1.5/",
  "provenance": {
    "collected_at": "2025-06-05 11:09:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}