{
  "id": "2020.bionlp-1.2",
  "title": "Sequence-to-Set Semantic Tagging for Complex Query Reformulation and Automated Text Categorization in Biomedical {IR} using Self-Attention",
  "authors": [
    "Das, Manirupa  and\nLi, Juanxi  and\nFosler-Lussier, Eric  and\nLin, Simon  and\nRust, Steve  and\nHuang, Yungui  and\nRamnath, Rajiv"
  ],
  "year": "2020",
  "venue": "Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing",
  "abstract": "Novel contexts, comprising a set of terms referring to one or more concepts, may often arise in complex querying scenarios such as in evidence-based medicine (EBM) involving biomedical literature. These may not explicitly refer to entities or canonical concept forms occurring in a fact-based knowledge source, e.g. the UMLS ontology. Moreover, hidden associations between related concepts meaningful in the current context, may not exist within a single document, but across documents in the collection. Predicting semantic concept tags of documents can therefore serve to associate documents related in unseen contexts, or categorize them, in information filtering or retrieval scenarios. Thus, inspired by the success of sequence-to-sequence neural models, we develop a novel sequence-to-set framework with attention, for learning document representations in a unique unsupervised setting, using no human-annotated document labels or external knowledge resources and only corpus-derived term statistics to drive the training, that can effect term transfer within a corpus for semantically tagging a large collection of documents. Our sequence-to-set modeling approach to predict semantic tags, gives to the best of our knowledge, the state-of-the-art for both, an unsupervised query expansion (QE) task for the TREC CDS 2016 challenge dataset when evaluated on an Okapi BM25â€“based document retrieval system; and also over the MLTM system baseline baseline (Soleimani and Miller, 2016), for both supervised and semi-supervised multi-label prediction tasks on the del.icio.us and Ohsumed datasets. We make our code and data publicly available.",
  "keywords": [
    "code",
    "semantic",
    "we",
    "self-attention novel",
    "the umls ontology",
    "current",
    "training",
    "neural",
    "self",
    "retrieval",
    "information",
    "sequence-to-set semantic tagging",
    "sequence",
    "transfer",
    "semantic tags"
  ],
  "url": "https://aclanthology.org/2020.bionlp-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 07:54:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}