{
  "id": "2024.acl-long.105",
  "title": "Towards Faithful and Robust {LLM} Specialists for Evidence-Based Question-Answering",
  "authors": [
    "Schimanski, Tobias  and\nNi, Jingwei  and\nKraus, Mathias  and\nAsh, Elliott  and\nLeippold, Markus"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.",
  "keywords": [
    "work",
    "tuning",
    "language",
    "generation",
    "evidence-based qa",
    "extensive evaluation",
    "large language models llms",
    "question",
    "this evidence-based qa",
    "information",
    "fine",
    "we",
    "a data generation pipeline",
    "evaluation",
    "diversified"
  ],
  "url": "https://aclanthology.org/2024.acl-long.105/",
  "provenance": {
    "collected_at": "2025-06-05 10:35:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}