{
  "id": "2023.findings-acl.646",
  "title": "H}a{VQA}: A Dataset for Visual Question Answering and Multimodal Research in {H}ausa Language",
  "authors": [
    "Parida, Shantipriya  and\nAbdulmumin, Idris  and\nMuhammad, Shamsuddeen Hassan  and\nBose, Aneesh  and\nKohli, Guneet Singh  and\nAhmad, Ibrahim Said  and\nKotwal, Ketan  and\nDeb Sarkar, Sayan  and\nBojar, Ond{\\v{r}}ej  and\nKakudi, Habeebah"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "This paper presents “HaVQA”, the first multimodal dataset for visual question answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, the dataset provides 12,044 gold standard English-Hausa parallel sentences that were translated in a fashion that guarantees their semantic match with the corresponding visual information. We conducted several baseline experiments on the dataset, including visual question answering, visual question elicitation, text-only and multimodal machine translation.",
  "keywords": [
    "answer",
    "language",
    "translation",
    "machine",
    "text",
    "vqa tasks",
    "question",
    "information",
    "semantic",
    "we",
    "visual",
    "vqa",
    "havqa",
    "their semantic match",
    "that"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.646/",
  "provenance": {
    "collected_at": "2025-06-05 10:17:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}