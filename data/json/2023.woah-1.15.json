{
  "id": "2023.woah-1.15",
  "title": "``Female Astronaut: Because sandwiches won{'}t make themselves up there'': Towards Multimodal misogyny detection in memes",
  "authors": [
    "Singh, Smriti  and\nHaridasan, Amritha  and\nMooney, Raymond"
  ],
  "year": "2023",
  "venue": "The 7th Workshop on Online Abuse and Harms (WOAH)",
  "abstract": "A rise in the circulation of memes has led to the spread of a new form of multimodal hateful content. Unfortunately, the degree of hate women receive on the internet is disproportionately skewed against them. This, combined with the fact that multimodal misogyny is more challenging to detect as opposed to traditional text-based misogyny, signifies that the task of identifying misogynistic memes online is one of utmost importance. To this end, the MAMI dataset was released, consisting of 12000 memes annotated for misogyny and four sub-classes of misogyny - shame, objectification, violence and stereotype. While this balanced dataset is widely cited, we find that the task itself remains largely unsolved. Thus, in our work, we investigate the performance of multiple models in an effort to analyse whether domain specific pretraining helps model performance. We also investigate why even state of the art models find this task so challenging, and whether domain-specific pretraining can help. Our results show that pretraining BERT on hateful memes and leveraging an attention based approach with ViT outperforms state of the art models by more than 10%. Further, we provide insight into why these models may be struggling with this task with an extensive qualitative analysis of random samples from the test set.",
  "keywords": [
    "t",
    "end",
    "form",
    "we",
    "an attention",
    "analysis",
    "bert",
    "text",
    "-",
    "work",
    "random",
    "model",
    "attention",
    "hateful memes",
    "approach"
  ],
  "url": "https://aclanthology.org/2023.woah-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 10:34:07",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}