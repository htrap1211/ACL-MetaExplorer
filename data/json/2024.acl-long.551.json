{
  "id": "2024.acl-long.551",
  "title": "M}uggle{M}ath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
  "authors": [
    "Li, Chengpeng  and\nYuan, Zheng  and\nYuan, Hongyi  and\nDong, Guanting  and\nLu, Keming  and\nWu, Jiancan  and\nTan, Chuanqi  and\nWang, Xiang  and\nZhou, Chang"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks?To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH.We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH.A log-linear relationship and a segmented log-linear are presented between MuggleMathâ€™s performance and the amount of augmented data on GSM8K and MATH, respectively.We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization.",
  "keywords": [
    "cutting-edge proprietary llms",
    "end",
    "fine-tuning llama models",
    "series",
    "we",
    "edge",
    "it",
    "queries",
    "log",
    "open-sourced llms",
    "llms",
    "strategies",
    "generalization",
    "the queries",
    "fine-tuning data augmentation"
  ],
  "url": "https://aclanthology.org/2024.acl-long.551/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}