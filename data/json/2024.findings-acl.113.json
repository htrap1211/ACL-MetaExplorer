{
  "id": "2024.findings-acl.113",
  "title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?",
  "authors": [
    "Yang, Yixin  and\nLi, Zheng  and\nDong, Qingxiu  and\nXia, Heming  and\nSui, Zhifang"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Modelsâ€™ (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision). Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis reveals that LMM performance on DEEPEVAL varies according to the specific facets of deep semantics explored, indicating the fundamental challenges remaining in developing LMMs.",
  "keywords": [
    "deep",
    "deep semantics",
    "gpt-4v",
    "semantic",
    "era",
    "we",
    "a notable deficiency",
    "current",
    "example gpt-4v",
    "semantics",
    "it",
    "lmms capacities",
    "deficiency",
    "the deep semantics",
    "visual"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.113/",
  "provenance": {
    "collected_at": "2025-06-05 10:50:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}