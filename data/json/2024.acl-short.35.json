{
  "id": "2024.acl-short.35",
  "title": "Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval",
  "authors": [
    "Coelho, Jo{\\~a}o  and\nMartins, Bruno  and\nMagalhaes, Joao  and\nCallan, Jamie  and\nXiong, Chenyan"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "This study investigates the existence of positional biases in Transformer-based language models for text representation learning, particularly in the context of web document retrieval. We build on previous research that demonstrated loss of information in the middle of input sequences for causal language models, extending it to the domain of embedding learning. We examine positional biases at multiple stages of the training pipeline for an encoder-decoder neural retrieval model, namely language model pre-training, contrastive pre-training, and contrastive fine-tuning. Experiments with the MS-MARCO document collection reveal that after contrastive pre-training the model already generates embeddings that better capture the beginning of the input content, with fine-tuning further aggravating this effect.",
  "keywords": [
    "embeddings",
    "transformer",
    "language",
    "neural",
    "web document retrieval",
    "positional biases",
    "model",
    "embedding learning",
    "causal language models",
    "text",
    "it",
    "language models",
    "encoder",
    "decoder",
    "loss"
  ],
  "url": "https://aclanthology.org/2024.acl-short.35/",
  "provenance": {
    "collected_at": "2025-06-05 10:46:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}