{
  "id": "2022.acl-short.37",
  "title": "Hierarchical Curriculum Learning for {AMR} Parsing",
  "authors": [
    "Wang, Peiyi  and\nChen, Liang  and\nLiu, Tianyu  and\nDai, Damai  and\nCao, Yunbo  and\nChang, Baobao  and\nSui, Zhifang"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Abstract Meaning Representation (AMR) parsing aims to translate sentences to semantic representation with a hierarchical structure, and is recently empowered by pretrained sequence-to-sequence models. However, there exists a gap between their flat training objective (i.e., equally treats all output tokens) and the hierarchical AMR structure, which limits the model generalization. To bridge this gap, we propose a Hierarchical Curriculum Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula (IC). SC switches progressively from core to detail AMR semantic elements while IC transits from structure-simple to -complex AMR instances during training. Through these two warming-up processes, HCL reduces the difficulty of learning complex structures, thus the flat model can better adapt to the AMR hierarchy. Extensive experiments on AMR2.0, AMR3.0, structure-complex and out-of-distribution situations verify the effectiveness of HCL.",
  "keywords": [
    "hierarchical",
    "amr semantic elements",
    "hierarchical curriculum",
    "parsing",
    "i",
    "their flat training objective",
    "a hierarchical structure",
    "the hierarchical amr structure",
    "model",
    "objective",
    "the model generalization",
    "a hierarchical curriculum",
    "sc",
    "e",
    "generalization"
  ],
  "url": "https://aclanthology.org/2022.acl-short.37/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:56",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}