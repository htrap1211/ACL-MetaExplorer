{
  "id": "2024.findings-acl.772",
  "title": "I}n{F}o{B}ench: Evaluating Instruction Following Ability in Large Language Models",
  "authors": [
    "Qin, Yiwei  and\nSong, Kaiqiang  and\nHu, Yebowen  and\nYao, Wenlin  and\nCho, Sangwoo  and\nWang, Xiaoyang  and\nWu, Xuansheng  and\nLiu, Fei  and\nLiu, Pengfei  and\nYu, Dong"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "This paper introduces the Decomposed Requirements Following Ratio (DRFR), a new metric for evaluating Large Language Models’ (LLMs) ability to follow instructions. Addressing a gap in current methodologies, DRFR breaks down complex instructions into simpler criteria, facilitating a detailed analysis of LLMs’ compliance with various aspects of tasks. Alongside this metric, we present InFoBench, a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories. Our experiments compare DRFR with traditional scoring methods and explore annotation sources, including human experts, crowd-sourced workers, and GPT-4. The findings demonstrate DRFR’s higher reliability and the effectiveness of using GPT-4 as a cost-efficient annotator. The evaluation of several advanced LLMs using this framework reveals their strengths and areas needing improvement, particularly in complex instruction-following. This study contributes a novel metric and benchmark, offering insights for future LLM development and evaluation.",
  "keywords": [
    "efficient",
    "we",
    "current",
    "llm",
    "several advanced llms",
    "instruction",
    "analysis",
    "a cost-efficient annotator",
    "llms",
    "i",
    "categories",
    "metric",
    "gpt-4",
    "current methodologies",
    "ratio"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.772/",
  "provenance": {
    "collected_at": "2025-06-05 10:59:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}