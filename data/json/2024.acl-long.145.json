{
  "id": "2024.acl-long.145",
  "title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale",
  "authors": [
    "Hu, Xiang  and\nJi, Pengyu  and\nZhu, Qingyang  and\nWu, Wei  and\nTu, Kewei"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner.We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion.We pre-train GPST on OpenWebText, a corpus with billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation. Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training.",
  "keywords": [
    "transformers",
    "uni",
    "we",
    "training",
    "language generation",
    "it",
    "loss",
    "gpt-2",
    "manner",
    "generative",
    "generative pretrained structured transformers",
    "syntactic language models",
    "language",
    "generation",
    "model"
  ],
  "url": "https://aclanthology.org/2024.acl-long.145/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}