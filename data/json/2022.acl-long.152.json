{
  "id": "2022.acl-long.152",
  "title": "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis",
  "authors": [
    "Ling, Yan  and\nYu, Jianfei  and\nXia, Rui"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention inrecent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodalalignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify fine-grainedaspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-LanguagePre-training framework for MABSA (VLP-MABSA), which is a unified multimodal encoder-decoder architecture for all the pretrainingand downstream tasks. We further design three types of task-specific pre-training tasks from the language, vision, and multimodalmodalities, respectively. Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks. Further analysis demonstrates the effectiveness of each pre-training task. The source code is publicly released athttps://github.com/NUSTM/VLP-MABSA.",
  "keywords": [
    "code",
    "their alignments",
    "alignments",
    "a task-specific vision-languagepre-training framework",
    "each pre-training task",
    "we",
    "training",
    "unified",
    "decoder",
    "crossmodalalignment",
    "visual",
    "multimodal aspect-based sentiment analysis",
    "analysis",
    "general pre-training tasks",
    "i"
  ],
  "url": "https://aclanthology.org/2022.acl-long.152/",
  "provenance": {
    "collected_at": "2025-06-05 08:26:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}