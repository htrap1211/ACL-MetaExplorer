{
  "id": "2021.acl-long.196",
  "title": "KACE}: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference",
  "authors": [
    "Chen, Qianglong  and\nJi, Feng  and\nZeng, Xiangji  and\nLi, Feng-Lin  and\nZhang, Ji  and\nChen, Haiqing  and\nZhang, Yin"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "In order to better understand the reason behind model behaviors (i.e., making predictions), most recent works have exploited generative models to provide complementary explanations. However, existing approaches in NLP mainly focus on “WHY A” rather than contrastive “WHY A NOT B”, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields. In this paper, we focus on generating contrastive explanations with counterfactual examples in NLI and propose a novelKnowledge-AwareContrastiveExplanation generation framework (KACE).Specifically, we first identify rationales (i.e., key phrases) from input sentences, and use them as key perturbations for generating counterfactual examples. After obtaining qualified counterfactual examples, we take them along with original examples and external knowledge as input, and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations. Experimental results show that contrastive explanations are beneficial to fit the scenarios by clarifying the difference between the predicted answer and other possible wrong ones. Moreover, we train an NLI model enhanced with contrastive explanations and achieves an accuracy of 91.9% on SNLI, gaining improvements of 5.7% against ETPA (“Explain-Then-Predict-Attention”) and 0.6% against NILE (“WHY A”).",
  "keywords": [
    "qualified counterfactual examples",
    "generative models",
    "other research fields",
    "data efficiency",
    "efficiency",
    "we",
    "fields",
    "qualified",
    "answer",
    "natural",
    "a",
    "generative",
    "i",
    "an accuracy",
    "accuracy"
  ],
  "url": "https://aclanthology.org/2021.acl-long.196/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}