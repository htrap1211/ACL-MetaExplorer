{
  "id": "2022.acl-long.396",
  "title": "Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering",
  "authors": [
    "Zhang, Jing  and\nZhang, Xiaokang  and\nYu, Jifan  and\nTang, Jian  and\nTang, Jie  and\nLi, Cuiping  and\nChen, Hong"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent works on knowledge base question answering (KBQA) retrieve subgraphs for easier reasoning. The desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises. However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the reasoning bias when the intermediate supervision is missing. This paper proposes a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive experiments demonstrate SR achieves significantly better retrieval and QA performance than existing retrieval methods. Via weakly supervised pre-training as well as the end-to-end fine-tuning, SR achieves new state-of-the-art performance when combined with NSM (He et al., 2021), a subgraph-oriented reasoner, for embedding-based KBQA methods. Codes and datasets are available online (https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA)",
  "keywords": [
    "bias",
    "end",
    "retrieve",
    "question",
    "retriever sr",
    "kbqa",
    "training",
    "existing retrieval methods",
    "answer",
    "retrieval",
    "subgraph retrieval enhanced model",
    "subgraphretrievalkbqa",
    "significantly better retrieval",
    "tuning",
    "the reasoning bias"
  ],
  "url": "https://aclanthology.org/2022.acl-long.396/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}