{
  "id": "2021.acl-long.467",
  "title": "POS}-{C}onstrained {P}arallel {D}ecoding for {N}on-autoregressive {G}eneration",
  "authors": [
    "Yang, Kexin  and\nLei, Wenqiang  and\nLiu, Dayiheng  and\nQi, Weizhen  and\nLv, Jiancheng"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems. A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as “teacher AG”). The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. To provide a feasible solution to the multimodality problem of NAG, we propose incorporating linguistic structure (Part-of-Speech sequence in particular) into NAG inference instead of relying on teacher AG. More specifically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at providing a specific POS sequence to constrain the NAG model during decoding. Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation. This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.",
  "keywords": [
    "the text generation tasks",
    "text summarization",
    "eneration",
    "summarization",
    "we",
    "training",
    "latent",
    "sequence",
    "on-autoregressive g eneration",
    "i",
    "text",
    "pos",
    "autoregressive generation hereinafter",
    "work",
    "knowledge"
  ],
  "url": "https://aclanthology.org/2021.acl-long.467/",
  "provenance": {
    "collected_at": "2025-06-05 08:05:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}