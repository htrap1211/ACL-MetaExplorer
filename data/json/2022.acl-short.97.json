{
  "id": "2022.acl-short.97",
  "title": "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks",
  "authors": [
    "Wu, Xing  and\nGao, Chaochen  and\nLin, Meng  and\nZang, Liangjun  and\nHu, Songlin"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Before entering the neural network, a token needs to be converted to its one-hot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model, which can be seen as a more informative augmented substitution to the one-hot representation. We propose an efficient data augmentation method, dub as text smoothing, by converting a sentence from its one-hot representation to controllable smoothed representation. We evaluate text smoothing on different datasets in a low-resource regime. Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. Moreover, text smoothing can be combined with these data augmentation methods to achieve better performance.",
  "keywords": [
    "hot",
    "language",
    "neural",
    "model",
    "text",
    "efficient",
    "token",
    "the neural network",
    "network",
    "we",
    "text classification tasks",
    "pre",
    "classification",
    "candidate tokens",
    "text smoothing"
  ],
  "url": "https://aclanthology.org/2022.acl-short.97/",
  "provenance": {
    "collected_at": "2025-06-05 08:33:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}