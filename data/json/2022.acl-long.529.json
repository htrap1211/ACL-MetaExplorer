{
  "id": "2022.acl-long.529",
  "title": "Make the Best of Cross-lingual Transfer: Evidence from {POS} Tagging with over 100 Languages",
  "authors": [
    "de Vries, Wietse  and\nWieling, Martijn  and\nNissim, Malvina"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data. Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data, and test data in a selection of target languages. We explore a more extensive transfer learning setup with 65 different source languages and 105 target languages for part-of-speech tagging. Through our analysis, we show that pre-training of both source and target language, as well as matching language families, writing systems, word order systems, and lexical-phonetic distance significantly impact cross-lingual performance. The findings described in this paper can be used as indicators of which factors are important for effective zero-shot cross-lingual transfer to zero- and low-resource languages.",
  "keywords": [
    "cross",
    "families",
    "language",
    "generalisability",
    "zero-",
    "evaluations",
    "pos",
    "-",
    "effective zero-shot cross-lingual transfer",
    "tagging",
    "word",
    "we",
    "transfer",
    "language families",
    "pre"
  ],
  "url": "https://aclanthology.org/2022.acl-long.529/",
  "provenance": {
    "collected_at": "2025-06-05 08:31:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}