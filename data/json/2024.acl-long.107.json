{
  "id": "2024.acl-long.107",
  "title": "Self-Alignment for Factuality: Mitigating Hallucinations in {LLM}s via Self-Evaluation",
  "authors": [
    "Zhang, Xiaoying  and\nPeng, Baolin  and\nTian, Ye  and\nZhou, Jingyan  and\nJin, Lifeng  and\nSong, Linfeng  and\nMi, Haitao  and\nMeng, Helen"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Despite showing impressive abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e., ”hallucinations”, even when they hold relevant knowledge. To mitigate these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM’s self-evaluation ability by improving the model’s confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.",
  "keywords": [
    "inaccuracies",
    "an llm",
    "we",
    "current",
    "llm",
    "training",
    "self",
    "its own generated responses",
    "a self-evaluation component",
    "self-alignment",
    "direct preference optimization algorithm",
    "llms",
    "abilities",
    "self-evaluation",
    "tuning"
  ],
  "url": "https://aclanthology.org/2024.acl-long.107/",
  "provenance": {
    "collected_at": "2025-06-05 10:35:45",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}