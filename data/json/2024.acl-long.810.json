{
  "id": "2024.acl-long.810",
  "title": "C}hat{D}ev: Communicative Agents for Software Development",
  "authors": [
    "Qian, Chen  and\nLiu, Wei  and\nLiu, Hongzhang  and\nChen, Nuo  and\nDang, Yufan  and\nLi, Jiahao  and\nYang, Cheng  and\nChen, Weize  and\nSu, Yusheng  and\nCong, Xin  and\nXu, Juyuan  and\nLi, Dahai  and\nLiu, Zhiyuan  and\nSun, Maosong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.",
  "keywords": [
    "deep",
    "code",
    "chain",
    "we",
    "llm agents",
    "llm",
    "natural",
    "numerous studies",
    "their multi-turn dialogues",
    "unified",
    "learning",
    "unified language-based communication",
    "natural language",
    "llms",
    "large language models llms"
  ],
  "url": "https://aclanthology.org/2024.acl-long.810/",
  "provenance": {
    "collected_at": "2025-06-05 10:45:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}