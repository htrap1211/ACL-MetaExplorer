{
  "id": "2024.findings-acl.810",
  "title": "Efficient Domain Adaptation for Non-Autoregressive Machine Translation",
  "authors": [
    "You, WangJie  and\nGuo, Pei  and\nLi, Juntao  and\nChen, Kehai  and\nZhang, Min"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Domain adaptation remains a challenge in the realm of Neural Machine Translation (NMT), even in the era of large language models (LLMs). Existing non-parametric approaches like nearest neighbor machine translation have made small Autoregressive Translation (AT) models achieve efficient domain generalization and adaptation without updating parameters, but leaving the Non-Autoregressive Translation (NAT) counterparts under-explored. To fill this blank, we introduceBi-kNN, an innovative and efficient domain adaptation approach for NAT models that tailors a k-nearest-neighbor algorithm for NAT. Specifically, we introduce an effective datastore construction and correlated updating strategies to conform the parallel nature of NAT. Additionally, we train a meta-network that seamlessly integrates the NN distribution with the NMT distribution robustly during the iterative decoding process of NAT. Our experimental results across four benchmark datasets demonstrate that ourBi-kNN not only achieves significant improvements over the Base-NAT model (7.8 BLEU on average) but also exhibits enhanced efficiency.",
  "keywords": [
    "nearest neighbor machine translation",
    "7 8 bleu",
    "the non-autoregressive translation",
    "efficient",
    "bleu",
    "efficiency",
    "era",
    "we",
    "translation",
    "efficient domain generalization",
    "neural",
    "nat",
    "enhanced efficiency",
    "strategies",
    "small autoregressive translation"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.810/",
  "provenance": {
    "collected_at": "2025-06-05 10:59:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}