{
  "id": "W19-3812",
  "title": "Transfer Learning from Pre-trained {BERT} for Pronoun Resolution",
  "authors": [
    "Bao, Xingce  and\nQiao, Qianqian"
  ],
  "year": "2019",
  "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
  "abstract": "The paper describes the submission of the team “We used bert!” to the shared task Gendered Pronoun Resolution (Pair pronouns to their correct entities). Our final submission model based on the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) ranks 14th among 838 teams with a multi-class logarithmic loss of 0.208. In this work, contribution of transfer learning technique to pronoun resolution systems is investigated and the gender bias contained in classification models is evaluated.",
  "keywords": [
    "work",
    "classification models",
    "transformers",
    "bias",
    "transfer learning technique",
    "bert",
    "model",
    "entities",
    "class",
    "encoder",
    "pre-trained bert",
    "loss",
    "the gender bias",
    "we",
    "transfer"
  ],
  "url": "https://aclanthology.org/W19-3812/",
  "provenance": {
    "collected_at": "2025-06-05 00:59:44",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}