{
  "id": "2023.acl-long.846",
  "title": "Exploring Large Language Models for Classical Philology",
  "authors": [
    "Riemenschneider, Frederick  and\nFrank, Anette"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5â€™s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.",
  "keywords": [
    "ancient",
    "roberta",
    "ancient greek results",
    "we",
    "training",
    "ancient greek",
    "a monolingual ancient greek",
    "decoder",
    "powerful language models",
    "analysis",
    "abilities",
    "generative",
    "i",
    "bert",
    "language models"
  ],
  "url": "https://aclanthology.org/2023.acl-long.846/",
  "provenance": {
    "collected_at": "2025-06-05 09:47:15",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}