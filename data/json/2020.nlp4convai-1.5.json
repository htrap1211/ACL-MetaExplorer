{
  "id": "2020.nlp4convai-1.5",
  "title": "Efficient Intent Detection with Dual Sentence Encoders",
  "authors": [
    "Casanueva, I{\\~n}igo  and\nTem{\\v{c}}inas, Tadas  and\nGerz, Daniela  and\nHenderson, Matthew  and\nVuli{\\'c}, Ivan"
  ],
  "year": "2020",
  "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI",
  "abstract": "Building conversational systems in new domains and with added functionality requires resource-efficient models that work under low-data regimes (i.e., in few-shot setups). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT. We demonstrate the usefulness and wide applicability of the proposed intent detectors, showing that: 1) they outperform intent detectors based on fine-tuning the full BERT-Large model or using BERT as a fixed black-box encoder on three diverse intent detection data sets; 2) the gains are especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated examples per intent); 3) our intent detectors can be trained in a matter of minutes on a single CPU; and 4) they are stable across different hyperparameter settings. In hope of facilitating and democratizing research focused on intention detection, we release our code, as well as a new challenging single-domain intent detection dataset comprising 13,083 annotated examples over 77 intents.",
  "keywords": [
    "code",
    "resource-efficient models",
    "few-shot setups",
    "efficient",
    "we",
    "shot",
    "dual sentence encoders",
    "e",
    "i",
    "bert",
    "pretrained dual sentence encoders",
    "different hyperparameter settings",
    "encoders",
    "conversational systems",
    "the full bert-large model"
  ],
  "url": "https://aclanthology.org/2020.nlp4convai-1.5/",
  "provenance": {
    "collected_at": "2025-06-05 07:57:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}