{
  "id": "2024.acl-long.782",
  "title": "Exploring Collaboration Mechanisms for {LLM} Agents: A Social Psychology View",
  "authors": [
    "Zhang, Jintian  and\nXu, Xin  and\nZhang, Ningyu  and\nLiu, Ruibo  and\nHooi, Bryan  and\nDeng, Shumin"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: *Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)?* This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets, hoping to catalyze further research in this promising avenue.",
  "keywords": [
    "code",
    "a social psychology view",
    "theories",
    "tier",
    "efficiency",
    "we",
    "llm agents",
    "certain collaborative strategies",
    "llm",
    "a multi-agent society",
    "natural",
    "contemporary nlp systems",
    "multiple large language models",
    "llms",
    "processing"
  ],
  "url": "https://aclanthology.org/2024.acl-long.782/",
  "provenance": {
    "collected_at": "2025-06-05 10:45:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}