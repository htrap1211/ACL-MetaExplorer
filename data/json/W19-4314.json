{
  "id": "W19-4314",
  "title": "Constructive Type-Logical Supertagging With Self-Attention Networks",
  "authors": [
    "Kogkalidis, Konstantinos  and\nMoortgat, Michael  and\nDeoskar, Tejaswini"
  ],
  "year": "2019",
  "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
  "abstract": "We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammarâ€™s type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.",
  "keywords": [
    "accuracy",
    "knowledge",
    "model",
    "semantics",
    "self",
    "generalization",
    "attention",
    "we",
    "word",
    "the syntax",
    "an attention-based supertagger",
    "syntax",
    "its adequate accuracy",
    "training",
    "self-attention networks"
  ],
  "url": "https://aclanthology.org/W19-4314/",
  "provenance": {
    "collected_at": "2025-06-05 01:03:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}