{
  "id": "2023.acl-long.572",
  "title": "Local Interpretation of Transformer Based on Linear Decomposition",
  "authors": [
    "Yang, Sen  and\nHuang, Shujian  and\nZou, Wei  and\nZhang, Jianbing  and\nDai, Xinyu  and\nChen, Jiajun"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks.",
  "keywords": [
    "our evaluation",
    "deep",
    "transformer",
    "work",
    "neural",
    "neural networks",
    "model",
    "machine",
    "deep neural networks dnns",
    "efficiency",
    "we",
    "evaluation",
    "relu",
    "analysis",
    "the relu-activated transformer"
  ],
  "url": "https://aclanthology.org/2023.acl-long.572/",
  "provenance": {
    "collected_at": "2025-06-05 09:43:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}