{
  "id": "2022.acl-long.319",
  "title": "Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System",
  "authors": [
    "Su, Yixuan  and\nShu, Lei  and\nMansimov, Elman  and\nGupta, Arshit  and\nCai, Deng  and\nLai, Yi-An  and\nZhang, Yi"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.",
  "keywords": [
    "end",
    "we",
    "dialogue",
    "training",
    "task-oriented dialogue",
    "classification",
    "classification experimental results",
    "unified",
    "a cascaded generation problem",
    "-",
    "task-oriented dialogue tod systems",
    "-task pre-training strategy",
    "language",
    "generation",
    "model"
  ],
  "url": "https://aclanthology.org/2022.acl-long.319/",
  "provenance": {
    "collected_at": "2025-06-05 08:28:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}