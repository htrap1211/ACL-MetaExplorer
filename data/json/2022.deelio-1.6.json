{
  "id": "2022.deelio-1.6",
  "title": "KIQA}: Knowledge-Infused Question Answering Model for Financial Table-Text Data",
  "authors": [
    "Nararatwong, Rungsiman  and\nKertkeidkachorn, Natthawut  and\nIchise, Ryutaro"
  ],
  "year": "2022",
  "venue": "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
  "abstract": "While entity retrieval models continue to advance their capabilities, our understanding of their wide-ranging applications is limited, especially in domain-specific settings. We highlighted this issue by using recent general-domain entity-linking models, LUKE and GENRE, to inject external knowledge into a question-answering (QA) model for a financial QA task with a hybrid tabular-textual dataset. We found that both models improved the baseline model by 1.57% overall and 8.86% on textual data. Nonetheless, the challenge remains as they still struggle to handle tabular inputs. We subsequently conducted a comprehensive attention-weight analysis, revealing how LUKE utilizes external knowledge supplied by GENRE. The analysis also elaborates how the injection of symbolic knowledge can be helpful and what needs further improvement, paving the way for future research on this challenging QA task and advancing our understanding of how a language model incorporates external knowledge.",
  "keywords": [
    "general",
    "knowledge",
    "answering",
    "language",
    "model",
    "text",
    "a comprehensive attention-weight analysis",
    "entity retrieval models",
    "a question-answering qa model",
    "retrieval",
    "recent general-domain entity-linking models",
    "question",
    "capabilities",
    "we",
    "attention"
  ],
  "url": "https://aclanthology.org/2022.deelio-1.6/",
  "provenance": {
    "collected_at": "2025-06-05 08:41:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}