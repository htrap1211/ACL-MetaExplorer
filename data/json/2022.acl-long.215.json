{
  "id": "2022.acl-long.215",
  "title": "Cross-Modal Discrete Representation Learning",
  "authors": [
    "Liu, Alexander  and\nJin, SouYoung  and\nLai, Cheng-I  and\nRouditchenko, Andrew  and\nOliva, Aude  and\nGlass, James"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In contrast to recent advances focusing on high-level representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities. Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision. We show that the proposed discretized multi-modal fine-grained representation (e.g., pixel/word/frame) can complement high-level summary representations (e.g., video/sentence/waveform) for improved performance on cross-modal retrieval tasks. We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities.",
  "keywords": [
    "code",
    "finer",
    "finer levels",
    "semantic",
    "we",
    "different views modalities",
    "a discretized embedding space",
    "cross",
    "self",
    "retrieval",
    "views",
    "word",
    "learning",
    "visual",
    "different modalities"
  ],
  "url": "https://aclanthology.org/2022.acl-long.215/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}