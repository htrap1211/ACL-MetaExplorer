{
  "id": "2023.repl4nlp-1.2",
  "title": "Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords",
  "authors": [
    "Golchin, Shahriar  and\nSurdeanu, Mihai  and\nTavabi, Nazgol  and\nKiapour, Ata"
  ],
  "year": "2023",
  "venue": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
  "abstract": "We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).",
  "keywords": [
    "our in-domain pre-training strategy",
    "i",
    "the pre-training time",
    "language",
    "random",
    "bert",
    "generic",
    "-",
    "fine",
    "bert large devlin",
    "we",
    "pre",
    "time",
    "keybert grootendorst",
    "training"
  ],
  "url": "https://aclanthology.org/2023.repl4nlp-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 10:26:15",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}