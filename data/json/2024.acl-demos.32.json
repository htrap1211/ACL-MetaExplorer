{
  "id": "2024.acl-demos.32",
  "title": "LEGENT}: Open Platform for Embodied Agents",
  "authors": [
    "Cheng, Zhili  and\nWang, Zhitong  and\nHu, Jinyi  and\nHu, Shengding  and\nLiu, An  and\nTu, Yuge  and\nLi, Pengkai  and\nShi, Lei  and\nLiu, Zhiyuan  and\nSun, Maosong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
  "abstract": "Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in 3D environments. Existing integrations often feature limited open-sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich 3D environment with interactive, communicable, and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities. The demo video is available at the following link https://video.legent.ai.",
  "keywords": [
    "legent-generated data surpasses gpt-4v",
    "field",
    "gpt-4v",
    "we",
    "embodied tasks",
    "promising generalization capabilities",
    "friendly",
    "rich",
    "a user-friendly interface",
    "llms",
    "embodied agents",
    "large language models llms",
    "generalization",
    "action",
    "language"
  ],
  "url": "https://aclanthology.org/2024.acl-demos.32/",
  "provenance": {
    "collected_at": "2025-06-05 10:47:42",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}