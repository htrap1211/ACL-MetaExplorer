{
  "id": "2024.acl-long.309",
  "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
  "authors": [
    "Tang, Tianyi  and\nLuo, Wenyang  and\nHuang, Haoyang  and\nZhang, Dongdong  and\nWang, Xiaolei  and\nZhao, Xin  and\nWei, Furu  and\nWen, Ji-Rong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora.It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts.In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions.Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs.Based on LAPE, we conduct comprehensive experiments on several representative LLMs, such as LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs’ proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models’ top and bottom layers.Furthermore, we showcase the feasibility to “steer” the output language of LLMs by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of LLMs.",
  "keywords": [
    "transformer",
    "proficiency",
    "language",
    "the multilingual capabilities",
    "remarkable multilingual capabilities",
    "it",
    "large language models",
    "activation",
    "transformer architectures",
    "large language models llms",
    "multilingual capabilities",
    "capabilities",
    "we",
    "several representative llms",
    "entropy"
  ],
  "url": "https://aclanthology.org/2024.acl-long.309/",
  "provenance": {
    "collected_at": "2025-06-05 10:38:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}