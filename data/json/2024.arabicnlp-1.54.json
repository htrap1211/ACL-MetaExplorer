{
  "id": "2024.arabicnlp-1.54",
  "title": "S}emantic{C}uet{S}ync at {A}r{AIE}val Shared Task: Detecting Propagandistic Spans with Persuasion Techniques Identification using Pre-trained Transformers",
  "authors": [
    "Shohan, Symom  and\nHossain, Md.  and\nParan, Ashraful  and\nAhsan, Shawly  and\nHossain, Jawad  and\nHoque, Mohammed Moshiul"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "Detecting propagandistic spans and identifying persuasion techniques are crucial for promoting informed decision-making, safeguarding democratic processes, and fostering a media environment characterized by integrity and transparency. Various machine learning (Logistic Regression, Random Forest, and Multinomial Naive Bayes), deep learning (CNN, CNN+LSTM, CNN+BiLSTM), and transformer-based (AraBERTv2, AraBERT-NER, CamelBERT, BERT-Base-Arabic) models were exploited to perform the task. The evaluation results indicate that CamelBERT achieved the highest micro-F1 score (24.09%), outperforming CNN+LSTM and AraBERTv2. The study found that most models struggle to detect propagandistic spans when multiple spans are present within the same article. Overall, the modelâ€™s performance secured a6thplace ranking in the ArAIEval Shared Task-1.",
  "keywords": [
    "the evaluation results",
    "the araieval",
    "deep",
    "val",
    "transformer",
    "ner",
    "transformers",
    "random",
    "forest",
    "cnn",
    "bert",
    "model",
    "machine",
    "the highest micro-f1 score",
    "bilstm"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.54/",
  "provenance": {
    "collected_at": "2025-06-05 11:02:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}