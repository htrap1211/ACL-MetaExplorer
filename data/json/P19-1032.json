{
  "id": "P19-1032",
  "title": "Adaptive Attention Span in Transformers",
  "authors": [
    "Sukhbaatar, Sainbayar  and\nGrave, Edouard  and\nBojanowski, Piotr  and\nJoulin, Armand"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",
  "keywords": [
    "transformer",
    "transformers",
    "language",
    "self",
    "modeling",
    "a novel self-attention mechanism",
    "us",
    "attention",
    "we",
    "its optimal attention span",
    "time",
    "character level language modeling",
    "that",
    "performances",
    "characters"
  ],
  "url": "https://aclanthology.org/P19-1032/",
  "provenance": {
    "collected_at": "2025-06-05 00:28:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}