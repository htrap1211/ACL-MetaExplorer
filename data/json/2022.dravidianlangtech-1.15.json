{
  "id": "2022.dravidianlangtech-1.15",
  "title": "GJG}@{T}amil{NLP}-{ACL}2022: Using Transformers for Abusive Comment Classification in {T}amil",
  "authors": [
    "Prasad, Gaurang  and\nPrasad, Janvi  and\nC, Gunavathi"
  ],
  "year": "2022",
  "venue": "Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages",
  "abstract": "This paper presents transformer-based models for the “Abusive Comment Detection” shared task at the Second Workshop on Speech and Language Technologies for Dravidian Languages at ACL 2022. Our team participated in both the multi-class classification sub-tasks as a part of this shared task. The dataset for sub-task A was in Tamil text; while B was code-mixed Tamil-English text. Both the datasets contained 8 classes of abusive comments. We trained an XLM-RoBERTa and DeBERTA base model on the training splits for each sub-task. For sub-task A, the XLM-RoBERTa model achieved an accuracy of 0.66 and the DeBERTa model achieved an accuracy of 0.62. For sub-task B, both the models achieved a classification accuracy of 0.72; however, the DeBERTa model performed better in other classification metrics. Our team ranked 2nd in the code-mixed classification sub-task and 8th in Tamil-text sub-task.",
  "keywords": [
    "transformer",
    "code",
    "a classification accuracy",
    "transformers",
    "accuracy",
    "roberta",
    "speech and language technologies",
    "language",
    "deberta",
    "nlp",
    "model",
    "text",
    "class",
    "the deberta model",
    "other classification metrics"
  ],
  "url": "https://aclanthology.org/2022.dravidianlangtech-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 08:41:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}