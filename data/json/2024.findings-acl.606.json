{
  "id": "2024.findings-acl.606",
  "title": "Efficient Continual Pre-training for Building Domain Specific Large Language Models",
  "authors": [
    "Xie, Yong  and\nAggarwal, Karan  and\nAhmad, Aitzaz"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large language models (LLMs) have demonstrated remarkable open-domain capabilities. LLMs tailored for a domain are typically trained entirely on domain corpus to excel at handling domain-specific tasks. In this work, we explore an alternative strategy of continual pre-training as a means to develop domain-specific LLMs over an existing open-domain LLM. We introduceFinPythia-6.9B, developed through domain-adaptive continual pre-training on the financial domain.Continual pre-trained FinPythia showcases consistent improvements on financial tasks over the original foundational model. We further explore simple but effective data selection strategies for continual pre-training. Our data selection strategies outperform vanilla continual pre-trainingâ€™s performance with just 10% of corpus size and cost, without any degradation on open-domain standard tasks. Our work proposes an alternative solution to building domain-specific LLMs cost-effectively.",
  "keywords": [
    "work",
    "means",
    "domain-specific llms",
    "language",
    "model",
    "efficient",
    "strategies",
    "-",
    "our data selection strategies",
    "large language models llms",
    "capabilities",
    "we",
    "pre",
    "remarkable open-domain capabilities llms",
    "llm"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.606/",
  "provenance": {
    "collected_at": "2025-06-05 10:56:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}