{
  "id": "2023.findings-acl.735",
  "title": "Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt",
  "authors": [
    "Yang, Xiaocui  and\nFeng, Shi  and\nWang, Daling  and\nSun, Qi  and\nWu, Wenfang  and\nZhang, Yifei  and\nHong, Pengfei  and\nPoria, Soujanya"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data distribution. To capture the specific prompt for each aspect term in a few-shot scenario, we propose a novel Generative Multimodal Prompt (GMP) model for MABSA, which includes the Multimodal Encoder module and the N-Stream Decoders module. We further introduce a subtask to predict the number of aspect terms in each instance to construct the multimodal prompt. Extensive experiments on two datasets demonstrate that our approach outperforms strong baselines on two MABSA-related tasks in the few-shot setting.",
  "keywords": [
    "the specific prompt",
    "we",
    "shot",
    "conventional studies",
    "decoders",
    "the n-stream decoders module",
    "analysis",
    "generative",
    "prompt",
    "the multimodal encoder module",
    "a few-shot scenario",
    "sentiment",
    "the few-shot setting",
    "model",
    "studies"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.735/",
  "provenance": {
    "collected_at": "2025-06-05 10:18:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}