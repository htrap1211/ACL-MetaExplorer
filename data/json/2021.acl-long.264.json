{
  "id": "2021.acl-long.264",
  "title": "Consistency Regularization for Cross-Lingual Fine-Tuning",
  "authors": [
    "Zheng, Bo  and\nDong, Li  and\nHuang, Shaohan  and\nWang, Wenhui  and\nChi, Zewen  and\nSinghal, Saksham  and\nChe, Wanxiang  and\nLiu, Ting  and\nSong, Xia  and\nWei, Furu"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling.",
  "keywords": [
    "work",
    "cross",
    "code",
    "text classification question",
    "i",
    "language",
    "machine",
    "model",
    "text",
    "consistency regularization",
    "regularization",
    "question",
    "fine",
    "we",
    "sequence"
  ],
  "url": "https://aclanthology.org/2021.acl-long.264/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:56",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}