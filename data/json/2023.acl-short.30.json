{
  "id": "2023.acl-short.30",
  "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
  "authors": [
    "Thakur, Himanshu  and\nJain, Atishay  and\nVaddamanu, Praneetha  and\nLiang, Paul Pu  and\nMorency, Louis-Philippe"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
  "keywords": [
    "our de-biasing technique",
    "variety",
    "bias",
    "we",
    "language modeling ability",
    "current",
    "shot",
    "training",
    "loss",
    "a variety",
    "data intervention strategies",
    "pre-trained large language models",
    "biasing",
    "strategies",
    "language models"
  ],
  "url": "https://aclanthology.org/2023.acl-short.30/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}