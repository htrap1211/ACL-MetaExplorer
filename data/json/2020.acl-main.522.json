{
  "id": "2020.acl-main.522",
  "title": "Improving Event Detection via Open-domain Trigger Knowledge",
  "authors": [
    "Tong, Meihan  and\nXu, Bin  and\nWang, Shuai  and\nCao, Yixin  and\nHou, Lei  and\nLi, Juanzi  and\nXie, Jun"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released onhttps://github.com/shuaiwa16/ekd.git.",
  "keywords": [
    "code",
    "knowledge",
    "ed",
    "model",
    "the in-built biases",
    "we",
    "biases",
    "training",
    "a fundamental task",
    "the source code",
    "baselines",
    "task",
    "github",
    "benchmark ace2005",
    "fundamental"
  ],
  "url": "https://aclanthology.org/2020.acl-main.522/",
  "provenance": {
    "collected_at": "2025-06-05 07:49:11",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}