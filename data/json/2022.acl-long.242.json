{
  "id": "2022.acl-long.242",
  "title": "Multilingual Molecular Representation Learning via Contrastive Pre-training",
  "authors": [
    "Guo, Zhihui  and\nSharma, Pramod  and\nMartinez, Andy  and\nDu, Liang  and\nAbraham, Robin"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Molecular representation learning plays an essential role in cheminformatics. Recently, language model-based approaches have gained popularity as an alternative to traditional expert-designed features to encode molecules. However, these approaches only utilize a single molecular language for representation learning. Motivated by the fact that a given molecule can be described using different languages such as Simplified Molecular Line Entry System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International Chemical Identifier (InChI), we propose a multilingual molecular embedding generation approach called MM-Deacon (multilingual molecular domain embedding analysis via contrastive learning). MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on large-scale molecules. We evaluated the robustness of our method on seven molecular property prediction tasks from MoleculeNet benchmark, zero-shot cross-lingual retrieval, and a drug-drug interaction prediction task.",
  "keywords": [
    "encode",
    "we",
    "applied",
    "training",
    "cross",
    "retrieval",
    "learning",
    "analysis",
    "identifier",
    "simplified",
    "language model-based approaches",
    "language",
    "generation",
    "model",
    "pre"
  ],
  "url": "https://aclanthology.org/2022.acl-long.242/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}