{
  "id": "2022.acl-short.12",
  "title": "Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models",
  "authors": [
    "Chuang, Chengyu  and\nYang, Yi"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .",
  "keywords": [
    "we",
    "finbert",
    "several nlp tasks",
    "pre-trained language models",
    "the implicit biases",
    "nlp models",
    "their financial nlp pipelines",
    "bert",
    "the bert",
    "language models",
    "the language models",
    "practitioners",
    "biases",
    "work",
    "language"
  ],
  "url": "https://aclanthology.org/2022.acl-short.12/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}