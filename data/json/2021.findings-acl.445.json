{
  "id": "2021.findings-acl.445",
  "title": "Highlight-Transformer: Leveraging Key Phrase Aware Attention to Improve Abstractive Multi-Document Summarization",
  "authors": [
    "Liu, Shuaiqi  and\nCao, Jiannong  and\nYang, Ruosong  and\nWen, Zhiyuan"
  ],
  "year": "2021",
  "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
  "abstract": "Correct abstract if needed. Retain XML formatting tags such as <tex-math>.",
  "keywords": [
    "abstractive multi-document summarization",
    "transformer",
    "tex",
    "attention",
    "summarization",
    "multi",
    "abstractive",
    "document",
    "phrase",
    "key phrase",
    "correct",
    "key",
    "aware",
    "math",
    "formatting tags"
  ],
  "url": "https://aclanthology.org/2021.findings-acl.445/",
  "provenance": {
    "collected_at": "2025-06-05 08:15:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}