{
  "id": "2023.iwslt-1.32",
  "title": "Language Model Based Target Token Importance Rescaling for Simultaneous Neural Machine Translation",
  "authors": [
    "Jain, Aditi  and\nKambhatla, Nishant  and\nSarkar, Anoop"
  ],
  "year": "2023",
  "venue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
  "abstract": "The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Under this notion of target adaptive training, generating rare or difficult tokens is rewarded which improves the translation quality while reducing latency. The predictions made by a language model in the decoder are combined with the traditional cross entropy loss which frees up the focus on the source side context. Our experimental results over multiple language pairs show that compared to previous state of the art methods in simultaneous translation, we can use an augmented target side context to improve BLEU scores significantly. We show improvements over the state of the art in the low latency range with lower average lagging values (faster output).",
  "keywords": [
    "the decoder",
    "bleu",
    "we",
    "simultaneous neural machine translation",
    "training",
    "translation",
    "cross",
    "neural",
    "decoder",
    "loss",
    "information",
    "bleu scores",
    "a language model",
    "the decoder model",
    "simultaneous translation"
  ],
  "url": "https://aclanthology.org/2023.iwslt-1.32/",
  "provenance": {
    "collected_at": "2025-06-05 10:25:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}