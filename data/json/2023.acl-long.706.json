{
  "id": "2023.acl-long.706",
  "title": "DT}-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
  "authors": [
    "Wang, Haiming  and\nYuan, Ye  and\nLiu, Zhengying  and\nShen, Jianhao  and\nYin, Yichun  and\nXiong, Jing  and\nXie, Enze  and\nShi, Han  and\nLi, Yujun  and\nLi, Lin  and\nYin, Jian  and\nLi, Zhenguo  and\nLiang, Xiaodan"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average).",
  "keywords": [
    "rate",
    "we",
    "general theorems",
    "current",
    "neural",
    "sequence",
    "a language model",
    "the language model",
    "function",
    "general",
    "language",
    "model",
    "human",
    "large language models",
    "states"
  ],
  "url": "https://aclanthology.org/2023.acl-long.706/",
  "provenance": {
    "collected_at": "2025-06-05 09:45:11",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}