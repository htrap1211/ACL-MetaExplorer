{
  "id": "2023.iwslt-1.16",
  "title": "I}2{R}{'}s End-to-End Speech Translation System for {IWSLT} 2023 Offline Shared Task",
  "authors": [
    "Huzaifah, Muhammad  and\nMin Tan, Kye  and\nDuan, Richeng"
  ],
  "year": "2023",
  "venue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
  "abstract": "This paper describes I2R’s submission to the offline speech translation track for IWSLT 2023. We focus on an end-to-end approach for translation from English audio to German text, one of the three available language directions in this year’s edition. The I2R system leverages on pretrained models that have been exposed to large-scale audio and text data for our base model. We introduce several stages of additional pretraining followed by fine-tuning to adapt the system for the downstream speech translation task. The strategy is supplemented by other techniques such as data augmentation, domain tagging, knowledge distillation, and model ensemble, among others. We evaluate the system on several publicly available test sets for comparison.",
  "keywords": [
    "ensemble",
    "tuning",
    "knowledge",
    "end",
    "i",
    "language",
    "model",
    "text",
    "model ensemble",
    "tagging",
    "fine",
    "we",
    "translation",
    "fine-tuning",
    "i2r s submission"
  ],
  "url": "https://aclanthology.org/2023.iwslt-1.16/",
  "provenance": {
    "collected_at": "2025-06-05 10:24:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}