{
  "id": "2024.acl-long.229",
  "title": "Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models",
  "authors": [
    "Zhang, Liang  and\nJin, Qin  and\nHuang, Haoyang  and\nZhang, Dongdong  and\nWei, Furu"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Large Language Models (LLMs) show strong instruction understanding ability across multiple languages. However, they are easily biased towards English in instruction tuning, and generate English responses even given non-English instructions. In this paper, we investigate the language inconsistent generation problem in monolingual instruction tuning. We find that instruction tuning in English increases the modelsâ€™ preference for English responses. It attaches higher probabilities to English responses than to responses in the same language as the instruction. Based on the findings, we alleviate the language inconsistent generation problem by counteracting the model preference for English responses in both the training and inference stages. Specifically, we propose Pseudo-Inconsistent Penalization (PIP) which prevents the model from generating English responses when given non-English language prompts during training, and Prior Enhanced Decoding (PED) which improves the language-consistent prior by leveraging the untuned base language model. Experimental results show that our two methods significantly improve the language consistency of the model without requiring any multilingual data. Our code, data, and models will be released.",
  "keywords": [
    "code",
    "non-english language prompts",
    "we",
    "training",
    "instruction",
    "it",
    "biased",
    "higher probabilities",
    "llms",
    "instruction tuning",
    "tuning",
    "large language models llms",
    "monolingual instruction tuning",
    "response generation",
    "prompts"
  ],
  "url": "https://aclanthology.org/2024.acl-long.229/",
  "provenance": {
    "collected_at": "2025-06-05 10:37:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}