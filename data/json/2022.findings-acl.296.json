{
  "id": "2022.findings-acl.296",
  "title": "Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics",
  "authors": [
    "Deutsch, Daniel  and\nRoth, Dan"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Question answering-based summarization evaluation metrics must automatically determine whether the QA model’s prediction is correct or not, a task known as answer verification. In this work, we benchmark the lexical answer verification methods which have been used by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore and LERC. We find that LERC out-performs the other methods in some settings while remaining statistically indistinguishable from lexical overlap in others. However, our experiments reveal that improved verification performance does not necessarily translate to overall QA-based metric quality: In some scenarios, using a worse verification method — or using none at all — has comparable performance to using the best verification method, a result that we attribute to properties of the datasets.",
  "keywords": [
    "work",
    "bertscore",
    "answer",
    "answering",
    "model",
    "metric",
    "overall qa-based metric quality",
    "text",
    "metrics",
    "current qa-based metrics",
    "properties",
    "question",
    "summarization",
    "we",
    "evaluation"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.296/",
  "provenance": {
    "collected_at": "2025-06-05 08:38:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}