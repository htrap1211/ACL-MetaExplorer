{
  "id": "P18-2121",
  "title": "Pretraining Sentiment Classifiers with Unlabeled Dialog Data",
  "authors": [
    "Shimizu, Toru  and\nShimizu, Nobuyuki  and\nKobayashi, Hayato"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification. Recent studies showed that pretraining with unlabeled data via a language model can improve the performance of classification models. In this paper, we take the concept a step further by using a conditional language model, instead of a language model. Specifically, we address a sentiment classification task for a tweet analysis service as a case study and propose a pretraining strategy with unlabeled dialog data (tweet-reply pairs) via an encoder-decoder model. Experimental results show that our strategy can improve the performance of sentiment classifiers and outperform several state-of-the-art strategies including language model pretraining.",
  "keywords": [
    "classification models",
    "sentiment",
    "language",
    "a conditional language model",
    "model",
    "language model",
    "studies",
    "strategies",
    "classifiers",
    "dialog",
    "encoder",
    "decoder",
    "we",
    "sentiment classifiers",
    "learning"
  ],
  "url": "https://aclanthology.org/P18-2121/",
  "provenance": {
    "collected_at": "2025-06-05 00:19:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}