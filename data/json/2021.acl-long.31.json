{
  "id": "2021.acl-long.31",
  "title": "Deep Differential Amplifier for Extractive Summarization",
  "authors": [
    "Jia, Ruipeng  and\nCao, Yanan  and\nFang, Fang  and\nZhou, Yuchen  and\nFang, Zheng  and\nLiu, Yanbing  and\nWang, Shi"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the accuracy. The imbalanced classification of summarization is inherent, which canâ€™t be addressed by common algorithms easily. In this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework. Specifically, we first calculate and amplify the semantic difference between each sentence and all other sentences, and then apply the residual unit as the second item of the differential amplifier to deepen the architecture. Finally, to compensate for the imbalance, the corresponding objective loss of minority class is boosted by a weighted cross-entropy. In contrast to previous approaches, this model pays more attention to the pivotal information of one sentence, instead of all the informative context modeling by recurrent or Transformer architecture. We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods. Our source code will be available on Github.",
  "keywords": [
    "deep",
    "t",
    "code",
    "the accuracy",
    "amplifier",
    "semantic",
    "summarization",
    "we",
    "deep differential amplifier",
    "the semantic difference",
    "classification",
    "cross",
    "the imbalanced classification",
    "the single-document extractive summarization",
    "loss"
  ],
  "url": "https://aclanthology.org/2021.acl-long.31/",
  "provenance": {
    "collected_at": "2025-06-05 07:59:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}