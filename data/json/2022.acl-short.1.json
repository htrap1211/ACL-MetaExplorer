{
  "id": "2022.acl-short.1",
  "title": "B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
  "authors": [
    "Ben Zaken, Elad  and\nGoldberg, Yoav  and\nRavfogel, Shauli"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
  "keywords": [
    "transformer",
    "tuning",
    "process",
    "bias",
    "knowledge",
    "only the bias-terms",
    "language",
    "pre-trained bert models",
    "bert",
    "model",
    "it",
    "efficient",
    "other sparse fine-tuning methods",
    "question",
    "transformer-based masked language-models"
  ],
  "url": "https://aclanthology.org/2022.acl-short.1/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}