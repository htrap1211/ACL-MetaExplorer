{
  "id": "2020.repl4nlp-1.20",
  "title": "A Cross-Task Analysis of Text Span Representations",
  "authors": [
    "Toshniwal, Shubham  and\nShi, Haoyue  and\nShi, Bowen  and\nGao, Lingyu  and\nLivescu, Karen  and\nGimpel, Kevin"
  ],
  "year": "2020",
  "venue": "Proceedings of the 5th Workshop on Representation Learning for NLP",
  "abstract": "Many natural language processing (NLP) tasks involve reasoning with textual spans, including question answering, entity recognition, and coreference resolution. While extensive research has focused on functional architectures for representing words and sentences, there is less work on representing arbitrary spans of text within sentences. In this paper, we conduct a comprehensive empirical evaluation of six span representation methods using eight pretrained language representation models across six tasks, including two tasks that we introduce. We find that, although some simple span representations are fairly reliable across tasks, in general the optimal span representation varies by task, and can also vary within different facets of individual tasks. We also find that the choice of span representation has a bigger impact with a fixed pretrained encoder than with a fine-tuned encoder.",
  "keywords": [
    "cross",
    "work",
    "a fixed pretrained encoder",
    "general",
    "processing",
    "language",
    "nlp",
    "natural",
    "text",
    "encoder",
    "a comprehensive empirical evaluation",
    "coreference resolution",
    "question",
    "we",
    "evaluation"
  ],
  "url": "https://aclanthology.org/2020.repl4nlp-1.20/",
  "provenance": {
    "collected_at": "2025-06-05 07:58:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}