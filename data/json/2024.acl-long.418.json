{
  "id": "2024.acl-long.418",
  "title": "EIT}: Enhanced Interactive Transformer",
  "authors": [
    "Zheng, Tong  and\nLi, Bei  and\nBao, Huiwen  and\nXiao, Tong  and\nZhu, JingBo"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Two principles: the complementary principle and the consensus principle are widely acknowledged in the literature of multi-view learning. However, the current design of multi-head self-attention, an instance of multi-view learning, prioritizes the complementarity while ignoring the consensus. To address this problem, we propose an enhanced multi-head self-attention (EMHA). First, to satisfy the complementary principle, EMHA removes the one-to-one mapping constraint among queries and keys in multiple subspaces and allows each query to attend to multiple keys. On top of that, we develop a method to fully encourage consensus among heads by introducing two interaction models, namely inner-subspace interaction and cross-subspace interaction. Extensive experiments on a wide range of language tasks (e.g., machine translation, abstractive summarization and grammar correction, language modeling), show its superiority, with a very modest increase in model size. Our code would be available at: https://github.com/zhengkid/EIT-Enhanced-Interactive-Transformer.",
  "keywords": [
    "code",
    "top",
    "interactive transformer two principles",
    "summarization",
    "we",
    "current",
    "translation",
    "cross",
    "queries",
    "self",
    "inner",
    "learning",
    "multi-head self-attention",
    "multi-view learning",
    "transformer"
  ],
  "url": "https://aclanthology.org/2024.acl-long.418/",
  "provenance": {
    "collected_at": "2025-06-05 10:40:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}