{
  "id": "2022.acl-long.564",
  "title": "Revisiting Over-Smoothness in Text to Speech",
  "authors": [
    "Ren, Yi  and\nTan, Xu  and\nQin, Tao  and\nZhao, Zhou  and\nLiu, Tie-Yan"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Non-autoregressive text to speech (NAR-TTS) models have attracted much attention from both academia and industry due to their fast generation speed. One limitation of NAR-TTS models is that they ignore the correlation in time and frequency domains while generating speech mel-spectrograms, and thus cause blurry and over-smoothed results. In this work, we revisit this over-smoothing problem from a novel perspective: the degree of over-smoothness is determined by the gap between the complexity of data distributions and the capability of modeling methods. Both simplifying data distributions and improving modeling methods can alleviate the problem. Accordingly, we first study methods reducing the complexity of data distributions. Then we conduct a comprehensive study on NAR-TTS models that use some advanced modeling methods. Based on these studies, we find that 1) methods that provide additional condition inputs reduce the complexity of data distributions to model, thus alleviating the over-smoothing problem and achieving better voice quality. 2) Among advanced modeling methods, Laplacian mixture loss performs well at modeling multimodal distributions and enjoys its simplicity, while GAN and Glow achieve the best voice quality while suffering from increased training or model complexity. 3) The two categories of methods can be combined to further alleviate the over-smoothness and improve the voice quality. 4) Our experiments on the multi-speaker dataset lead to similar conclusions as above and providing more variance information can reduce the difficulty of modeling the target data distribution and alleviate the requirements for model capacity.",
  "keywords": [
    "much attention",
    "these studies",
    "their fast generation",
    "we",
    "variance",
    "training",
    "loss",
    "information",
    "the two categories",
    "more variance information",
    "categories",
    "text",
    "fast",
    "work",
    "generation"
  ],
  "url": "https://aclanthology.org/2022.acl-long.564/",
  "provenance": {
    "collected_at": "2025-06-05 08:31:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}