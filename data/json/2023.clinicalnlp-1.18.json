{
  "id": "2023.clinicalnlp-1.18",
  "title": "Large Scale Sequence-to-Sequence Models for Clinical Note Generation from Patient-Doctor Conversations",
  "authors": [
    "Singh, Gagandeep  and\nPan, Yue  and\nAndres-Ferrer, Jesus  and\nDel-Agua, Miguel  and\nDiehl, Frank  and\nPinto, Joel  and\nVozila, Paul"
  ],
  "year": "2023",
  "venue": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
  "abstract": "We present our work on building large scale sequence-to-sequence models for generating clinical note from patient-doctor conversation. This is formulated as an abstractive summarization task for which we use encoder-decoder transformer model with pointer-generator. We discuss various modeling enhancements to this baseline model which include using subword and multiword tokenization scheme, prefixing the targets with a chain-of-clinical-facts, and training with contrastive loss that is defined over various candidate summaries. We also use flash attention during training and query chunked attention during inference to be able to process long input and output sequences and to improve computational efficiency. Experiments are conducted on a dataset containing about 900K encounters from around 1800 healthcare providers covering 27 specialties. The results are broken down into primary care and non-primary care specialties. Consistent accuracy improvements are observed across both of these categories.",
  "keywords": [
    "chain",
    "conversations",
    "efficiency",
    "summarization",
    "we",
    "training",
    "an abstractive summarization task",
    "consistent accuracy improvements",
    "specialties",
    "decoder",
    "loss",
    "flash attention",
    "sequence",
    "patient-doctor conversations",
    "tokenization"
  ],
  "url": "https://aclanthology.org/2023.clinicalnlp-1.18/",
  "provenance": {
    "collected_at": "2025-06-05 10:23:31",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}