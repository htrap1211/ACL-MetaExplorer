{
  "id": "2020.acl-main.411",
  "title": "D}e{F}ormer: Decomposing Pre-trained Transformers for Faster Question Answering",
  "authors": [
    "Cao, Qingqing  and\nTrivedi, Harsh  and\nBalasubramanian, Aruna  and\nBalasubramanian, Niranjan"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Transformer-based QA models use input-wide self-attention – i.e. across both the question and the input passage – at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code athttps://github.com/StonyBrookNLP/deformer.",
  "keywords": [
    "the pre-training weights",
    "code",
    "transformers",
    "input-wide self-attention",
    "attentions",
    "question",
    "we",
    "training",
    "the full self-attention",
    "it",
    "self",
    "question-wide and passage-wide self-attentions",
    "a decomposed transformer",
    "processing",
    "bert"
  ],
  "url": "https://aclanthology.org/2020.acl-main.411/",
  "provenance": {
    "collected_at": "2025-06-05 07:47:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}