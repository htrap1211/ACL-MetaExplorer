{
  "id": "2024.acl-long.411",
  "title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment",
  "authors": [
    "Li, Yunxin  and\nChen, Xinyu  and\nHu, Baotian  and\nShi, Haoyuan  and\nZhang, Min"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.",
  "keywords": [
    "the accuracy",
    "end",
    "a small language model",
    "question",
    "knowledge-based vqa average gain",
    "we",
    "current",
    "instruction",
    "knowledge-based vqa benchmarks",
    "it",
    "aligner",
    "information",
    "visual",
    "llms",
    "5 0 ablation studies"
  ],
  "url": "https://aclanthology.org/2024.acl-long.411/",
  "provenance": {
    "collected_at": "2025-06-05 10:39:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}