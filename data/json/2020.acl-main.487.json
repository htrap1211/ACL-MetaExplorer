{
  "id": "2020.acl-main.487",
  "title": "Social Biases in {NLP} Models as Barriers for Persons with Disabilities",
  "authors": [
    "Hutchinson, Ben  and\nPrabhakaran, Vinodkumar  and\nDenton, Emily  and\nWebster, Kellie  and\nZhong, Yu  and\nDenuyl, Stephen"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",
  "keywords": [
    "topical biases",
    "embeddings",
    "language",
    "neural",
    "nlp",
    "model",
    "social biases",
    "the neural embeddings",
    "most nlp pipelines",
    "disabilities",
    "technologies",
    "barriers",
    "undesirable social biases",
    "we",
    "nlp models"
  ],
  "url": "https://aclanthology.org/2020.acl-main.487/",
  "provenance": {
    "collected_at": "2025-06-05 07:48:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}