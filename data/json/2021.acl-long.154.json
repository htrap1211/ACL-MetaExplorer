{
  "id": "2021.acl-long.154",
  "title": "UXLA}: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual {NLP",
  "authors": [
    "Bari, M Saiful  and\nMohiuddin, Tasnim  and\nJoty, Shafiq"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. We propose UXLA, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios. In particular, UXLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language. At its core, UXLA performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin. With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to its success.",
  "keywords": [
    "we",
    "training",
    "cross",
    "self",
    "learning",
    "transfer",
    "many supervised nlp tasks",
    "core",
    "language",
    "nlp",
    "state",
    "low",
    "unsupervised",
    "effectiveness",
    "framework"
  ],
  "url": "https://aclanthology.org/2021.acl-long.154/",
  "provenance": {
    "collected_at": "2025-06-05 08:01:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}