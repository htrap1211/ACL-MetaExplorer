{
  "id": "2024.acl-long.245",
  "title": "Graph Language Models",
  "authors": [
    "Plenz, Moritz  and\nFrank, Anette"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs – which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure – but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM’s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.",
  "keywords": [
    "structured knowledge graphs kgs",
    "glm embeddings",
    "we",
    "graph",
    "current",
    "shot",
    "classification",
    "both empirical evaluations",
    "neural",
    "relation classification tasks",
    "information",
    "kgs",
    "graph language models",
    "i",
    "text"
  ],
  "url": "https://aclanthology.org/2024.acl-long.245/",
  "provenance": {
    "collected_at": "2025-06-05 10:37:39",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}