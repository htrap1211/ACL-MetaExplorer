{
  "id": "2024.acl-long.743",
  "title": "REFINESUMM}: Self-Refining {MLLM} for Generating a Multimodal Summarization Dataset",
  "authors": [
    "Patil, Vaidehi  and\nRibeiro, Leonardo F. R.  and\nLiu, Mengwen  and\nBansal, Mohit  and\nDreyer, Markus"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Multimodal Large Language Models (MLLMs) excel at synthesizing key information from diverse sources. However, generating accurate and faithful multimodal summaries is challenging, primarily due to the lack of appropriate multimodal datasets for fine-tuning that meaningfully integrate textual and visual modalities. To address this gap, we present a new dataset designed specifically for image-text multimodal summarization, harnessing the capabilities of state-of-the-art MLLMs. We generate summaries from Wikipedia sections and corresponding images and evaluate them across text-based, visual and multimodal dimensions, employing reference-free metrics. To refine the dataset, we: (1) Filter the MLLM-generated summaries by training a critic model on human annotations and using its predictions to remove low-quality summaries; (2) Fine-tune the MLLM with the filtered high-quality summaries; (3) Use the fine-tuned model in turn to regenerate the summaries. This self-refinement process significantly improves summary quality, as measured by human judgements and automatic multimodal metrics, resulting in a valuable dataset for multimodal summarization research. The dataset is publicly available at https://github.com/amazon-science/refinesumm.",
  "keywords": [
    "the filtered high-quality summaries",
    "the mllm",
    "refinesumm self-refining mllm",
    "automatic multimodal metrics",
    "summarization",
    "we",
    "multimodal summarization research",
    "fine-tuning",
    "self",
    "the summaries",
    "information",
    "science",
    "the capabilities",
    "visual",
    "mllm"
  ],
  "url": "https://aclanthology.org/2024.acl-long.743/",
  "provenance": {
    "collected_at": "2025-06-05 10:44:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}