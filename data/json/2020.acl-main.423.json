{
  "id": "2020.acl-main.423",
  "title": "S}ense{BERT}: Driving Some Sense into {BERT",
  "authors": [
    "Levine, Yoav  and\nLenz, Barak  and\nDagan, Or  and\nRam, Ori  and\nPadnos, Dan  and\nSharir, Or  and\nShalev-Shwartz, Shai  and\nShashua, Amnon  and\nShoham, Yoav"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ weak-supervision directly at the word sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a lexical-semantic level language model, without the use of human annotation. SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the ‘Word in Context’ task.",
  "keywords": [
    "the frontier",
    "human annotation sensebert",
    "language",
    "neural",
    "natural",
    "bert",
    "model",
    "ense bert",
    "human",
    "ense",
    "self",
    "frontier",
    "the underlying semantic content",
    "form",
    "semantic"
  ],
  "url": "https://aclanthology.org/2020.acl-main.423/",
  "provenance": {
    "collected_at": "2025-06-05 07:47:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}