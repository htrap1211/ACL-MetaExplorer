{
  "id": "P19-1028",
  "title": "Augmenting Neural Networks with First-order Logic",
  "authors": [
    "Li, Tao  and\nSrikumar, Vivek"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes.",
  "keywords": [
    "knowledge",
    "end",
    "a neural network",
    "neural",
    "natural",
    "neural networks",
    "model",
    "extra",
    "machine",
    "text",
    "modeling",
    "loss",
    "question",
    "network",
    "we"
  ],
  "url": "https://aclanthology.org/P19-1028/",
  "provenance": {
    "collected_at": "2025-06-05 00:28:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}