{
  "id": "2022.acl-long.427",
  "title": "XLM}-{E}: Cross-lingual Language Model Pre-training via {ELECTRA",
  "authors": [
    "Chi, Zewen  and\nHuang, Shaohan  and\nDong, Li  and\nMa, Shuming  and\nZheng, Bo  and\nSinghal, Saksham  and\nBajaj, Payal  and\nSong, Xia  and\nMao, Xian-Ling  and\nHuang, Heyan  and\nWei, Furu"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.",
  "keywords": [
    "cross",
    "cross-lingual language model",
    "language",
    "translation",
    "model",
    "token",
    "we",
    "pre",
    "two pre-training tasks",
    "analysis",
    "training",
    "understanding",
    "xlm-e",
    "computation",
    "paper"
  ],
  "url": "https://aclanthology.org/2022.acl-long.427/",
  "provenance": {
    "collected_at": "2025-06-05 08:30:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}