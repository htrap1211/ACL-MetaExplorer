{
  "id": "2021.repl4nlp-1.20",
  "title": "Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers",
  "authors": [
    "Bujel, Kamil  and\nYannakoudakis, Helen  and\nRei, Marek"
  ],
  "year": "2021",
  "venue": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
  "abstract": "We investigate how sentence-level transformers can be modified into effective sequence labelers at the token level without any direct supervision. Existing approaches to zero-shot sequence labeling do not perform well when applied on transformer-based architectures. As transformers contain multiple layers of multi-head self-attention, information in the sentence gets distributed between many tokens, negatively affecting zero-shot token-level performance. We find that a soft attention module which explicitly encourages sharpness of attention weights can significantly outperform existing methods.",
  "keywords": [
    "transformer",
    "transformer-based sentence classifiers",
    "transformers",
    "a soft attention module",
    "zero-shot token-level performance",
    "self",
    "classifiers",
    "transformer-based architectures",
    "token",
    "sentence-level transformers",
    "information",
    "attention weights",
    "attention",
    "we",
    "sequence"
  ],
  "url": "https://aclanthology.org/2021.repl4nlp-1.20/",
  "provenance": {
    "collected_at": "2025-06-05 08:19:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}