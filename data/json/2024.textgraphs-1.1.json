{
  "id": "2024.textgraphs-1.1",
  "title": "Learning Human Action Representations from Temporal Context in Lifestyle Vlogs",
  "authors": [
    "Ignat, Oana  and\nCastro, Santiago  and\nLi, Weiji  and\nMihalcea, Rada"
  ],
  "year": "2024",
  "venue": "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
  "abstract": "We address the task of human action representation and show how the approach to generating word representations based on co-occurrence can be adapted to generate human action representations by analyzing their co-occurrence in videos. To this end, we formalize the new task of human action co-occurrence identification in online videos, i.e., determine whether two human actions are likely to co-occur in the same interval of time.We create and make publicly available the Co-Act (Action Co-occurrence) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring.We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains.",
  "keywords": [
    "end",
    "we",
    "graph",
    "co",
    "information",
    "word",
    "visual",
    "i",
    "-",
    "action",
    "human",
    "time",
    "act",
    "occurrence",
    "our task"
  ],
  "url": "https://aclanthology.org/2024.textgraphs-1.1/",
  "provenance": {
    "collected_at": "2025-06-05 11:11:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}