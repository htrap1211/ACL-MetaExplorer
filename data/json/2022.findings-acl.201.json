{
  "id": "2022.findings-acl.201",
  "title": "DU}-{VLG}: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training",
  "authors": [
    "Huang, Luyang  and\nNiu, Guocheng  and\nLiu, Jiachen  and\nXiao, Xinyan  and\nWu, Hua"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation. In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems. DU-VLG is trained with novel dual pre-training tasks: multi-modal denoising autoencoder tasks and modality translation tasks. To bridge the gap between image understanding and generation, we further design a novel commitment loss. We compare pre-training objectives on image captioning and text-to-image generation datasets. Results show that DU-VLG yields better performance than variants trained with uni-directional generation objectives or the variant without the commitment loss. We also obtain higher scores compared to previous state-of-the-art systems on three vision-and-language generation tasks. In addition, human judges further confirm that our model generates real and relevant images as well as faithful and informative captions.",
  "keywords": [
    "objectives",
    "uni",
    "novel dual pre-training tasks",
    "autoencoder",
    "du",
    "we",
    "du-vlg yields",
    "training",
    "translation",
    "vision-and-language generation",
    "loss",
    "sequence",
    "yields",
    "modality translation tasks",
    "sequence generation problems"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.201/",
  "provenance": {
    "collected_at": "2025-06-05 08:37:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}