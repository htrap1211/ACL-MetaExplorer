{
  "id": "2022.acl-long.88",
  "title": "Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation",
  "authors": [
    "Zhou, Pei  and\nGopalakrishnan, Karthik  and\nHedayatnia, Behnam  and\nKim, Seokhwan  and\nPujara, Jay  and\nRen, Xiang  and\nLiu, Yang  and\nHakkani-Tur, Dilek"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge. In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak). We argue that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models. We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues. Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators. TBS also generatesknowledgethat makes sense and is relevant to the dialogue around 85% of the time",
  "keywords": [
    "knowledge-aligned dialogues",
    "generatesknowledgethat",
    "generative",
    "response generation implicit knowledge",
    "knowledge",
    "end",
    "generation",
    "more efficient learning",
    "neural",
    "dialogues",
    "human",
    "efficient",
    "most automatic metrics",
    "metrics",
    "conversations"
  ],
  "url": "https://aclanthology.org/2022.acl-long.88/",
  "provenance": {
    "collected_at": "2025-06-05 08:25:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}