{
  "id": "2024.repl4nlp-1.6",
  "title": "IT}-Tuning : Parameter Efficient Information Token Tuning for Language Model",
  "authors": [
    "Kim, Jungu  and\nKim, Hyeoncheol"
  ],
  "year": "2024",
  "venue": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
  "abstract": "Recently, language models have demonstrated exceptional performance compared to their predecessors. In this context, attention mechanisms and pre-training significantly contribute to the enhanced performance of modern language models. Additionally, a continuously increasing number of parameters plays a crucial role in these advancements . However, an increase in the number of parameters significantly increases the GPU memory and training time required during fine-tuning of language models, this makes fine-tuning infeasible in environments with limited computing resources. Furthermore, after fine-tuning, the storage space required for deployment increases proportionally with the number of tasks, making it challenging to deploy devices with limited storage capacities. In this study, we propose IT-Tuning, a Parameter Efficient Fine-Tuning method that introduces a new concept called information tokens to address these issues.",
  "keywords": [
    "tuning",
    "language",
    "language model",
    "model",
    "it",
    "modern language models",
    "efficient",
    "this context attention mechanisms",
    "language models",
    "information",
    "fine",
    "attention",
    "we",
    "capacities",
    "pre"
  ],
  "url": "https://aclanthology.org/2024.repl4nlp-1.6/",
  "provenance": {
    "collected_at": "2025-06-05 11:09:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}