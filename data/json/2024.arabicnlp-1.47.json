{
  "id": "2024.arabicnlp-1.47",
  "title": "Mela at {A}r{AIE}val Shared Task: Propagandistic Techniques Detection in {A}rabic with a Multilingual Approach",
  "authors": [
    "Riyadh, Md Abdur Razzaq  and\nNabhani, Sara"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "This paper presents our system submitted for Task 1 of the ArAIEval Shared Task on Unimodal (Text) Propagandistic Technique Detection in Arabic. Task 1 involves identifying all employed propaganda techniques in a given text from a set of possible techniques or detecting that no propaganda technique is present. Additionally, the task requires identifying the specific spans of text where these techniques occur. We explored the capabilities of a multilingual BERT model for this task, focusing on the effectiveness of using outputs from different hidden layers within the model. By fine-tuning the multilingual BERT, we aimed to improve the modelâ€™s ability to recognize and locate various propaganda techniques. Our experiments showed that leveraging the hidden layers of the BERT model enhanced detection performance. Our system achieved competitive results, ranking second in the shared task, demonstrating that multilingual BERT models, combined with outputs from hidden layers, can effectively detect and identify spans of propaganda techniques in Arabic text.",
  "keywords": [
    "the araieval",
    "a multilingual bert model",
    "val",
    "bert",
    "model",
    "text",
    "that multilingual bert models",
    "araieval",
    "capabilities",
    "fine",
    "we",
    "the capabilities",
    "aie",
    "the multilingual bert",
    "the effectiveness"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.47/",
  "provenance": {
    "collected_at": "2025-06-05 11:02:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}