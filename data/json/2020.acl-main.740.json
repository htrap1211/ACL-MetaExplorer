{
  "id": "2020.acl-main.740",
  "title": "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
  "authors": [
    "Gururangan, Suchin  and\nMarasovi{\\'c}, Ana  and\nSwayamdipta, Swabha  and\nLo, Kyle  and\nBeltagy, Iz  and\nDowney, Doug  and\nSmith, Noah A."
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
  "keywords": [
    "eight classification tasks",
    "variety",
    "language",
    "nlp",
    "model",
    "text",
    "it",
    "strategies",
    "today s nlp",
    "science",
    "simple data selection strategies",
    "tasks language models",
    "we",
    "adapt language models",
    "a wide variety"
  ],
  "url": "https://aclanthology.org/2020.acl-main.740/",
  "provenance": {
    "collected_at": "2025-06-05 07:52:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}