{
  "id": "2023.iwslt-1.31",
  "title": "NAIST} Simultaneous Speech-to-speech Translation System for {IWSLT} 2023",
  "authors": [
    "Fukuda, Ryo  and\nNishikawa, Yuta  and\nKano, Yasumasa  and\nKo, Yuka  and\nYanagita, Tomoya  and\nDoi, Kosuke  and\nMakinae, Mana  and\nSakti, Sakriani  and\nSudoh, Katsuhito  and\nNakamura, Satoshi"
  ],
  "year": "2023",
  "venue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
  "abstract": "This paper describes NAISTâ€™s submission to the IWSLT 2023 Simultaneous Speech Translation task: English-to-German, Japanese, Chinese speech-to-text translation and English-to-Japanese speech-to-speech translation. Our speech-to-text system uses an end-to-end multilingual speech translation model based on large-scale pre-trained speech and text models. We add Inter-connections into the model to incorporate the outputs from intermediate layers of the pre-trained speech model and augment prefix-to-prefix text data using Bilingual Prefix Alignment to enhance the simultaneity of the offline speech translation model. Our speech-to-speech system employs an incremental text-to-speech module that consists of a Japanese pronunciation estimation model, an acoustic model, and a neural vocoder.",
  "keywords": [
    "alignment",
    "end",
    "neural",
    "model",
    "text",
    "-",
    "we",
    "pre",
    "bilingual prefix alignment",
    "translation",
    "speech",
    "that",
    "prefix-to-prefix text data",
    "large-scale pre-trained speech",
    "a neural vocoder"
  ],
  "url": "https://aclanthology.org/2023.iwslt-1.31/",
  "provenance": {
    "collected_at": "2025-06-05 10:25:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}