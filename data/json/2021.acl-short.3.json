{
  "id": "2021.acl-short.3",
  "title": "Coreference Resolution without Span Representations",
  "authors": [
    "Kirstain, Yuval  and\nRam, Ori  and\nLevy, Omer"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effective, the model has a very large memory footprint – primarily due to dynamically-constructed span and span-pair representations – which hinders the processing of complete documents and the ability to train on multiple instances in a single batch. We introduce a lightweight end-to-end coreference model that removes the dependency on span representations, handcrafted features, and heuristics. Our model performs competitively with the current standard model, while being simpler and more efficient.",
  "keywords": [
    "transformer",
    "processing",
    "end",
    "language",
    "nlp",
    "model",
    "efficient",
    "pretrained language models",
    "encoder",
    "coreference resolution",
    "dependency",
    "we",
    "the dependency",
    "current",
    "coreference"
  ],
  "url": "https://aclanthology.org/2021.acl-short.3/",
  "provenance": {
    "collected_at": "2025-06-05 08:07:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}