{
  "id": "2021.unimplicit-1.2",
  "title": "Implicit Phenomena in Short-answer Scoring Data",
  "authors": [
    "Bexte, Marie  and\nHorbach, Andrea  and\nZesch, Torsten"
  ],
  "year": "2021",
  "venue": "Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language",
  "abstract": "Short-answer scoring is the task of assessing the correctness of a short text given as response to a question that can come from a variety of educational scenarios. As only content, not form, is important, the exact wording including the explicitness of an answer should not matter. However, many state-of-the-art scoring models heavily rely on lexical information, be it word embeddings in a neural network or n-grams in an SVM. Thus, the exact wording of an answer might very well make a difference. We therefore quantify to what extent implicit language phenomena occur in short answer datasets and examine the influence they have on automatic scoring performance. We find that the level of implicitness depends on the individual question, and that some phenomena are very frequent. Resolving implicit wording to explicit formulations indeed tends to improve automatic scoring performance.",
  "keywords": [
    "variety",
    "question",
    "form",
    "we",
    "svm",
    "answer",
    "neural",
    "it",
    "information",
    "word",
    "a variety",
    "text",
    "word embeddings",
    "embeddings",
    "language"
  ],
  "url": "https://aclanthology.org/2021.unimplicit-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 08:23:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}