{
  "id": "2020.challengehml-1.9",
  "title": "Exploring Weaknesses of {VQA} Models through Attribution Driven Insights",
  "authors": [
    "Halbe, Shaunak"
  ],
  "year": "2020",
  "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
  "abstract": "Deep Neural Networks have been successfully used for the task of Visual Question Answering for the past few years owing to the availability of relevant large scale datasets. However these datasets are created in artificial settings and rarely reflect the real world scenario. Recent research effectively applies these VQA models for answering visual questions for the blind. Despite achieving high accuracy these models appear to be susceptible to variation in input questions. We analyze popular VQA models through the lens of attribution (inputâ€™s influence on predictions) to gain valuable insights. Further, We use these insights to craft adversarial attacks which inflict significant damage to these systems with negligible change in meaning of the input questions. We believe this will enhance development of systems more robust to the possible variations in inputs when deployed to assist the visually impaired.",
  "keywords": [
    "deep",
    "question",
    "deep neural networks",
    "we",
    "neural",
    "high accuracy",
    "visual",
    "accuracy",
    "popular vqa models",
    "these vqa models",
    "vqa models",
    "vqa",
    "significant damage",
    "visual questions",
    "scenario"
  ],
  "url": "https://aclanthology.org/2020.challengehml-1.9/",
  "provenance": {
    "collected_at": "2025-06-05 07:55:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}