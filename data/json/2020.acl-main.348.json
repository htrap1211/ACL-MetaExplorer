{
  "id": "2020.acl-main.348",
  "title": "Meta-Transfer Learning for Code-Switched Speech Recognition",
  "authors": [
    "Winata, Genta Indra  and\nCahyawijaya, Samuel  and\nLin, Zhaojiang  and\nLiu, Zihan  and\nXu, Peng  and\nFung, Pascale"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data. Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.",
  "keywords": [
    "code",
    "language",
    "language modeling tasks",
    "model",
    "modeling",
    "information",
    "optimization",
    "meta-transfer learning",
    "we",
    "learning",
    "transfer",
    "the optimization",
    "speech",
    "speech recognition",
    "recognition"
  ],
  "url": "https://aclanthology.org/2020.acl-main.348/",
  "provenance": {
    "collected_at": "2025-06-05 07:46:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}