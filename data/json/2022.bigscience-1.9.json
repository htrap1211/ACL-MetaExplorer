{
  "id": "2022.bigscience-1.9",
  "title": "GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model",
  "authors": [
    "Black, Sidney  and\nBiderman, Stella  and\nHallahan, Eric  and\nAnthony, Quentin  and\nGao, Leo  and\nGolding, Laurence  and\nHe, Horace  and\nLeahy, Connor  and\nMcDonell, Kyle  and\nPhang, Jason  and\nPieler, Michael  and\nPrashanth, Usvsn Sai  and\nPurohit, Shivanshu  and\nReynolds, Laria  and\nTow, Jonathan  and\nWang, Ben  and\nWeinbach, Samuel"
  ],
  "year": "2022",
  "venue": "Proceedings of BigScience Episode {\\#}5 -- Workshop on Challenges {\\&} Perspectives in Creating Large Language Models",
  "abstract": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20Bâ€™s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, athttps://github.com/EleutherAI/gpt-neox.",
  "keywords": [
    "work",
    "code",
    "knowledge",
    "language",
    "n",
    "model",
    "it",
    "gpt",
    "we",
    "evaluation",
    "time",
    "parameter",
    "training",
    "that",
    "-20"
  ],
  "url": "https://aclanthology.org/2022.bigscience-1.9/",
  "provenance": {
    "collected_at": "2025-06-05 08:39:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}