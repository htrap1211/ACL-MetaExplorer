{
  "id": "2021.acl-long.197",
  "title": "Self-Guided Contrastive Learning for {BERT} Sentence Representations",
  "authors": [
    "Kim, Taeuk  and\nYoo, Kang Min  and\nLee, Sang-goo"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers. In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations. Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors. Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning. We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. We also show it is efficient at inference and robust to domain shifts.",
  "keywords": [
    "embeddings",
    "work",
    "bert sentence representations",
    "the nlp landscape",
    "transformers",
    "nlp",
    "bert",
    "such pre-trained transformers",
    "it",
    "objective",
    "vectors",
    "efficient",
    "self",
    "token",
    "our method fine-tunes bert"
  ],
  "url": "https://aclanthology.org/2021.acl-long.197/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}