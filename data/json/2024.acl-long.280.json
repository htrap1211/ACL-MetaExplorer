{
  "id": "2024.acl-long.280",
  "title": "W}ave{C}oder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning",
  "authors": [
    "Yu, Zhaojian  and\nZhang, Xin  and\nShang, Ning  and\nHuang, Yangyu  and\nXu, Can  and\nZhao, Yishujie  and\nHu, Wenxiang  and\nYin, Qiufeng"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.",
  "keywords": [
    "code",
    "the generalization ability",
    "series",
    "we",
    "current",
    "instruction",
    "current instruction tuning methods",
    "code llms",
    "code large language models",
    "impressive capabilities",
    "llms",
    "abilities",
    "tuning",
    "generalization",
    "work"
  ],
  "url": "https://aclanthology.org/2024.acl-long.280/",
  "provenance": {
    "collected_at": "2025-06-05 10:38:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}