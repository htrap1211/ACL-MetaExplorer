{
  "id": "2021.unimplicit-1.5",
  "title": "Improvements and Extensions on Metaphor Detection",
  "authors": [
    "Ma, Weicheng  and\nLiu, Ruibo  and\nWang, Lili  and\nVosoughi, Soroush"
  ],
  "year": "2021",
  "venue": "Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language",
  "abstract": "Metaphors are ubiquitous in human language. The metaphor detection task (MD) aims at detecting and interpreting metaphors from written language, which is crucial in natural language understanding (NLU) research. In this paper, we introduce a pre-trained Transformer-based model into MD. Our model outperforms the previous state-of-the-art models by large margins in our evaluations, with relative improvements on the F-1 score from 5.33% to 28.39%. Second, we extend MD to a classification task about the metaphoricity of an entire piece of text to make MD applicable in more general NLU scenes. Finally, we clean up the improper or outdated annotations in one of the MD benchmark datasets and re-benchmark it with our Transformer-based model. This approach could be applied to other existing MD datasets as well, since the metaphoricity annotations in these benchmark datasets may be outdated. Future research efforts are also necessary to build an up-to-date and well-annotated dataset consisting of longer and more complex texts.",
  "keywords": [
    "a classification task",
    "more general nlu scenes",
    "we",
    "classification",
    "natural",
    "it",
    "a pre-trained transformer-based model",
    "natural language",
    "piece",
    "an entire piece",
    "text",
    "-",
    "our transformer-based model",
    "re",
    "transformer"
  ],
  "url": "https://aclanthology.org/2021.unimplicit-1.5/",
  "provenance": {
    "collected_at": "2025-06-05 08:23:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}