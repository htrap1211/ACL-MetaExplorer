{
  "id": "P18-1097",
  "title": "Fluency Boost Learning and Inference for Neural Grammatical Error Correction",
  "authors": [
    "Ge, Tao  and\nWei, Furu  and\nZhou, Ming"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence’s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence’s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.",
  "keywords": [
    "seq2seq",
    "neural",
    "fluency boosting inference",
    "a seq2seq model",
    "model",
    "boosting learning",
    "boosting",
    "normal seq2seq inference",
    "boost",
    "sequence",
    "we",
    "learning",
    "seq2seq models",
    "training",
    "normal"
  ],
  "url": "https://aclanthology.org/P18-1097/",
  "provenance": {
    "collected_at": "2025-06-05 00:11:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}