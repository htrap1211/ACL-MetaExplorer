{
  "id": "P19-1129",
  "title": "Entity-Relation Extraction as Multi-Turn Question Answering",
  "authors": [
    "Li, Xiaoya  and\nYin, Fan  and\nSun, Zijun  and\nLi, Xiayu  and\nYuan, Arianna  and\nChai, Duo  and\nZhou, Mingxin  and\nLi, Jiwei"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. Additionally, we construct and will release a newly developed dataset RESUME, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset.",
  "keywords": [
    "extraction",
    "question",
    "all",
    "we",
    "answer",
    "natural",
    "it",
    "dependencies",
    "information",
    "turn qa model",
    "secondly qa",
    "i",
    "answering",
    "model",
    "machine"
  ],
  "url": "https://aclanthology.org/P19-1129/",
  "provenance": {
    "collected_at": "2025-06-05 00:32:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}