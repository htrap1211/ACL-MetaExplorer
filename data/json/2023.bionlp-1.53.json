{
  "id": "2023.bionlp-1.53",
  "title": "SINAI} at {R}ad{S}um23: Radiology Report Summarization Based on Domain-Specific Sequence-To-Sequence Transformer Model",
  "authors": [
    "Chizhikova, Mariia  and\nDiaz-Galiano, Manuel  and\nUrena-Lopez, L. Alfonso  and\nMartin-Valdivia, M. Teresa"
  ],
  "year": "2023",
  "venue": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
  "abstract": "This paper covers participation of the SINAI team in the shared task 1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual general domain and monolingual biomedical domain pre-trained language models. The best performing system based on domain-specific model reached 33.96 F1RadGraph score which is the fourth best result among the challenge participants. This model was made publicly available on HuggingFace. We also describe an attempt of Proximal Policy Optimization Reinforcement Learning that was made in order to improve the factual correctness measured with F1RadGraph but did not lead to satisfactory results.",
  "keywords": [
    "transformer",
    "general",
    "reinforcement",
    "language",
    "pre-trained multilingual general domain",
    "model",
    "bionlp",
    "the bionlp workshop",
    "optimization",
    "summarization",
    "sequence",
    "we",
    "learning",
    "f1radgraph",
    "pre"
  ],
  "url": "https://aclanthology.org/2023.bionlp-1.53/",
  "provenance": {
    "collected_at": "2025-06-05 10:22:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}