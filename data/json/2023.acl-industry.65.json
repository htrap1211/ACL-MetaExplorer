{
  "id": "2023.acl-industry.65",
  "title": "NAG}-{NER}: a Unified Non-Autoregressive Generation Framework for Various {NER} Tasks",
  "authors": [
    "Zhang, Xinpeng  and\nTan, Ming  and\nZhang, Jingfan  and\nZhu, Wei"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
  "abstract": "Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to generate entities as a set instead of a sequence, avoiding error propagation. Second, we propose incorporating NAG in NER tasks for efficient decoding by treating each entity as a target sequence. Third, to enhance the generation performances of the NAG decoder, we employ the NAG encoder to detect potential entity mentions. Extensive experiments show that our NAG-NER model outperforms the state-of-the-art generative NER models on three benchmark NER datasets of different types and two of our proprietary NER tasks.\\footnote{Code will be publicly available to the research community upon acceptance.}",
  "keywords": [
    "code",
    "inefficient",
    "efficient",
    "field",
    "we",
    "current",
    "the entities",
    "nag - ner",
    "increasing attention",
    "three benchmark ner datasets",
    "unified",
    "decoder",
    "sequence",
    "generative",
    "ner"
  ],
  "url": "https://aclanthology.org/2023.acl-industry.65/",
  "provenance": {
    "collected_at": "2025-06-05 09:52:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}