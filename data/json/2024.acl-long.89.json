{
  "id": "2024.acl-long.89",
  "title": "Language Model Adaption for Reinforcement Learning with Natural Language Action Space",
  "authors": [
    "Wang, Jiangxing  and\nLi, Jiachen  and\nHan, Xiao  and\nYe, Deheng  and\nLu, Zongqing"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Reinforcement learning with natural language action space often suffers from the curse of dimensionality due to the combinatorial nature of the natural language. Previous research leverages pretrained language models to capture action semantics and reduce the size of the action space. However, since pretrained models are typically trained on general corpora, there can be an unpredictable mismatch between the priors encoded in pretrained models and the characteristics of the specific RL environment. To address this issue, we propose Mutual-Information Regularized Policy Optimization, MIPO. MIPO enables implicit and dynamic reduction of the action space. Starting from the prior provided by the pretrained language model, our method dynamically adjusts the prior during the learning process based on the guidance of mutual information regularization. Theoretically, we demonstrate that this policy optimization process leads to the monotonic improvement on the mutual-information regularized RL objective. Empirically, we conduct experiments in various environments and demonstrate the effectiveness of MIPO.",
  "keywords": [
    "general corpora",
    "general",
    "process",
    "reinforcement",
    "language",
    "the pretrained language model",
    "natural",
    "model",
    "semantics",
    "this policy optimization process",
    "dimensionality",
    "reduction",
    "language models",
    "mutual information regularization",
    "regularization"
  ],
  "url": "https://aclanthology.org/2024.acl-long.89/",
  "provenance": {
    "collected_at": "2025-06-05 10:35:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}