{
  "id": "2023.acl-industry.11",
  "title": "Label efficient semi-supervised conversational intent classification",
  "authors": [
    "Kulkarni, Mandar  and\nKim, Kyung  and\nGarera, Nikesh  and\nTrivedi, Anusua"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
  "abstract": "To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e., understanding the userâ€™s intention from the query text. Due to non-English speaking users interacting with the chatbot, we often get a significant percentage of code mix queries and queries with grammatical errors, which makes the problem more challenging. This paper proposes a simple yet competent Semi-Supervised Learning (SSL) approach for label-efficient intent classification. We use a small labeled corpus and relatively larger unlabeled query data to train a transformer model. For training the model with labeled data, we explore supervised MixUp data augmentation. To train with unlabeled data, we explore label consistency with dropout noise. We experiment with different pre-trained transformer architectures, such as BERT and sentence-BERT. Experimental results demonstrate that the proposed approach significantly improves over the supervised baseline, even with a limited labeled set. A variant of the model is currently deployed in production.",
  "keywords": [
    "variety",
    "code",
    "such queries",
    "related or post-purchase queries",
    "efficient",
    "dropout noise",
    "the user queries",
    "we",
    "classification",
    "code mix queries",
    "chatbot",
    "scale conversational platforms",
    "queries",
    "different pre-trained transformer architectures",
    "information"
  ],
  "url": "https://aclanthology.org/2023.acl-industry.11/",
  "provenance": {
    "collected_at": "2025-06-05 09:52:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}