{
  "id": "2024.findings-acl.50",
  "title": "Effective In-Context Example Selection through Data Compression",
  "authors": [
    "Sun, ZhongXiang  and\nZhang, Kepu  and\nWang, Haoyu  and\nZhang, Xiao  and\nXu, Jun"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "In-context learning has been extensively validated in large language models. However, the mechanism and selection strategy for in-context example selection, which is a crucial ingredient in this approach, lacks systematic and in-depth research. In this paper, we propose a data compression approach to the selection of in-context examples. We introduce a two-stage method that can effectively choose relevant examples and retain sufficient information about the training dataset within the in-context examples. Our method shows a significant improvement of an average of 5.90% across five different real-world datasets using four language models.",
  "keywords": [
    "language",
    "large language models",
    "information",
    "sufficient information",
    "sufficient",
    "four language models",
    "we",
    "learning",
    "a crucial ingredient",
    "ingredient",
    "training",
    "that",
    "a two-stage method",
    "the training dataset",
    "approach"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.50/",
  "provenance": {
    "collected_at": "2025-06-05 10:49:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}