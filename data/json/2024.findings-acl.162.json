{
  "id": "2024.findings-acl.162",
  "title": "Simulated Misinformation Susceptibility ({SMISTS}): Enhancing Misinformation Research with Large Language Model Simulations",
  "authors": [
    "Ma, Weicheng  and\nDeng, Chunyuan  and\nMoossavi, Aram  and\nWang, Lili  and\nVosoughi, Soroush  and\nYang, Diyi"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Psychological inoculation, a strategy designed to build resistance against persuasive misinformation, has shown efficacy in curbing its spread and mitigating its adverse effects at early stages. Despite its effectiveness, the design and optimization of these inoculations typically demand substantial human and financial resources, primarily due to the need for repeated experimental trials. To address these challenges, this paper introduces Simulated Misinformation Susceptibility Tests (SMISTs), leveraging Large Language Models (LLMs) to simulate participant responses in misinformation studies. SMIST employs a life experience-driven simulation methodology, which accounts for various aspects of participantsâ€™ backgrounds, to mitigate common issues of caricatures and stereotypes in LLM simulations and enhance response diversity. Our extensive experimentation demonstrates that SMIST, utilizing GPT-4 as the backend model, yields results that align closely with those obtained from human-subject studies in misinformation susceptibility. This alignment suggests that LLMs can effectively serve as proxies in evaluating the impact of psychological inoculations. Moreover, SMIST offers the critical benefit of being applicable to emerging or anticipated misinformation scenarios without exposing human participants to potentially harmful content. This characteristic of SMIST not only preserves participant safety but also expands the scope of misinformation research to include more sensitive or speculative topics.",
  "keywords": [
    "this alignment",
    "large language model simulations",
    "proxies",
    "llm",
    "backgrounds",
    "human-subject studies",
    "llms",
    "early",
    "misinformation studies",
    "participants backgrounds",
    "gpt-4",
    "experience",
    "llm simulations",
    "alignment",
    "language"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.162/",
  "provenance": {
    "collected_at": "2025-06-05 10:50:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}