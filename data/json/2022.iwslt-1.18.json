{
  "id": "2022.iwslt-1.18",
  "title": "NVIDIA} {N}e{M}o Offline Speech Translation Systems for {IWSLT} 2022",
  "authors": [
    "Hrinchuk, Oleksii  and\nNoroozi, Vahid  and\nKhattar, Abhinav  and\nPeganov, Anton  and\nSubramanian, Sandeep  and\nMajumdar, Somshubra  and\nKuchaiev, Oleksii"
  ],
  "year": "2022",
  "venue": "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)",
  "abstract": "This paper provides an overview of NVIDIA NeMo’s speech translation systems for the IWSLT 2022 Offline Speech Translation Task. Our cascade system consists of 1) Conformer RNN-T automatic speech recognition model, 2) punctuation-capitalization model based on pre-trained T5 encoder, 3) ensemble of Transformer neural machine translation models fine-tuned on TED talks. Our end-to-end model has less parameters and consists of Conformer encoder and Transformer decoder. It relies on the cascade system by re-using its pre-trained ASR encoder and training on synthetic translations generated with the ensemble of NMT models. Our En->De cascade and end-to-end systems achieve 29.7 and 26.2 BLEU on the 2020 test set correspondingly, both outperforming the previous year’s best of 26 BLEU.",
  "keywords": [
    "ensemble",
    "transformer",
    "its pre-trained asr encoder",
    "end",
    "translation",
    "pre-trained t5 encoder",
    "neural",
    "n",
    "3 ensemble",
    "model",
    "machine",
    "it",
    "an overview",
    "bleu",
    "encoder"
  ],
  "url": "https://aclanthology.org/2022.iwslt-1.18/",
  "provenance": {
    "collected_at": "2025-06-05 08:43:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}