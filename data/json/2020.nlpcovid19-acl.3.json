{
  "id": "2020.nlpcovid19-acl.3",
  "title": "Document Classification for {COVID-19} Literature",
  "authors": [
    "Jim{\\'e}nez Guti{\\'e}rrez, Bernal  and\nZeng, Juncheng  and\nZhang, Dongdong  and\nZhang, Ping  and\nSu, Yu"
  ],
  "year": "2020",
  "venue": "Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020",
  "abstract": "The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset. We find that pre-trained language models outperform other models in both low and high data regimes, achieving a maximum F1 score of around 86%. We note that even the highest performing models still struggle with label correlation, distraction from introductory text and CORD-19 generalization. Both data and code are available on GitHub.",
  "keywords": [
    "code",
    "language",
    "document classification",
    "pre-trained language models",
    "text",
    "it",
    "relevant scientific literature",
    "cord-19 generalization",
    "generalization",
    "a maximum f1 score",
    "we",
    "fields",
    "scientific",
    "pre",
    "analysis"
  ],
  "url": "https://aclanthology.org/2020.nlpcovid19-acl.3/",
  "provenance": {
    "collected_at": "2025-06-05 07:54:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}