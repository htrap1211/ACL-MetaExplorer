{
  "id": "2020.sigmorphon-1.1",
  "title": "SIGMORPHON} 2020 Shared Task 0: Typologically Diverse Morphological Inflection",
  "authors": [
    "Vylomova, Ekaterina  and\nWhite, Jennifer  and\nSalesky, Elizabeth  and\nMielke, Sabrina J.  and\nWu, Shijie  and\nPonti, Edoardo Maria  and\nMaudslay, Rowan Hall  and\nZmigrod, Ran  and\nValvoda, Josef  and\nToldova, Svetlana  and\nTyers, Francis  and\nKlyachko, Elena  and\nYegorov, Ilya  and\nKrizhanovsky, Natalia  and\nCzarnowska, Paula  and\nNikkarinen, Irene  and\nKrizhanovsky, Andrew  and\nPimentel, Tiago  and\nTorroba Hennigen, Lucas  and\nKirov, Christo  and\nNicolai, Garrett  and\nWilliams, Adina  and\nAnastasopoulos, Antonios  and\nCruz, Hilaria  and\nChodroff, Eleanor  and\nCotterell, Ryan  and\nSilfverberg, Miikka  and\nHulden, Mans"
  ],
  "year": "2020",
  "venue": "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
  "abstract": "A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systemsâ€™ ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.",
  "keywords": [
    "families",
    "transformers",
    "training",
    "natural language processing nlp",
    "neural",
    "natural",
    "neural two monolingual transformers",
    "over 90 mean accuracy",
    "ensembles",
    "processing",
    "non-neural learners",
    "just 5 language families",
    "gated attention",
    "rnn",
    "some language families"
  ],
  "url": "https://aclanthology.org/2020.sigmorphon-1.1/",
  "provenance": {
    "collected_at": "2025-06-05 07:58:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}