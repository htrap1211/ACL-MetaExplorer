{
  "id": "W18-3402",
  "title": "Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data",
  "authors": [
    "Hedderich, Michael A.  and\nKlakow, Dietrich"
  ],
  "year": "2018",
  "venue": "Proceedings of the Workshop on Deep Learning Approaches for Low-Resource {NLP}",
  "abstract": "Manually labeled corpora are expensive to create and often not available for low-resource languages or domains. Automatic labeling approaches are an alternative way to obtain labeled data in a quicker and cheaper way. However, these labels often contain more errors which can deteriorate a classifierâ€™s performance when trained on this data. We propose a noise layer that is added to a neural network architecture. This allows modeling the noise and train on a combination of clean and noisy data. We show that in a low-resource NER task we can improve performance by up to 35% by using additional, noisy data and handling the noise.",
  "keywords": [
    "a neural network architecture",
    "ner",
    "a neural network",
    "neural",
    "classifier",
    "a low-resource ner task",
    "layer",
    "network",
    "we",
    "a classifier s performance",
    "that",
    "an alternative way",
    "this",
    "alternative",
    "task"
  ],
  "url": "https://aclanthology.org/W18-3402/",
  "provenance": {
    "collected_at": "2025-06-05 00:08:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}