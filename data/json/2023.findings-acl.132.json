{
  "id": "2023.findings-acl.132",
  "title": "Nonparametric Masked Language Modeling",
  "authors": [
    "Min, Sewon  and\nShi, Weijia  and\nLewis, Mike  and\nChen, Xilun  and\nYih, Wen-tau  and\nHajishirzi, Hannaneh  and\nZettlemoyer, Luke"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.",
  "keywords": [
    "code",
    "retrieve",
    "existing language models",
    "we",
    "softmax",
    "shot",
    "classification",
    "a contrastive objective",
    "classification fact",
    "it",
    "token",
    "retrieval",
    "word",
    "answering",
    "language model"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.132/",
  "provenance": {
    "collected_at": "2025-06-05 09:55:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}