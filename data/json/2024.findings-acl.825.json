{
  "id": "2024.findings-acl.825",
  "title": "DADA}: Distribution-Aware Domain Adaptation of {PLM}s for Information Retrieval",
  "authors": [
    "Lee, Dohyeon  and\nKim, Jongyoon  and\nHwang, Seung-won  and\nPark, Joonsuk"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Pre-trained language models (PLMs) exhibit promise in retrieval tasks but struggle with out-of-domain data due to distribution shifts.Addressing this, generative domain adaptation (DA), known as GPL, tackles distribution shifts by generating pseudo queries and labels to train models for predicting query-document relationships in new domains.However, it overlooks the domain distribution, causing the model to struggle with aligning the distribution in the target domain.We, therefore, propose a Distribution-Aware Domain Adaptation (DADA) to guide the model to consider the domain distribution knowledge at the level of both a single document and the corpus, which is referred to as observation-level feedback and domain-level feedback, respectively.Our method effectively adapts the model to the target domain and expands document representation to unseen gold query terms using domain and observation feedback, as demonstrated by empirical results on the BEIR benchmark.",
  "keywords": [
    "generative",
    "knowledge",
    "retrieval tasks",
    "language",
    "model",
    "feedback",
    "it",
    "information retrieval",
    "queries",
    "retrieval",
    "information",
    "this generative domain adaptation",
    "pre-trained language models plms",
    "we",
    "pre"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.825/",
  "provenance": {
    "collected_at": "2025-06-05 10:59:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}