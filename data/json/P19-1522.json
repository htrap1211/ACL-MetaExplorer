{
  "id": "P19-1522",
  "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
  "authors": [
    "Yang, Sen  and\nFeng, Dawei  and\nQiao, Linbo  and\nKan, Zhigang  and\nLi, Dongsheng"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
  "keywords": [
    "process",
    "the f1 score",
    "language",
    "generation",
    "generated samples",
    "pre-trained language models",
    "extraction",
    "model",
    "it",
    "our generation method",
    "we",
    "learning",
    "pre",
    "argument classification",
    "insufficient"
  ],
  "url": "https://aclanthology.org/P19-1522/",
  "provenance": {
    "collected_at": "2025-06-05 00:43:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}