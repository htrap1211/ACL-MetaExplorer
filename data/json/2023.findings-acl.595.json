{
  "id": "2023.findings-acl.595",
  "title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification",
  "authors": [
    "Sun, Renliang  and\nXu, Wei  and\nWan, Xiaojun"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART. At the end, we compare SimpleBART with several representative large language models (LLMs).",
  "keywords": [
    "end",
    "language",
    "model",
    "text",
    "it",
    "the pre-training stage",
    "we",
    "pre",
    "pre-training bart",
    "training",
    "strategy",
    "new",
    "text simplification",
    "the results",
    "ability"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.595/",
  "provenance": {
    "collected_at": "2025-06-05 10:16:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}