{
  "id": "2021.semeval-1.4",
  "title": "S}em{E}val-2021 Task 4: Reading Comprehension of Abstract Meaning",
  "authors": [
    "Zheng, Boyuan  and\nYang, Xiaoyu  and\nRuan, Yu-Ping  and\nLing, Zhenhua  and\nLiu, Quan  and\nWei, Si  and\nZhu, Xiaodan"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "This paper introduces the SemEval-2021 shared task 4: Reading Comprehension of Abstract Meaning (ReCAM). This shared task is designed to help evaluate the ability of machines in representing and understanding abstract concepts. Given a passage and the corresponding question, a participating system is expected to choose the correct answer from five candidates of abstract concepts in cloze-style machine reading comprehension tasks. Based on two typical definitions of abstractness, i.e., the imperceptibility and nonspecificity, our task provides three subtasks to evaluate models’ ability in comprehending the two types of abstract meaning and the models’ generalizability. Specifically, Subtask 1 aims to evaluate how well a participating system models concepts that cannot be directly perceived in the physical world. Subtask 2 focuses on models’ ability in comprehending nonspecific concepts located high in a hypernym hierarchy given the context of a passage. Subtask 3 aims to provide some insights into models’ generalizability over the two types of abstractness. During the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 to Subtask 2. The participating teams additionally made 29 submissions to Subtask 3. The leaderboard and competition website can be found athttps://competitions.codalab.org/competitions/26153. The data and baseline code are available athttps://github.com/boyuanzheng010/SemEval2021-Reading-Comprehension-of-Abstract-Meaning.",
  "keywords": [
    "code",
    "em",
    "a hypernym hierarchy",
    "question",
    "we",
    "generalizability",
    "the models generalizability",
    "answer",
    "models generalizability",
    "i",
    "hierarchy",
    "machine",
    "evaluation",
    "models ability",
    "our task"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.4/",
  "provenance": {
    "collected_at": "2025-06-05 08:19:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}