{
  "id": "2021.acl-demo.38",
  "title": "LEGOE}val: An Open-Source Toolkit for Dialogue System Evaluation via Crowdsourcing",
  "authors": [
    "Li, Yu  and\nArnold, Josh  and\nYan, Feifan  and\nShi, Weiyan  and\nYu, Zhou"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
  "abstract": "We present LEGOEval, an open-source toolkit that enables researchers to easily evaluate dialogue systems in a few lines of code using the online crowdsource platform, Amazon Mechanical Turk. Compared to existing toolkits, LEGOEval features a flexible task design by providing a Python API that maps to commonly used React.js interface components. Researchers can personalize their evaluation procedures easily with our built-in pages as if playing with LEGO blocks. Thus, LEGOEval provides a fast, consistent method for reproducing human evaluation results. Besides the flexible task design, LEGOEval also offers an easy API to review collected data.",
  "keywords": [
    "code",
    "val",
    "human",
    "dialogue systems",
    "dialogue system evaluation",
    "human evaluation results",
    "we",
    "dialogue",
    "evaluation",
    "their evaluation procedures",
    "fast",
    "that",
    "task",
    "crowdsource",
    "a python api"
  ],
  "url": "https://aclanthology.org/2021.acl-demo.38/",
  "provenance": {
    "collected_at": "2025-06-05 08:10:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}