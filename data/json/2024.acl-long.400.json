{
  "id": "2024.acl-long.400",
  "title": "A}bout{M}e: Using Self-Descriptions in Webpages to Document the Effects of {E}nglish Pretraining Data Filters",
  "authors": [
    "Lucy, Li  and\nGururangan, Suchin  and\nSoldaini, Luca  and\nStrubell, Emma  and\nBamman, David  and\nKlein, Lauren  and\nDodge, Jesse"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Large language models’ (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten “quality” and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.",
  "keywords": [
    "classifiers",
    "some quality classifiers",
    "we",
    "self",
    "information",
    "e",
    "abilities",
    "text",
    "work",
    "language",
    "model",
    "large language models",
    "act",
    "topical",
    "implications"
  ],
  "url": "https://aclanthology.org/2024.acl-long.400/",
  "provenance": {
    "collected_at": "2025-06-05 10:39:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}