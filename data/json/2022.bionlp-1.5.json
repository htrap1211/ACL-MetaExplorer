{
  "id": "2022.bionlp-1.5",
  "title": "Zero-Shot Aspect-Based Scientific Document Summarization using Self-Supervised Pre-training",
  "authors": [
    "Soleimani, Amir  and\nNikoulina, Vassilina  and\nFavre, Benoit  and\nAit Mokhtar, Salah"
  ],
  "year": "2022",
  "venue": "Proceedings of the 21st Workshop on Biomedical Language Processing",
  "abstract": "We study the zero-shot setting for the aspect-based scientific document summarization task. Summarizing scientific documents with respect to an aspect can remarkably improve document assistance systems and readers experience. However, existing large-scale datasets contain a limited variety of aspects, causing summarization models to over-fit to a small set of aspects and a specific domain. We establish baseline results in zero-shot performance (over unseen aspects and the presence of domain shift), paraphrasing, leave-one-out, and limited supervised samples experimental setups. We propose a self-supervised pre-training approach to enhance the zero-shot performance. We leverage the PubMed structured abstracts to create a biomedical aspect-based summarization dataset. Experimental results on the PubMed and FacetSum aspect-based datasets show promising performance when the model is pre-trained using unlabelled in-domain data.",
  "keywords": [
    "the zero-shot performance",
    "scientific documents",
    "variety",
    "zero-shot performance",
    "a self-supervised pre-training approach",
    "model",
    "fit",
    "self",
    "the zero-shot setting",
    "readers experience",
    "summarization models",
    "a biomedical aspect-based summarization",
    "summarization",
    "we",
    "experience"
  ],
  "url": "https://aclanthology.org/2022.bionlp-1.5/",
  "provenance": {
    "collected_at": "2025-06-05 08:39:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}