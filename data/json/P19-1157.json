{
  "id": "P19-1157",
  "title": "Historical Text Normalization with Delayed Rewards",
  "authors": [
    "Flachs, Simon  and\nBollmann, Marcel  and\nS{\\o}gaard, Anders"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrase-based models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words.",
  "keywords": [
    "tuning",
    "normalization",
    "reinforcement",
    "neural",
    "text",
    "token",
    "gradient",
    "policy gradient training",
    "direct optimization",
    "optimization",
    "fine",
    "sequence",
    "we",
    "learning",
    "policy gradient fine-tuning"
  ],
  "url": "https://aclanthology.org/P19-1157/",
  "provenance": {
    "collected_at": "2025-06-05 00:33:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}