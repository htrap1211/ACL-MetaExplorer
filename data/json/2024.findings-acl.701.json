{
  "id": "2024.findings-acl.701",
  "title": "M}at{P}lot{A}gent: Method and Evaluation for {LLM}-Based Agentic Scientific Data Visualization",
  "authors": [
    "Yang, Zhiyu  and\nZhou, Zihan  and\nWang, Shuo  and\nCong, Xin  and\nHan, Xu  and\nYan, Yukun  and\nLiu, Zhenghao  and\nTan, Zhixing  and\nLiu, Pengyuan  and\nYu, Dong  and\nLiu, Zhiyuan  and\nShi, Xiaodong  and\nSun, Maosong"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.",
  "keywords": [
    "code",
    "feedback",
    "multi-modal llms matplotagent",
    "efficient",
    "field",
    "gpt-4v",
    "we",
    "llm",
    "verified",
    "information",
    "the capabilities",
    "visual",
    "scientific",
    "core",
    "llms"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.701/",
  "provenance": {
    "collected_at": "2025-06-05 10:58:10",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}