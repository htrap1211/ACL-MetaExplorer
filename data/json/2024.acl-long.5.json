{
  "id": "2024.acl-long.5",
  "title": "G}en{T}ranslate: Large Language Models are Generative Multilingual Speech and Machine Translators",
  "authors": [
    "Hu, Yuchen  and\nChen, Chen  and\nYang, Chao-Han Huck  and\nLi, Ruizhe  and\nZhang, Dong  and\nChen, Zhehuai  and\nChng, Eng Siong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely GenTranslate, which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the diverse N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.",
  "keywords": [
    "t",
    "machine translation",
    "we",
    "both translation tasks",
    "llm",
    "translation",
    "translation tasks",
    "information",
    "rich",
    "sequence",
    "592k hypotheses-translation pairs",
    "llms",
    "generative",
    "abilities",
    "a new generative paradigm"
  ],
  "url": "https://aclanthology.org/2024.acl-long.5/",
  "provenance": {
    "collected_at": "2025-06-05 10:34:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}