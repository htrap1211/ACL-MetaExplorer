{
  "id": "2022.fl4nlp-1.1",
  "title": "A}ct{P}er{FL}: Active Personalized Federated Learning",
  "authors": [
    "Chen, Huili  and\nDing, Jie  and\nTramel, Eric  and\nWu, Shuang  and\nSahu, Anit Kumar  and\nAvestimehr, Salman  and\nZhang, Tao"
  ],
  "year": "2022",
  "venue": "Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022)",
  "abstract": "In the context of personalized federated learning (FL), the critical challenge is to balance local model improvement and global model tuning when the personal and global objectives may not be exactly aligned. Inspired by Bayesian hierarchical models, we develop ActPerFL, a self-aware personalized FL method where each client can automatically balance the training of its local personal model and the global model that implicitly contributes to other clients’ training. Such a balance is derived from the inter-client and intra-client uncertainty quantification. Consequently, ActPerFL can adapt to the underlying clients’ heterogeneity with uncertainty-driven local training and model aggregation. With experimental studies on Sent140 and Amazon Alexa audio data, we show that ActPerFL can achieve superior personalization performance compared with the existing counterparts.",
  "keywords": [
    "the underlying clients",
    "hierarchical",
    "tuning",
    "objectives",
    "client",
    "model",
    "studies",
    "each client",
    "self",
    "clients",
    "other clients",
    "we",
    "learning",
    "bayesian hierarchical models",
    "experimental studies"
  ],
  "url": "https://aclanthology.org/2022.fl4nlp-1.1/",
  "provenance": {
    "collected_at": "2025-06-05 08:42:39",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}