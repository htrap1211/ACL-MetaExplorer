{
  "id": "2021.acl-long.239",
  "title": "Few-Shot Question Answering by Pretraining Span Selection",
  "authors": [
    "Ram, Ori  and\nKirstain, Yuval  and\nBerant, Jonathan  and\nGloberson, Amir  and\nLevy, Omer"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. We propose a new pretraining scheme tailored for question answering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.",
  "keywords": [
    "tuning",
    "objectives",
    "answer",
    "model",
    "current pretraining objectives",
    "human",
    "token",
    "question",
    "fine",
    "we",
    "72 7 f1",
    "fine-tuning",
    "current",
    "shot",
    "few-shot question"
  ],
  "url": "https://aclanthology.org/2021.acl-long.239/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}