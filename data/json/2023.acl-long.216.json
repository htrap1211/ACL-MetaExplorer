{
  "id": "2023.acl-long.216",
  "title": "WACO}: Word-Aligned Contrastive Learning for Speech Translation",
  "authors": [
    "Ouyang, Siqi  and\nYe, Rong  and\nLi, Lei"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST modelâ€™s performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available athttps://github.com/owaski/WACO.",
  "keywords": [
    "code",
    "end",
    "9 bleu",
    "model",
    "text",
    "modalities",
    "bleu",
    "its embedding similarity",
    "word",
    "we",
    "learning",
    "speech translation",
    "text modalities",
    "training",
    "translation"
  ],
  "url": "https://aclanthology.org/2023.acl-long.216/",
  "provenance": {
    "collected_at": "2025-06-05 09:38:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}