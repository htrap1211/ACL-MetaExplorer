{
  "id": "P18-2001",
  "title": "Continuous Learning in a Hierarchical Multiscale Neural Network",
  "authors": [
    "Wolf, Thomas  and\nChaumond, Julien  and\nDelangue, Clement"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework. We propose a hierarchical multi-scale language model in which short time-scale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer time-scale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion. We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework.",
  "keywords": [
    "hierarchical",
    "language",
    "neural",
    "a meta-learner update",
    "model",
    "it",
    "dependencies",
    "the lower-level neural network",
    "short time-scale dependencies",
    "longer time-scale dependencies",
    "network",
    "we",
    "learning",
    "sequence",
    "learner"
  ],
  "url": "https://aclanthology.org/P18-2001/",
  "provenance": {
    "collected_at": "2025-06-05 00:16:07",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}