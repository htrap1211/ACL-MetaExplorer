{
  "id": "2020.acl-main.408",
  "title": "ERASER}: {A} Benchmark to Evaluate Rationalized {NLP} Models",
  "authors": [
    "DeYoung, Jay  and\nJain, Sarthak  and\nRajani, Nazneen Fatema  and\nLehman, Eric  and\nXiong, Caiming  and\nSocher, Richard  and\nWallace, Byron C."
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose theEvaluatingRationalesAndSimpleEnglishReasoning (ERASERa benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also howfaithfulthese rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available athttps://www.eraserbenchmark.com/",
  "keywords": [
    "deep",
    "code",
    "deep neural networks",
    "we",
    "rationalized nlp models",
    "neural",
    "it",
    "e",
    "i",
    "metrics",
    "several metrics",
    "nlp",
    "model",
    "human",
    "more interpretable nlp systems"
  ],
  "url": "https://aclanthology.org/2020.acl-main.408/",
  "provenance": {
    "collected_at": "2025-06-05 07:47:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}