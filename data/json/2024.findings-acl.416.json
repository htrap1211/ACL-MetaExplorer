{
  "id": "2024.findings-acl.416",
  "title": "LL}a{ST}: Improved End-to-end Speech Translation System Leveraged by Large Language Models",
  "authors": [
    "Chen, Xi  and\nZhang, Songyang  and\nBai, Qibing  and\nChen, Kai  and\nNakamura, Satoshi"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "We introduces ***LLaST***, a framework for building high-performance Large Language model based Speech-to-text Translation systems. We address the limitations of end-to-end speech translation (E2E ST) models by exploring model architecture design and optimization techniques tailored for LLMs. Our approach includes LLM-based speech translation architecture design, ASR-augmented training, multilingual data augmentation, and dual-LoRA optimization. Our approach demonstrates superior performance on the CoVoST-2 benchmark and showcases exceptional scaling capabilities powered by LLMs.We believe this effective method will serve as a strong baseline for speech translation and provide insights for futureimprovements of the LLM-based speech translation framework.",
  "keywords": [
    "end",
    "language",
    "model",
    "text",
    "large language models",
    "dual-lora optimization",
    "capabilities",
    "optimization",
    "we",
    "speech translation",
    "exceptional scaling capabilities",
    "training",
    "translation",
    "llms",
    "this effective method"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.416/",
  "provenance": {
    "collected_at": "2025-06-05 10:54:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}