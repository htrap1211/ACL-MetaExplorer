{
  "id": "2024.acl-long.419",
  "title": "MARS}: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative {LLM}s",
  "authors": [
    "Bakman, Yavuz Faruk  and\nYaldiz, Duygu Nur  and\nBuyukates, Baturalp  and\nTao, Chenyang  and\nDimitriadis, Dimitrios  and\nAvestimehr, Salman"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found here.",
  "keywords": [
    "code",
    "generative llms",
    "question",
    "semantic",
    "we",
    "generative llm outputs",
    "llm",
    "five popular pre-trained llms",
    "sequence",
    "llms",
    "generative",
    "the generated sequence",
    "generative llm",
    "the semantic contribution",
    "function"
  ],
  "url": "https://aclanthology.org/2024.acl-long.419/",
  "provenance": {
    "collected_at": "2025-06-05 10:40:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}