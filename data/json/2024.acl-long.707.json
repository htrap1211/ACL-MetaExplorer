{
  "id": "2024.acl-long.707",
  "title": "Symbol-{LLM}: Towards Foundational Symbol-centric Interface For Large Language Models",
  "authors": [
    "Xu, Fangzhi  and\nWu, Zhiyong  and\nSun, Qiushi  and\nRen, Siyu  and\nYuan, Fei  and\nYuan, Shuai  and\nLin, Qika  and\nQiao, Yu  and\nLiu, Jun"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models.",
  "keywords": [
    "families",
    "series",
    "synergies",
    "we",
    "symbol-llm series models",
    "20 distinct symbolic families",
    "llm",
    "training",
    "natural",
    "it",
    "generality",
    "the boundaries",
    "loss",
    "llms",
    "different symbolic families"
  ],
  "url": "https://aclanthology.org/2024.acl-long.707/",
  "provenance": {
    "collected_at": "2025-06-05 10:44:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}