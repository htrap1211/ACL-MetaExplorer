{
  "id": "2024.findings-acl.18",
  "title": "Small Models are Valuable Plug-ins for Large Language Models",
  "authors": [
    "Xu, Canwen  and\nXu, Yichong  and\nWang, Shuohang  and\nLiu, Yang  and\nZhu, Chenguang  and\nMcAuley, Julian"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of supervised examples due to context length limits. In this paper, we propose Super In-Context Learning (SuperICL) which allows black-box LLMs to work with locally fine-tuned smaller models, resulting in superior performance on supervised tasks. Our experiments demonstrate that SuperICL can improve performance beyond state-of-the-art fine-tuned models while addressing the instability problem of in-context learning.",
  "keywords": [
    "language",
    "black-box llms",
    "large language models",
    "gpt-3",
    "large language models llms",
    "ins",
    "gpt-4",
    "we",
    "learning",
    "llms",
    "supervised",
    "supericl",
    "alternative",
    "a small number",
    "state"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.18/",
  "provenance": {
    "collected_at": "2025-06-05 10:48:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}