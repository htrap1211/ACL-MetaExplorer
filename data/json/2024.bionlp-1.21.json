{
  "id": "2024.bionlp-1.21",
  "title": "Get the Best out of 1{B} {LLM}s: Insights from Information Extraction on Clinical Documents",
  "authors": [
    "Farzi, Saeed  and\nGhosh, Soumitra  and\nLavelli, Alberto  and\nMagnini, Bernardo"
  ],
  "year": "2024",
  "venue": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
  "abstract": "While the popularity of large, versatile language models like ChatGPT continues to rise, the landscape shifts when considering open-source models tailored to specific domains. Moreover, many areas, such as clinical documents, suffer from a scarcity of training data, often amounting to only a few hundred instances. Additionally, in certain settings, such as hospitals, cloud-based solutions pose privacy concerns, necessitating the deployment of language models on traditional hardware, such as single GPUs or powerful CPUs. To address these complexities, we conduct extensive experiments on both clinical entity detection and relation extraction in clinical documents using 1B parameter models. Our study delves into traditional fine-tuning, continuous pre-training in the medical domain, and instruction-tuning methods, providing valuable insights into their effectiveness in a multilingual setting. Our results underscore the importance of domain-specific models and pre-training for clinical natural language processing tasks. Furthermore, data augmentation using cross-lingual information improves performance in most cases, highlighting the potential for multilingual enhancements.",
  "keywords": [
    "extraction",
    "we",
    "traditional fine-tuning continuous pre",
    "complexities",
    "llm",
    "parameter",
    "training",
    "instruction",
    "cross",
    "natural",
    "cloud",
    "llm s insights",
    "information",
    "chatgpt",
    "large versatile language models"
  ],
  "url": "https://aclanthology.org/2024.bionlp-1.21/",
  "provenance": {
    "collected_at": "2025-06-05 11:04:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}