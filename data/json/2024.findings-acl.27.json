{
  "id": "2024.findings-acl.27",
  "title": "I}nfi{MM}: Advancing Multimodal Understanding with an Open-Sourced Visual Language Model",
  "authors": [
    "Liu, Haogeng  and\nYou, Quanzeng  and\nWang, Yiqi  and\nHan, Xiaotian  and\nZhai, Bohan  and\nLiu, Yongfei  and\nChen, Wentao  and\nJian, Yiren  and\nTao, Yunzhe  and\nYuan, Jianbo  and\nHe, Ran  and\nYang, Hongxia"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "In this work, we present InfiMM, an advanced Multimodal Large Language Model that adapts to intricate vision-language tasks. InfiMM, inspired by the Flamingo architecture, distinguishes itself through the utilization of large-scale training data, comprehensive training strategies, and diverse large language models. This approach ensures the preservation of Flamingo’s foundational strengths while simultaneously introducing augmented capabilities. Empirical evaluations across a variety of benchmarks underscore InfiMM’s remarkable capability in multimodal understanding. The code can be found at: https://anonymous.4open.science/r/infimm-zephyr-F60C/.",
  "keywords": [
    "work",
    "variety",
    "code",
    "i",
    "language",
    "augmented capabilities empirical evaluations",
    "model",
    "evaluations",
    "large language models",
    "strategies",
    "science",
    "capabilities",
    "we",
    "a variety",
    "visual"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.27/",
  "provenance": {
    "collected_at": "2025-06-05 10:48:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}