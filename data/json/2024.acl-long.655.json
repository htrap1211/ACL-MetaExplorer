{
  "id": "2024.acl-long.655",
  "title": "One Prompt To Rule Them All: {LLM}s for Opinion Summary Evaluation",
  "authors": [
    "Siledar, Tejpalsingh  and\nNath, Swaroop  and\nMuddu, Sankara  and\nRangaraju, Rupasai  and\nNath, Swaprava  and\nBhattacharyya, Pushpak  and\nBanerjee, Suman  and\nPatil, Amey  and\nSingh, Sudhanshu  and\nChelliah, Muthusamy  and\nGarera, Nikesh"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Evaluation of opinion summaries using conventional reference-based metrics often fails to provide a comprehensive assessment and exhibits limited correlation with human judgments. While Large Language Models (LLMs) have shown promise as reference-free metrics for NLG evaluation, their potential remains unexplored for opinion summary evaluation. Furthermore, the absence of sufficient opinion summary evaluation datasets hinders progress in this area. In response, we introduce the SUMMEVAL-OP dataset, encompassing 7 dimensions crucial to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We propose OP-I-PROMPT, a dimension-independent prompt, along with OP-PROMPTS, a dimension-dependent set of prompts for opinion summary evaluation. Our experiments demonstrate that OP-I-PROMPT emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of 0.70 with human judgments, surpassing prior methodologies. Remarkably, we are the first to explore the efficacy of LLMs as evaluators, both on closed-source and open-source models, in the opinion summary evaluation domain.",
  "keywords": [
    "we",
    "prior methodologies",
    "opinion summaries",
    "one prompt",
    "nlg evaluation",
    "llm",
    "conventional reference-based metrics",
    "sufficient",
    "llms",
    "prompt",
    "i",
    "metrics",
    "large language models llms",
    "opinion summary evaluation",
    "reference-free metrics"
  ],
  "url": "https://aclanthology.org/2024.acl-long.655/",
  "provenance": {
    "collected_at": "2025-06-05 10:43:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}