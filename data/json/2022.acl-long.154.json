{
  "id": "2022.acl-long.154",
  "title": "Efficient Cluster-Based $k$-Nearest-Neighbor Machine Translation",
  "authors": [
    "Wang, Dexin  and\nFan, Kai  and\nChen, Boxing  and\nXiong, Deyi"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation (NMT). It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed from in-domain data. Previous studies (Khandelwal et al., 2021; Zheng et al., 2021) have already demonstrated that non-parametric NMT is even superior to models fine-tuned on out-of-domain data. In spite of this success,kNN retrieval is at the expense of high latency, in particular for large datastores. To make it practical, in this paper, we explore a more efficientkNN-MT and propose to use clustering to improve the retrieval efficiency. Concretely, we first propose a cluster-based Compact Network for feature reduction in a contrastive learning manner to compress context features into 90+% lower dimensional vectors. We then suggest a cluster-based pruning solution to filter out 10% 40% redundant nodes in large datastores while retaining translation quality. Our proposed methods achieve better or comparable performance while reducing up to 57% inference latency against the advanced non-parametric MT model on several machine translation benchmarks. Experimental results indicate that the proposed methods maintain the most useful information of the original datastore and the Compact Network shows good generalization on unseen domains. Codes are available athttps://github.com/tjunlp-lab/PCKMT.",
  "keywords": [
    "the retrieval efficiency",
    "efficient",
    "good generalization",
    "efficiency",
    "we",
    "translation",
    "a contrastive learning manner",
    "neural",
    "several machine translation",
    "cluster",
    "90 lower dimensional vectors",
    "it",
    "token",
    "retrieval",
    "information"
  ],
  "url": "https://aclanthology.org/2022.acl-long.154/",
  "provenance": {
    "collected_at": "2025-06-05 08:26:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}