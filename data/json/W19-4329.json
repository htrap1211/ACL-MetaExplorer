{
  "id": "W19-4329",
  "title": "Learning Word Embeddings without Context Vectors",
  "authors": [
    "Zobnin, Alexey  and\nElistratova, Evgenia"
  ],
  "year": "2019",
  "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
  "abstract": "Most word embedding algorithms such as word2vec or fastText construct two sort of vectors: for words and for contexts. Naive use of vectors of only one sort leads to poor results. We suggest using indefinite inner product in skip-gram negative sampling algorithm. This allows us to use only one sort of vectors without loss of quality. Our “context-free” cf algorithm performs on par with SGNS on word similarity datasets",
  "keywords": [
    "embeddings",
    "vectors",
    "par",
    "loss",
    "us",
    "fasttext",
    "inner",
    "indefinite inner product",
    "we",
    "word",
    "word embeddings",
    "word2vec",
    "context vectors",
    "sgns",
    "naive"
  ],
  "url": "https://aclanthology.org/W19-4329/",
  "provenance": {
    "collected_at": "2025-06-05 01:03:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}