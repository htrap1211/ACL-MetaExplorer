{
  "id": "2024.acl-long.546",
  "title": "Meta-Task Prompting Elicits Embeddings from Large Language Models",
  "authors": [
    "Lei, Yibin  and\nWu, Di  and\nZhou, Tianyi  and\nShen, Tao  and\nCao, Yu  and\nTao, Chongyang  and\nYates, Andrew"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We introduce a new unsupervised text embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law, offering a versatile and resource-efficient approach for embedding generation across diverse scenarios.",
  "keywords": [
    "embeddings",
    "embedding generation",
    "prompts",
    "meta-task prompting metaeol",
    "a series",
    "tuning",
    "model fine-tuning",
    "language",
    "generation",
    "model",
    "text",
    "efficient",
    "large language models",
    "elicits embeddings",
    "series"
  ],
  "url": "https://aclanthology.org/2024.acl-long.546/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}