{
  "id": "2021.acl-short.71",
  "title": "A Mixture-of-Experts Model for Antonym-Synonym Discrimination",
  "authors": [
    "Xie, Zhipeng  and\nZeng, Nan"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "Discrimination between antonyms and synonyms is an important and challenging NLP task. Antonyms and synonyms often share the same or similar contexts and thus are hard to make a distinction. This paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. It works on the basis of a divide-and-conquer strategy, where a number of localized experts focus on their own domains (or subspaces) to learn their specialties, and a gating mechanism determines the space partitioning and the expert mixture. Experimental results have shown that our method achieves the state-of-the-art performance on the task.",
  "keywords": [
    "nlp",
    "model",
    "it",
    "their specialties",
    "specialties",
    "basis",
    "gating",
    "contexts",
    "state",
    "the-art",
    "distinction",
    "task",
    "expert",
    "localized experts",
    "number"
  ],
  "url": "https://aclanthology.org/2021.acl-short.71/",
  "provenance": {
    "collected_at": "2025-06-05 08:08:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}