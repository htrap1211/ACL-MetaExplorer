{
  "id": "2023.acl-industry.61",
  "title": "R}ad{L}ing: Towards Efficient Radiology Report Understanding",
  "authors": [
    "Ghosh, Rikhiya  and\nFarri, Oladimeji  and\nKarn, Sanjeev Kumar  and\nDanu, Manuela  and\nVunikili, Ramya  and\nMicu, Larisa"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
  "abstract": "Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for fine tuning tasks in radiology domain. Our main contribution in this paper is knowledge-aware masking which is an taxonomic knowledge-assisted pre-training task that dynamically masks tokens to inject knowledge during pretraining. In addition, we also introduce an knowledge base-aided vocabulary extension to adapt the general tokenization vocabulary to radiology domain.",
  "keywords": [
    "tuning",
    "knowledge",
    "general",
    "language",
    "tokenization",
    "natural",
    "model",
    "fine-tuning tasks",
    "efficient",
    "the general tokenization vocabulary",
    "fine",
    "efficient radiology report",
    "we",
    "pre",
    "few pretrained language models"
  ],
  "url": "https://aclanthology.org/2023.acl-industry.61/",
  "provenance": {
    "collected_at": "2025-06-05 09:52:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}