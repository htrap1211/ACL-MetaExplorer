{
  "id": "W19-4302",
  "title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",
  "authors": [
    "Peters, Matthew E.  and\nRuder, Sebastian  and\nSmith, Noah A."
  ],
  "year": "2019",
  "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
  "abstract": "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.",
  "keywords": [
    "work",
    "practitioner",
    "different pretraining objectives",
    "diverse nlp tasks",
    "objectives",
    "nlp",
    "extraction",
    "model",
    "fine",
    "we",
    "transfer",
    "fine-tuning vs feature extraction",
    "the nlp practitioner",
    "pretrained representations",
    "the pretrained model"
  ],
  "url": "https://aclanthology.org/W19-4302/",
  "provenance": {
    "collected_at": "2025-06-05 01:02:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}