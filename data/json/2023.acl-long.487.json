{
  "id": "2023.acl-long.487",
  "title": "bg{GLUE}: A {B}ulgarian General Language Understanding Evaluation Benchmark",
  "authors": [
    "Hardalov, Momchil  and\nAtanasova, Pepa  and\nMihaylov, Todor  and\nAngelova, Galia  and\nSimov, Kiril  and\nOsenova, Petya  and\nStoyanov, Veselin  and\nKoychev, Ivan  and\nNakov, Preslav  and\nRadev, Dragomir"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard athttps://bgglue.github.io, and we hope that it will enable further advancements in developing NLU models for Bulgarian.",
  "keywords": [
    "the evaluation results",
    "variety",
    "the fine-tuning",
    "code",
    "tuning",
    "the first systematic evaluation",
    "general",
    "language",
    "nlp",
    "pre-trained language models",
    "natural",
    "machine",
    "it",
    "language models",
    "question"
  ],
  "url": "https://aclanthology.org/2023.acl-long.487/",
  "provenance": {
    "collected_at": "2025-06-05 09:42:01",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}