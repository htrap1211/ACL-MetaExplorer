{
  "id": "2022.bionlp-1.14",
  "title": "SNP}2{V}ec: Scalable Self-Supervised Pre-Training for Genome-Wide Association Study",
  "authors": [
    "Cahyawijaya, Samuel  and\nYu, Tiezheng  and\nLiu, Zihan  and\nZhou, Xiaopu  and\nMak, Tze Wing Tiffany  and\nIp, Yuk Yu Nancy  and\nFung, Pascale"
  ],
  "year": "2022",
  "venue": "Proceedings of the 21st Workshop on Biomedical Language Processing",
  "abstract": "Self-supervised pre-training methods have brought remarkable breakthroughs in the understanding of text, image, and speech. Recent developments in genomics has also adopted these pre-training methods for genome understanding. However, they focus only on understanding haploid sequences, which hinders their applicability towards understanding genetic variations, also known as single nucleotide polymorphisms (SNPs), which is crucial for genome-wide association study. In this paper, we introduce SNP2Vec, a scalable self-supervised pre-training approach for understanding SNP. We apply SNP2Vec to perform long-sequence genomics modeling, and we evaluate the effectiveness of our approach on predicting Alzheimerâ€™s disease risk in a Chinese cohort. Our approach significantly outperforms existing polygenic risk score methods and all other baselines, including the model that is trained entirely with haploid sequences.",
  "keywords": [
    "model",
    "text",
    "self",
    "modeling",
    "-",
    "we",
    "sequence",
    "pre",
    "these pre-training methods",
    "training",
    "that",
    "speech",
    "polymorphisms",
    "disease",
    "a chinese cohort"
  ],
  "url": "https://aclanthology.org/2022.bionlp-1.14/",
  "provenance": {
    "collected_at": "2025-06-05 08:39:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}