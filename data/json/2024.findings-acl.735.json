{
  "id": "2024.findings-acl.735",
  "title": "Non-Autoregressive Machine Translation as Constrained {HMM",
  "authors": [
    "Li, Haoran  and\nJie, Zhanming  and\nLu, Wei"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "In non-autoregressive translation (NAT), directed acyclic Transformers (DAT) have demonstrated their ability to achieve comparable performance to the autoregressive Transformers.In this paper, we first show that DAT is essentially a fully connected left-to-right Hidden Markov Model (HMM), with the source and target sequences being observations and the token positions being latent states.Even though generative models like HMM do not suffer from label bias in traditional task settings (e.g., sequence labeling), we argue here that the left-to-right HMM in NAT may still encounter this issue due to the missing observations at the inference stage.To combat label bias, we propose two constrained HMMs: 1) Adaptive Window HMM, which explicitly balances the number of outgoing transitions at different states; 2) Bi-directional HMM, i.e., a combination of left-to-right and right-to-left HMMs, whose uni-directional components can implicitly regularize each other’s biases via shared parameters.Experimental results on WMT’14 EnDe and WMT’17 ZhEn demonstrate that our methods can achieve better or comparable performance to the original DAT using various decoding methods.We also demonstrate that our methods effectively reduce the impact of label bias.",
  "keywords": [
    "generative models",
    "transformers",
    "bias",
    "uni",
    "we",
    "dat",
    "translation",
    "each other s biases",
    "ende",
    "token",
    "latent",
    "sequence",
    "label bias",
    "generative",
    "i"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.735/",
  "provenance": {
    "collected_at": "2025-06-05 10:58:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}