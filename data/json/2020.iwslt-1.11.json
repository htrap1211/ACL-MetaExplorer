{
  "id": "2020.iwslt-1.11",
  "title": "The {AFRL} {IWSLT} 2020 Systems: Work-From-Home Edition",
  "authors": [
    "Ore, Brian  and\nHansen, Eric  and\nAnderson, Tim  and\nGwinnup, Jeremy"
  ],
  "year": "2020",
  "venue": "Proceedings of the 17th International Conference on Spoken Language Translation",
  "abstract": "This report summarizes the Air Force Research Laboratory (AFRL) submission to the offline spoken language translation (SLT) task as part of the IWSLT 2020 evaluation campaign. As in previous years, we chose to adopt the cascade approach of using separate systems to perform speech activity detection, automatic speech recognition, sentence segmentation, and machine translation. All systems were neural based, including a fully-connected neural network for speech activity detection, a Kaldi factorized time delay neural network with recurrent neural network (RNN) language model rescoring for speech recognition, a bidirectional RNN with attention mechanism for sentence segmentation, and transformer networks trained with OpenNMT and Marian for machine translation. Our primary submission yielded BLEU scores of 21.28 on tst2019 and 23.33 on tst2020.",
  "keywords": [
    "work",
    "transformer",
    "transformer networks",
    "a fully-connected neural network",
    "language",
    "neural",
    "model",
    "machine",
    "attention mechanism",
    "bleu",
    "machine translation",
    "network",
    "attention",
    "we",
    "bleu scores"
  ],
  "url": "https://aclanthology.org/2020.iwslt-1.11/",
  "provenance": {
    "collected_at": "2025-06-05 07:56:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}