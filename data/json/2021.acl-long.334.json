{
  "id": "2021.acl-long.334",
  "title": "B}inary{BERT}: Pushing the Limit of {BERT} Quantization",
  "authors": [
    "Bai, Haoli  and\nZhang, Wei  and\nHou, Lu  and\nShang, Lifeng  and\nJin, Jin  and\nJiang, Xin  and\nLiu, Qun  and\nLyu, Michael  and\nKing, Irwin"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released.",
  "keywords": [
    "code",
    "precision",
    "language",
    "large pre-trained language models",
    "inary bert",
    "bert",
    "model",
    "our binarybert",
    "a binary bert",
    "loss",
    "drop",
    "fine",
    "network",
    "we",
    "the full-precision model"
  ],
  "url": "https://aclanthology.org/2021.acl-long.334/",
  "provenance": {
    "collected_at": "2025-06-05 08:03:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}