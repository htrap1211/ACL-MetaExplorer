{
  "id": "2023.acl-long.226",
  "title": "UTC}-{IE}: A Unified Token-pair Classification Architecture for Information Extraction",
  "authors": [
    "Yan, Hang  and\nSun, Yu  and\nLi, Xiaonan  and\nZhou, Yunhua  and\nHuang, Xuanjing  and\nQiu, Xipeng"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Information Extraction (IE) spans several tasks with different output structures, such as named entity recognition, relation extraction and event extraction. Previously, those tasks were solved with different models because of diverse task output structures. Through re-examining IE tasks, we find that all of them can be interpreted as extracting spans and span relations. They can further be decomposed into token-pair classification tasks by using the start and end token of a span to pinpoint the span, and using the start-to-start and end-to-end token pairs of two spans to determine the relation. Based on the reformulation, we propose a Unified Token-pair Classification architecture for Information Extraction (UTC-IE), where we introduce Plusformer on top of the token-pair feature matrix. Specifically, it models axis-aware interaction with plus-shaped self-attention and local interaction with Convolutional Neural Network over token pairs. Experiments show that our approach outperforms task-specific and unified models on all tasks in 10 datasets, and achieves better or comparable results on 2 joint IE datasets. Moreover, UTC-IE speeds up over state-of-the-art models on IE tasks significantly in most datasets, which verifies the effectiveness of our architecture.",
  "keywords": [
    "top",
    "token-pair classification tasks",
    "end",
    "extraction",
    "information extraction utc",
    "information extraction information extraction",
    "2 joint ie datasets",
    "all",
    "we",
    "convolutional",
    "classification",
    "neural",
    "it",
    "self",
    "token"
  ],
  "url": "https://aclanthology.org/2023.acl-long.226/",
  "provenance": {
    "collected_at": "2025-06-05 09:38:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}