{
  "id": "2020.challengehml-1.4",
  "title": "Low Rank Fusion based Transformers for Multimodal Sequences",
  "authors": [
    "Sahay, Saurav  and\nOkur, Eda  and\nH Kumar, Shachi  and\nNachman, Lama"
  ],
  "year": "2020",
  "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
  "abstract": "Our senses individually work in a coordinated fashion to express our emotional intentions. In this work, we experiment with modeling modality-specific sensory signals to attend to our latent multimodal emotional intentions and vice versa expressed via low-rank multimodal fusion and multimodal transformers. The low-rank factorization of multimodal fusion amongst the modalities helps represent approximate multiplicative latent signal interactions. Motivated by the work of (CITATION) and (CITATION), we present our transformer-based cross-fusion architecture without any over-parameterization of the model. The low-rank fusion helps represent the latent signal interactions while the modality-specific attention helps focus on relevant parts of the signal. We present two methods for the Multimodal Sentiment and Emotion Recognition results on CMU-MOSEI, CMU-MOSI, and IEMOCAP datasets and show that our models have lesser parameters, train faster and perform comparably to many larger fusion-based architectures.",
  "keywords": [
    "the modalities",
    "transformers",
    "our transformer-based cross-fusion architecture",
    "we",
    "fusion",
    "cross",
    "the modality-specific attention",
    "latent",
    "sentiment",
    "work",
    "transformer",
    "multimodal transformers",
    "model",
    "iemocap datasets",
    "modalities"
  ],
  "url": "https://aclanthology.org/2020.challengehml-1.4/",
  "provenance": {
    "collected_at": "2025-06-05 07:55:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}