{
  "id": "2024.loresmt-1.20",
  "title": "I}rish-based Large Language Model with Extreme Low-Resource Settings in Machine Translation",
  "authors": [
    "Tran, Khanh-Tung  and\nO{'}Sullivan, Barry  and\nNguyen, Hoang"
  ],
  "year": "2024",
  "venue": "Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)",
  "abstract": "Large Language Models (LLMs) have demonstrated exceptional performances in a wide range of natural language processing tasks. However, their success does not always extend to machine translation, particularly in challenging scenarios such as translating low-resource languages. This study investigates the multilingual capability of LLMs, with a case study on Irish, an extremely low-resource language, focusing on translation tasks between English and Irish. We propose a dynamic, efficient language adaptation framework for English-centric LLMs, which involves layer-specific adjustments and subsequent fine-tuning for machine translation. Our findings highlight several key insights: (1) different layers in the LLM serve distinct functions such as language understanding and task reasoning, (2) effective translation requires extensive pre-training on both source and target languages, and (3) targeted fine-tuning for machine translation leads to significant improvements of 36.7% for English to Irish and 133.4% for Irish to English compared to the previous state-of-the-art.",
  "keywords": [
    "natural language processing tasks",
    "efficient",
    "machine translation",
    "layer",
    "english-centric llms",
    "we",
    "llm",
    "training",
    "translation",
    "rish-based large language model",
    "translation tasks",
    "natural",
    "llms",
    "processing",
    "2 effective translation"
  ],
  "url": "https://aclanthology.org/2024.loresmt-1.20/",
  "provenance": {
    "collected_at": "2025-06-05 11:08:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}