{
  "id": "2020.iwpt-1.2",
  "title": "Distilling Neural Networks for Greener and Faster Dependency Parsing",
  "authors": [
    "Anderson, Mark  and\nG{\\'o}mez-Rodr{\\'i}guez, Carlos"
  ],
  "year": "2020",
  "venue": "Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies",
  "abstract": "The carbon footprint of natural language processing research has been increasing in recent years due to its reliance on large and inefficient neural network implementations. Distillation is a network compression technique which attempts to impart knowledge from a large model to a smaller one. We use teacher-student distillation to improve the efficiency of the Biaffine dependency parser which obtains state-of-the-art performance with respect to accuracy and parsing speed (Dozat and Manning, 2017). When distilling to 20% of the original model’s trainable parameters, we only observe an average decrease of ∼1 point for both UAS and LAS across a number of diverse Universal Dependency treebanks while being 2.30x (1.19x) faster than the baseline model on CPU (GPU) at inference time. We also observe a small increase in performance when compressing to 80% for some treebanks. Finally, through distillation we attain a parser which is not only faster but also more accurate than the fastest modern parser on the Penn Treebank.",
  "keywords": [
    "natural language processing research",
    "inefficient",
    "greener and faster dependency",
    "efficiency",
    "we",
    "neural",
    "natural",
    "the efficiency",
    "las",
    "processing",
    "neural networks",
    "greener",
    "accuracy",
    "knowledge",
    "language"
  ],
  "url": "https://aclanthology.org/2020.iwpt-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 07:56:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}