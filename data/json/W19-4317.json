{
  "id": "W19-4317",
  "title": "An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection",
  "authors": [
    "Garcia-Silva, Andres  and\nBerrio, Cristian  and\nG{\\'o}mez-P{\\'e}rez, Jos{\\'e} Manuel"
  ],
  "year": "2019",
  "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
  "abstract": "Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such language models are learned from large and well-formed text corpora from e.g. encyclopedic resources, books or news. However, a significant amount of the text to be analyzed nowadays is Web data, often from social media. In this paper we consider the research question: How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in social media? To answer this question, we focus on bot detection in Twitter as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.",
  "keywords": [
    "question",
    "such language models",
    "we",
    "peculiarities",
    "lstm",
    "the peculiarities",
    "neural",
    "cnn",
    "fine-tuning approaches",
    "our evaluation task",
    "tuning",
    "text",
    "language models",
    "-trained and contextualized embeddings",
    "bot"
  ],
  "url": "https://aclanthology.org/W19-4317/",
  "provenance": {
    "collected_at": "2025-06-05 01:03:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}