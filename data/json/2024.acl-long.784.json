{
  "id": "2024.acl-long.784",
  "title": "Distributional Inclusion Hypothesis and Quantifications: Probing for Hypernymy in Functional Distributional Semantics",
  "authors": [
    "Lo, Chun Hei  and\nLam, Wai  and\nCheng, Hong  and\nEmerson, Guy"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Functional Distributional Semantics (FDS) models the meaning of words by truth-conditional functions. This provides a natural representation for hypernymy but no guarantee that it can be learnt when FDS models are trained on a corpus. In this paper, we probe into FDS models and study the representations learnt, drawing connections between quantifications, the Distributional Inclusion Hypothesis (DIH), and the variational-autoencoding objective of FDS model training. Using synthetic data sets, we reveal that FDS models learn hypernymy on a restricted class of corpus that strictly follows the DIH. We further introduce a training objective that both enables hypernymy learning under the reverse of the DIH and improves hypernymy detection from real corpora.",
  "keywords": [
    "the variational-autoencoding objective",
    "natural",
    "model",
    "it",
    "semantics",
    "objective",
    "class",
    "a training objective",
    "we",
    "learning",
    "training",
    "that",
    "truth",
    "this",
    "distributional"
  ],
  "url": "https://aclanthology.org/2024.acl-long.784/",
  "provenance": {
    "collected_at": "2025-06-05 10:45:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}