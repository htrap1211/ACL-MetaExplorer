{
  "id": "2023.acl-long.85",
  "title": "On Prefix-tuning for Lightweight Out-of-distribution Detection",
  "authors": [
    "Ouyang, Yawen  and\nCao, Yongchang  and\nGao, Yuan  and\nWu, Zhen  and\nZhang, Jianbing  and\nDai, Xinyu"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario. In this paper, we depart from the classic fine-tuning based OOD detection toward a parameter-efficient alternative, and propose an unsupervised prefix-tuning based OOD detection framework termed PTO. Additionally, to take advantage of optional training data labels and targeted OOD data, two practical extensions of PTO are further proposed. Overall, PTO and its extensions offer several key advantages of being lightweight, easy-to-reproduce, and theoretically justified. Experimental results show that our methods perform comparably to, even better than, existing fine-tuning based OOD detection approaches under a wide range of metrics, detection settings, and OOD types.",
  "keywords": [
    "tuning",
    "growing attention",
    "nlp",
    "it",
    "efficient",
    "metrics",
    "metrics detection settings",
    "attention",
    "we",
    "a parameter-efficient alternative",
    "parameter",
    "training",
    "a fundamental task",
    "applications",
    "promising"
  ],
  "url": "https://aclanthology.org/2023.acl-long.85/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}