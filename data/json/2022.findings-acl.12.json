{
  "id": "2022.findings-acl.12",
  "title": "Going ``Deeper'': Structured Sememe Prediction via Transformer with Tree Attention",
  "authors": [
    "Ye, Yining  and\nQi, Fanchao  and\nLiu, Zhiyuan  and\nSun, Maosong"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Sememe knowledge bases (SKBs), which annotate words with the smallest semantic units (i.e., sememes), have proven beneficial to many NLP tasks. Building an SKB is very time-consuming and labor-intensive. Therefore, some studies have tried to automate the building process by predicting sememes for the unannotated words. However, all existing sememe prediction studies ignore the hierarchical structures of sememes, which are important in the sememe-based semantic description system. In this work, we tackle the structured sememe prediction problem for the first time, which is aimed at predicting a sememe tree with hierarchical structures rather than a set of sememes. We design a sememe tree generation model based on Transformer with adjusted attention mechanism, which shows its superiority over the baselines in experiments. We also conduct a series of quantitative and qualitative analyses of the effectiveness of our model. All the code and data of this paper are available athttps://github.com/thunlp/STG.",
  "keywords": [
    "work",
    "transformer",
    "hierarchical",
    "code",
    "a series",
    "process",
    "knowledge",
    "generation",
    "nlp",
    "model",
    "studies",
    "adjusted attention mechanism",
    "the hierarchical structures",
    "some studies",
    "many nlp tasks"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.12/",
  "provenance": {
    "collected_at": "2025-06-05 08:35:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}