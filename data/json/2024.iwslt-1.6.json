{
  "id": "2024.iwslt-1.6",
  "title": "The {NYA}{'}s Offline Speech Translation System for {IWSLT} 2024",
  "authors": [
    "Zhang, Yingxin  and\nMa, Guodong  and\nDu, Binbin"
  ],
  "year": "2024",
  "venue": "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
  "abstract": "This paper reports the NYAâ€™s submissions to IWSLT 2024 Offline Speech Translation (ST) task on the sub-tasks including English to Chinese, Japanese, and German. In detail, we participate in the unconstrained training track using the cascaded ST structure. For the automatic speech recognition (ASR) model, we use the Whisper large-v3 model. For the neural machine translation (NMT) model, the wider and deeper Transformer is adapted as the backbone model. Furthermore, we use data augmentation technologies to augment training data and data filtering strategies to improve the quality of training data. In addition, we explore many MT technologies such as Back Translation, Forward Translation, R-Drop, and Domain Adaptation.",
  "keywords": [
    "transformer",
    "neural",
    "many mt technologies",
    "model",
    "machine",
    "strategies",
    "-",
    "drop",
    "technologies",
    "we",
    "data augmentation technologies",
    "training",
    "translation",
    "speech",
    "recognition"
  ],
  "url": "https://aclanthology.org/2024.iwslt-1.6/",
  "provenance": {
    "collected_at": "2025-06-05 11:06:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}