{
  "id": "2021.semeval-1.15",
  "title": "U}ppsala {NLP} at {S}em{E}val-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation",
  "authors": [
    "You, Huiling  and\nZhu, Xingran  and\nStymne, Sara"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual and cross-lingual word-in-context disambiguation. We explore the usefulness of three pre-trained multilingual language models, XLM-RoBERTa (XLMR), Multilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We compare these three models in two setups, fine-tuning and as feature extractors. In the second case we also experiment with using dependency-based information. We find that fine-tuning is better than feature extraction. XLMR performs better than mBERT in the cross-lingual setting both with fine-tuning and feature extraction, whereas these two models give a similar performance in the multilingual setting. mDistilBERT performs poorly with fine-tuning but gives similar results to the other models when used as a feature extractor. We submitted our two best systems, fine-tuned with XLMR and mBERT.",
  "keywords": [
    "cross",
    "tuning",
    "roberta",
    "mbert",
    "language",
    "nlp",
    "extraction",
    "fine-tuning and feature extraction",
    "dependency-based information",
    "bert",
    "mdistilbert",
    "information",
    "fine",
    "dependency",
    "word"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 08:19:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}