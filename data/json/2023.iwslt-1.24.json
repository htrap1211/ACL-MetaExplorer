{
  "id": "2023.iwslt-1.24",
  "title": "GMU} Systems for the {IWSLT} 2023 Dialect and Low-resource Speech Translation Tasks",
  "authors": [
    "Mbuya, Jonathan  and\nAnastasopoulos, Antonios"
  ],
  "year": "2023",
  "venue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
  "abstract": "This paper describes the GMU Systems for the IWSLT 2023 Dialect and Low-resource Speech Translation Tasks. We submitted systems for five low-resource tasks and the dialectal task. In this work, we explored self-supervised pre-trained speech models and finetuned them on speech translation downstream tasks. We use the Wav2vec 2.0, XLSR-53, and Hubert as self-supervised models. Unlike Hubert, Wav2vec 2.0 and XLSR-53 achieve the best results when we remove the top three layers. Our results show that Wav2vec 2.0 and Hubert perform similarly with their relative best configuration. In addition, we found that Wav2vec 2.0 pre-trained on audio data of the same language as the source language of a speech translation model achieves better results. For the low-resource setting, the best results are achieved using either the Wav2vec 2.0 or Hubert models, while XLSR-53 achieves the best results for the dialectal transfer task. We find that XLSR-53 does not perform well for low-resource tasks. Using Wav2vec 2.0, we report close to 2 BLEU point improvements on the test set for the Tamasheq-French compared to the baseline system at the IWSLT 2022.",
  "keywords": [
    "work",
    "hubert",
    "language",
    "hubert models",
    "speech translation downstream tasks",
    "model",
    "self",
    "bleu",
    "hubert wav2vec",
    "a speech translation model",
    "we",
    "transfer",
    "pre",
    "translation",
    "speech"
  ],
  "url": "https://aclanthology.org/2023.iwslt-1.24/",
  "provenance": {
    "collected_at": "2025-06-05 10:24:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}