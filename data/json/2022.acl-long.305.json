{
  "id": "2022.acl-long.305",
  "title": "EPT}-{X}: An Expression-Pointer Transformer model that generates e{X}planations for numbers",
  "authors": [
    "Kim, Bugeun  and\nKi, Kyung Seo  and\nRhim, Sangkyu  and\nGweon, Gahgene"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",
  "keywords": [
    "we",
    "neural",
    "natural",
    "ept-x expression-pointer transformer",
    "information",
    "word",
    "pen",
    "strategies",
    "work",
    "transformer",
    "process",
    "language",
    "model",
    "human",
    "the numbers"
  ],
  "url": "https://aclanthology.org/2022.acl-long.305/",
  "provenance": {
    "collected_at": "2025-06-05 08:28:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}