{
  "id": "2021.acl-long.163",
  "title": "Optimizing Deeper Transformers on Small Datasets",
  "authors": [
    "Xu, Peng  and\nKumar, Dhruv  and\nYang, Wei  and\nZi, Wenjie  and\nTang, Keyi  and\nHuang, Chenyang  and\nCheung, Jackie Chi Kit  and\nPrince, Simon J.D.  and\nCao, Yanshuai"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension. In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work. Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.",
  "keywords": [
    "deep",
    "top",
    "transformers",
    "roberta",
    "parsing",
    "pre-trained roberta",
    "semantic",
    "we",
    "a common belief",
    "training",
    "cross",
    "deep transformers",
    "deeper transformers",
    "it",
    "no task-specific pre-training"
  ],
  "url": "https://aclanthology.org/2021.acl-long.163/",
  "provenance": {
    "collected_at": "2025-06-05 08:01:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}