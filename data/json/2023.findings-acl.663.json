{
  "id": "2023.findings-acl.663",
  "title": "How Well Do Large Language Models Perform on Faux Pas Tests?",
  "authors": [
    "Shapira, Natalie  and\nZwirn, Guy  and\nGoldberg, Yoav"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Motivated by the question of the extent to which large language models “understand” social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which is known to be more challenging for children than individual tests of theory-of-mind or social intelligence. Our results demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (“the ELIZA effect”). An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions. Finally, in an experiment in which the models are tasked with generating original faux pas stories, we find that while some models are capable of generating novel faux pas stories, the stories are all explicit, as the models are limited in their abilities to describe situations in an implicit manner.",
  "keywords": [
    "original faux pas stories",
    "question",
    "we",
    "novel faux pas stories",
    "manner",
    "abilities",
    "stories",
    "language",
    "their abilities",
    "human",
    "large language models",
    "the stories",
    "an implicit manner",
    "theory",
    "mind"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.663/",
  "provenance": {
    "collected_at": "2025-06-05 10:17:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}