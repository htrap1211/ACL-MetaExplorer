{
  "id": "2024.findings-acl.848",
  "title": "NUMC}o{T}: Numerals and Units of Measurement in Chain-of-Thought Reasoning using Large Language Models",
  "authors": [
    "Xu, Ancheng  and\nTan, Minghuan  and\nWang, Lei  and\nYang, Min  and\nXu, Ruifeng"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Numeral systems and units of measurement are two conjoined topics in activities of human beings and have mutual effects with the languages expressing them. Currently, the evaluation of Large Language Models (LLMs) often involves mathematical reasoning, yet little attention is given to how minor changes in numbers or units can drastically alter the complexity of problems and the performance of LLMs. In this paper, we scrutinize existing LLMs on processing of numerals and units of measurement by constructing datasets with perturbations. We first anatomize the reasoning of math word problems to different sub-procedures like numeral conversions from language to numbers and measurement conversions based on units. Then we further annotate math word problems from ancient Chinese arithmetic works which are challenging in numerals and units of measurement. Experiments on perturbed datasets demonstrate that LLMs still encounter difficulties in handling numeral and measurement conversions.",
  "keywords": [
    "ancient",
    "evaluation",
    "processing",
    "language",
    "thought",
    "human",
    "chain",
    "large language models",
    "-",
    "little attention",
    "large language models llms",
    "ancient chinese arithmetic works",
    "attention",
    "word",
    "we"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.848/",
  "provenance": {
    "collected_at": "2025-06-05 11:00:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}