{
  "id": "2022.acl-long.421",
  "title": "CLIP} Models are Few-Shot Learners: Empirical Studies on {VQA} and Visual Entailment",
  "authors": [
    "Song, Haoyu  and\nDong, Li  and\nZhang, Weinan  and\nLiu, Ting  and\nWei, Furu"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language. We first evaluate CLIPâ€™s zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task. We achieve competitive zero/few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.",
  "keywords": [
    "abilities",
    "work",
    "cross",
    "tuning",
    "language",
    "competitive zero few-shot results",
    "learners",
    "a remarkable zero-shot capability",
    "studies",
    "a parameter-efficient fine-tuning strategy",
    "efficient",
    "the vqa task",
    "encoder",
    "some few-shot abilities",
    "the few-shot performance"
  ],
  "url": "https://aclanthology.org/2022.acl-long.421/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}