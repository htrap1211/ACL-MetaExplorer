{
  "id": "2024.acl-long.510",
  "title": "Math-Shepherd: Verify and Reinforce {LLM}s Step-by-step without Human Annotations",
  "authors": [
    "Wang, Peiyi  and\nLi, Lei  and\nShao, Zhihong  and\nXu, Runxin  and\nDai, Damai  and\nLi, Yifei  and\nChen, Deli  and\nWu, Yu  and\nSui, Zhifang"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In this paper, we present an innovative process-oriented math process reward model called Math-shepherd, which assigns a reward score to each step of math problem solutions. The training of Math-shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-shepherd in two scenarios: 1)Verification: Math-shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2)Reinforcement Learning (RL): Math-shepherd is employed to reinforce LLMs.With Math-shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, process RL with Math-shepherd significantly enhances Mistral-7B (77.9%→84.1% on GSM8K and 28.6%→33.0% on MATH).The accuracy can be further improved to 89.1% and 43.5% on two benchmarks with verification of Math-shepherd.We believe that automatic process supervision holds significant potential for the future evolution of LLMs.",
  "keywords": [
    "work",
    "the accuracy",
    "process",
    "reinforcement",
    "language",
    "2 reinforcement learning",
    "model",
    "human",
    "large language models",
    "series",
    "open-source llms",
    "we",
    "learning",
    "llm",
    "math-shepherd a series"
  ],
  "url": "https://aclanthology.org/2024.acl-long.510/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}