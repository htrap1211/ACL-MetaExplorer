{
  "id": "2021.acl-short.127",
  "title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders",
  "authors": [
    "Li, Irene  and\nYan, Vanessa  and\nLi, Tianxiao  and\nQu, Rihao  and\nRadev, Dragomir"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "Learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. For example, one may be an expert in the natural language processing (NLP) domain, but want to determine the best order in which to learn new concepts in an unfamiliar Computer Vision domain (CV). Both domains share some common concepts, such as machine learning basics and deep learning models. In this paper, we solve the task of unsupervised cross-domain concept prerequisite chain learning, using an optimized variational graph autoencoder. Our model learns to transfer concept prerequisite relations from an information-rich domain (source domain) to an information-poor domain (target domain), substantially surpassing other baseline models. In addition, we expand an existing dataset by introducing two new domainsâ€”-CV and Bioinformatics (BIO). The annotated data and resources as well as the code will be made publicly available.",
  "keywords": [
    "deep learning models",
    "cross",
    "deep",
    "code",
    "knowledge",
    "processing",
    "language",
    "autoencoders",
    "nlp",
    "natural",
    "machine",
    "model",
    "chain",
    "information",
    "rich"
  ],
  "url": "https://aclanthology.org/2021.acl-short.127/",
  "provenance": {
    "collected_at": "2025-06-05 08:08:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}