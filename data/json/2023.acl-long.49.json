{
  "id": "2023.acl-long.49",
  "title": "On-the-fly Cross-lingual Masking for Multilingual Pre-training",
  "authors": [
    "Ai, Xi  and\nFang, Bin"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward pass. In this work, we present CLPM (Cross-lingual Prototype Masking), a dynamic and token-wise masking scheme, for multilingual pre-training, using a special token[ùíû]xto replace a random tokenxin the input sentence.[ùíû]xis a cross-lingual prototype forxand then forms an explicit cross-lingual forward pass. We instantiate CLPM for the multilingual pre-training phase of UNMT (unsupervised neural machine translation), and experiments show that CLPM can consistently improve the performance of UNMT models on{De, Ro, Ne } ‚Üî En. Beyond UNMT or bilingual tasks, we show that CLPM can consistently improve the performance of multilingual models on cross-lingual classification.",
  "keywords": [
    "work",
    "cross",
    "cross-lingual classification",
    "language",
    "random",
    "neural",
    "machine",
    "objective",
    "the multilingual pre-training phase",
    "the objective",
    "token",
    "modeling",
    "-",
    "we",
    "pre"
  ],
  "url": "https://aclanthology.org/2023.acl-long.49/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}