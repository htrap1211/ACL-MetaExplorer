{
  "id": "2021.iwslt-1.24",
  "title": "On Knowledge Distillation for Translating Erroneous Speech Transcriptions",
  "authors": [
    "Fukuda, Ryo  and\nSudoh, Katsuhito  and\nNakamura, Satoshi"
  ],
  "year": "2021",
  "venue": "Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)",
  "abstract": "Recent studies argue that knowledge distillation is promising for speech translation (ST) using end-to-end models. In this work, we investigate the effect of knowledge distillation with a cascade ST using automatic speech recognition (ASR) and machine translation (MT) models. We distill knowledge from a teacher model based on human transcripts to a student model based on erroneous transcriptions. Our experimental results demonstrated that knowledge distillation is beneficial for a cascade ST. Further investigation that combined knowledge distillation and fine-tuning revealed that the combination consistently improved two language pairs: English-Italian and Spanish-English.",
  "keywords": [
    "work",
    "tuning",
    "knowledge",
    "end",
    "language",
    "studies",
    "machine",
    "model",
    "human",
    "machine translation",
    "recent studies",
    "we",
    "speech translation st",
    "translation",
    "fine-tuning"
  ],
  "url": "https://aclanthology.org/2021.iwslt-1.24/",
  "provenance": {
    "collected_at": "2025-06-05 08:18:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}