{
  "id": "2021.acl-long.240",
  "title": "U}nited{QA}: {A} Hybrid Approach for Open Domain Question Answering",
  "authors": [
    "Cheng, Hao  and\nShen, Yelong  and\nLiu, Xiaodong  and\nHe, Pengcheng  and\nChen, Weizhu  and\nGao, Jianfeng"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively. In this paper, we study a hybrid approach for leveraging the strengths of both models. We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models. We demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively.",
  "keywords": [
    "work",
    "generative",
    "ensembles",
    "the retrieval-reader framework",
    "answer",
    "language",
    "triviaqa",
    "neural",
    "strategies",
    "retrieval",
    "u",
    "question",
    "open-domain qa",
    "we",
    "training"
  ],
  "url": "https://aclanthology.org/2021.acl-long.240/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}