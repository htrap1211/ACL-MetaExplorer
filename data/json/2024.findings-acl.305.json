{
  "id": "2024.findings-acl.305",
  "title": "Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes",
  "authors": [
    "Kweon, Sunjun  and\nKim, Junu  and\nKim, Jiyoun  and\nIm, Sujeong  and\nCho, Eunbyeol  and\nBae, Seongsu  and\nOh, Jungwoo  and\nLee, Gyubok  and\nMoon, Jong Hak  and\nYou, Seng Chan  and\nBaek, Seungjin  and\nHan, Chang Hoon  and\nJung, Yoon Bin  and\nJo, Yohan  and\nChoi, Edward"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "The development of large language models tailored for handling patients’ clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations.To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature.We then use these synthetic notes to train our specialized clinical large language model, Asclepius.While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes.We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructing high-performing clinical language models. This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals. All resources—including weights, codes, and data—used in the development of Asclepius will be made publicly accessible for future research.",
  "keywords": [
    "we",
    "it",
    "patients",
    "both gpt-4",
    "high-performing clinical language models",
    "gpt-4",
    "language",
    "detailed evaluations",
    "model",
    "evaluations",
    "large language models",
    "detailed",
    "approach",
    "alternatives",
    "future research"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.305/",
  "provenance": {
    "collected_at": "2025-06-05 10:52:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}