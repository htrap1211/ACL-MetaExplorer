{
  "id": "2024.arabicnlp-1.77",
  "title": "ASOS} at {KSAA}-{CAD} 2024: One Embedding is All You Need for Your Dictionary",
  "authors": [
    "Sibaee, Serry  and\nAlharbi, Abdullah  and\nAhmad, Samar  and\nNacar, Omer  and\nKoubaa, Anis  and\nGhouti, Lahouari"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "Semantic search tasks have grown extremely fast following the advancements in large language models, including the Reverse Dictionary and Word Sense Disambiguation in Arabic. This paper describes our participation in the Contemporary Arabic Dictionary Shared Task. We propose two models that achieved first place in both tasks. We conducted comprehensive experiments on the latest five multilingual sentence transformers and the Arabic BERT model for semantic embedding extraction. We achieved a ranking score of 0.06 for the reverse dictionary task, which is double than last yearâ€™s winner. We had an accuracy score of 0.268 for the Word Sense Disambiguation task.",
  "keywords": [
    "transformers",
    "language",
    "extraction",
    "bert",
    "model",
    "winner",
    "large language models",
    "semantic",
    "all",
    "word",
    "we",
    "last year s winner",
    "an accuracy score",
    "accuracy",
    "the arabic bert model"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.77/",
  "provenance": {
    "collected_at": "2025-06-05 11:03:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}