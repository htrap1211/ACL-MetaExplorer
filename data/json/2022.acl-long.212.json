{
  "id": "2022.acl-long.212",
  "title": "Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction",
  "authors": [
    "Chen, Hao  and\nZhai, Zepeng  and\nFeng, Fangxiang  and\nLi, Ruifan  and\nWang, Xiaojie"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is an emerging sentiment analysis task. Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion. However, these methods ignore the relations between words for ASTE task. In this paper, we propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully utilize the relations between words. Specifically, we first define ten types of relations for ASTE task, and then adopt a biaffine attention module to embed these relations as an adjacent tensor between words in a sentence. After that, our EMC-GCN transforms the sentence into a multi-channel graph by treating words and the relation adjacent tensor as nodes and edges, respectively. Thus, relation-aware node representations can be learnt. Furthermore, we consider diverse linguistic features to enhance our EMC-GCN model. Finally, we design an effective refining strategy on EMC-GCN for word-pair representation refinement, which considers the implicit results of aspect and opinion extraction when determining whether word pairs match or not. Extensive experimental results on the benchmark datasets demonstrate that the effectiveness and robustness of our proposed model, which outperforms state-of-the-art methods significantly.",
  "keywords": [
    "end",
    "extraction",
    "we",
    "graph",
    "convolutional",
    "word",
    "analysis",
    "the existing studies",
    "sentiment",
    "model",
    "studies",
    "tagging",
    "network",
    "attention",
    "a biaffine attention module"
  ],
  "url": "https://aclanthology.org/2022.acl-long.212/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}