{
  "id": "W19-4825",
  "title": "Open Sesame: Getting inside {BERT}{'}s Linguistic Knowledge",
  "authors": [
    "Lin, Yongjie  and\nTan, Yi Chern  and\nFrank, Robert"
  ],
  "year": "2019",
  "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
  "abstract": "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT’s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT’s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT’s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.",
  "keywords": [
    "self-attention vectors",
    "classifiers",
    "two studies",
    "we",
    "bert s linguistic knowledge",
    "diagnostic classifiers",
    "self",
    "dependencies",
    "information",
    "word",
    "processing",
    "bert s representation",
    "bert",
    "a hierarchically-oriented encoding",
    "work"
  ],
  "url": "https://aclanthology.org/W19-4825/",
  "provenance": {
    "collected_at": "2025-06-05 00:58:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}