{
  "id": "2023.findings-acl.725",
  "title": "H}yper{PELT}: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
  "authors": [
    "Zhang, Zhengkun  and\nGuo, Wenya  and\nMeng, Xiaojun  and\nWang, Yasheng  and\nWang, Yadao  and\nJiang, Xin  and\nLiu, Qun  and\nYang, Zhenglu"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "With the scale and capacity of pretrained models growing rapidly, parameter-efficient language model tuning has emerged as a popular paradigm for solving various NLP and Vision-and-Language (V&L) tasks. In this paper, we design a unified parameter-efficient multitask learning framework that works effectively on both NLP and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings and visual modality as input, and outputs weights for different modules in a pretrained language model, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.). Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework.",
  "keywords": [
    "efficient",
    "we",
    "multi-head attention blocks",
    "parameter",
    "hyper",
    "unified",
    "learning",
    "transfer",
    "visual",
    "yper",
    "feed",
    "tuning",
    "i",
    "a pretrained language model",
    "-"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.725/",
  "provenance": {
    "collected_at": "2025-06-05 10:18:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}