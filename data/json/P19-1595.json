{
  "id": "P19-1595",
  "title": "BAM}! Born-Again Multi-Task Networks for Natural Language Understanding",
  "authors": [
    "Clark, Kevin  and\nLuong, Minh-Thang  and\nKhandelwal, Urvashi  and\nManning, Christopher D.  and\nLe, Quoc V."
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training.",
  "keywords": [
    "knowledge",
    "language",
    "neural",
    "natural",
    "bert",
    "multi-task fine-tuning bert",
    "model",
    "it",
    "we",
    "learning",
    "multi-task neural networks",
    "training",
    "that",
    "a novel method",
    "multi"
  ],
  "url": "https://aclanthology.org/P19-1595/",
  "provenance": {
    "collected_at": "2025-06-05 00:45:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}