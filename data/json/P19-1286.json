{
  "id": "P19-1286",
  "title": "Domain Adaptation of Neural Machine Translation by Lexicon Induction",
  "authors": [
    "Hu, Junjie  and\nXia, Mengzhou  and\nNeubig, Graham  and\nCarbonell, Jaime"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",
  "keywords": [
    "neural",
    "model",
    "machine",
    "it",
    "neural machine translation",
    "bleu",
    "up to 2 bleu",
    "strong back-translation baselines",
    "fine",
    "word",
    "we",
    "pre",
    "neural machine translation nmt",
    "14 bleu",
    "translation"
  ],
  "url": "https://aclanthology.org/P19-1286/",
  "provenance": {
    "collected_at": "2025-06-05 00:37:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}