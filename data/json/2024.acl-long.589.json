{
  "id": "2024.acl-long.589",
  "title": "Make-A-Voice: Revisiting Voice Large Language Models as Scalable Multilingual and Multitask Learners",
  "authors": [
    "Huang, Rongjie  and\nZhang, Chunlei  and\nWang, Yongqi  and\nYang, Dongchao  and\nTian, Jinchuan  and\nYe, Zhenhui  and\nLiu, Luping  and\nWang, Zehan  and\nJiang, Ziyue  and\nChang, Xuankai  and\nShi, Jiatong  and\nWeng, Chao  and\nZhao, Zhou  and\nYu, Dong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Large language models (LLMs) have successfully served as a general-purpose interface across multiple tasks and languages, while the adaptation of voice LLMs is mostly designed for specific purposes (either single-task or monolingual), where the advantages of LLMs especially for low-resource language processing and zero-shot task generalization are less exploited in the audio community. To bridge the gap, we introduce Make-A-Voice as a multi-modal voice LLM and conduct a comprehensive study on its capability to deal with multiple tasks/languages. When trained on ~200K hours of 6-language data for 4 voice generation applications, Make-A-Voice emerges notable advantages: 1) as scalable learners to improve performance with end-to-end local and global multiscale transformers; and 2) as multitask learners by adjusting prompts to share common knowledge across modalities (speech/singing) and present in-context learning abilities by generalizing to unseen tasks not explicitly train on; 3) as multilingual learners to alleviate data scarcity of low-resource languages by including rich-resource language training data. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models in monolingual/cross-lingual voice generation. Audio samples are available at https://M-Voice.github.io",
  "keywords": [
    "transformers",
    "end",
    "scalable learners",
    "multitask learners",
    "we",
    "llm",
    "shot",
    "training",
    "cross",
    "a general-purpose interface",
    "multilingual learners",
    "rich",
    "llms",
    "abilities",
    "processing"
  ],
  "url": "https://aclanthology.org/2024.acl-long.589/",
  "provenance": {
    "collected_at": "2025-06-05 10:42:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}