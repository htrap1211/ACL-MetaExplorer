{
  "id": "2022.acl-long.266",
  "title": "Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction",
  "authors": [
    "Tarnavskyi, Maksym  and\nChernodub, Artem  and\nOmelianchuk, Kostiantyn"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In this paper, we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations. We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size. Our best ensemble achieves a new SOTA result with anF0.5score of 76.05 on BEA-2019 (test), even without pre-training on synthetic datasets. In addition, we perform knowledge distillation with a trained ensemble to generate new synthetic training datasets, “Troy-Blogs” and “Troy-1BW”. Our best single sequence tagging model that is pretrained on the generated Troy- datasets in combination with the publicly available synthetic PIE dataset achieves a near-SOTA result with anF0.5score of 73.21 on BEA-2019 (test). The code, datasets, and trained models are publicly available.",
  "keywords": [
    "code",
    "we",
    "pie",
    "training",
    "ensemble",
    "edge",
    "recent cutting-edge transformer-based encoders",
    "sequence",
    "our best ensemble",
    "encoders",
    "transformer",
    "the generated troy- datasets",
    "knowledge",
    "model",
    "a trained ensemble"
  ],
  "url": "https://aclanthology.org/2022.acl-long.266/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}