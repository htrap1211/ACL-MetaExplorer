{
  "id": "2020.acl-main.336",
  "title": "Hypernymy Detection for Low-Resource Languages via Meta Learning",
  "authors": [
    "Yu, Changlong  and\nHan, Jialong  and\nZhang, Haisong  and\nNg, Wilfred"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.",
  "keywords": [
    "language",
    "natural",
    "fitting",
    "-",
    "we",
    "learning",
    "time",
    "training",
    "three joint training paradigms",
    "low-resource hypernymy detection",
    "a fundamental sub",
    "task",
    "the three settings",
    "fundamental",
    "understanding"
  ],
  "url": "https://aclanthology.org/2020.acl-main.336/",
  "provenance": {
    "collected_at": "2025-06-05 07:46:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}