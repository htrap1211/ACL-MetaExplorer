{
  "id": "2024.acl-long.674",
  "title": "RAID}: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors",
  "authors": [
    "Dugan, Liam  and\nHwang, Alyssa  and\nTrhl{\\'i}k, Filip  and\nZhu, Andrew  and\nLudan, Josh Magnus  and\nXu, Hainiu  and\nIppolito, Daphne  and\nCallison-Burch, Chris"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99% or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challengingâ€”lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data along with a leaderboard to encourage future research.",
  "keywords": [
    "open-source generative models",
    "work",
    "generative",
    "penalties",
    "machine-generated text detectors",
    "robust evaluation",
    "machine",
    "text",
    "machine-generated text",
    "strategies",
    "machine-generated text detection raid",
    "sampling strategies repetition penalties",
    "generations",
    "extremely high accuracy",
    "we"
  ],
  "url": "https://aclanthology.org/2024.acl-long.674/",
  "provenance": {
    "collected_at": "2025-06-05 10:43:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}