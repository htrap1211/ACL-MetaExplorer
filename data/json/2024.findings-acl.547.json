{
  "id": "2024.findings-acl.547",
  "title": "LCS}: A Language Converter Strategy for Zero-Shot Neural Machine Translation",
  "authors": [
    "Sun, Zengkui  and\nLiu, Yijin  and\nMeng, Fandong  and\nXu, Jinan  and\nChen, Yufeng  and\nZhou, Jie"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Multilingual neural machine translation models generally distinguish translation directions by the language tag (LT) in front of the source or target sentences. However, current LT strategies cannot indicate the desired target language as expected on zero-shot translation, i.e., the off-target issue. Our analysis reveals that the indication of the target language is sensitive to the placement of the target LT. For example, when placing the target LT on the decoder side, the indication would rapidly degrade along with decoding steps, while placing the target LT on the encoder side would lead to copying or paraphrasing the source input. To address the above issues, we propose a simple yet effective strategy named Language Converter Strategy (LCS). By introducing the target language embedding into the top encoder layers, LCS mitigates confusion in the encoder and ensures stable language indication for the decoder. Experimental results on MultiUN, TED, and OPUS-100 datasets demonstrate that LCS could significantly mitigate the off-target issue, with language accuracy up to 95.28%, 96.21%, and 85.35% meanwhile outperforming the vanilla LT strategy by 3.07, 3,3, and 7.93 BLEU scores on zero-shot translation, respectively.",
  "keywords": [
    "bleu",
    "we",
    "confusion",
    "current",
    "shot",
    "translation",
    "the encoder",
    "neural",
    "language accuracy",
    "decoder",
    "tag",
    "zero-shot translation",
    "analysis",
    "i",
    "strategies"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.547/",
  "provenance": {
    "collected_at": "2025-06-05 10:56:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}