{
  "id": "2024.acl-long.704",
  "title": "C}ritique{LLM}: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
  "authors": [
    "Ke, Pei  and\nWen, Bosi  and\nFeng, Andrew  and\nLiu, Xiao  and\nLei, Xuanyu  and\nCheng, Jiale  and\nWang, Shengyuan  and\nZeng, Aohan  and\nDong, Yuxiao  and\nWang, Hongning  and\nTang, Jie  and\nHuang, Minlie"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4â€™s direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.",
  "keywords": [
    "feedback",
    "large language model generation",
    "we",
    "c ritique llm",
    "unsatisfactory evaluation performance",
    "llm",
    "fine-tuning",
    "our generated critiques",
    "informative evaluation data",
    "a critique generation model",
    "natural",
    "generated texts",
    "their generated critiques",
    "chatgpt",
    "llms"
  ],
  "url": "https://aclanthology.org/2024.acl-long.704/",
  "provenance": {
    "collected_at": "2025-06-05 10:44:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}