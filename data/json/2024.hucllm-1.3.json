{
  "id": "2024.hucllm-1.3",
  "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
  "authors": [
    "Niu, Tong  and\nXiong, Caiming  and\nZhou, Yingbo  and\nYavuz, Semih"
  ],
  "year": "2024",
  "venue": "Proceedings of the 1st Human-Centered Large Language Modeling Workshop",
  "abstract": "The field of natural language generation has witnessed significant advancements in recent years, including the development of controllable text generation techniques. However, controlling the attributes of the generated text remains a challenge, especially when aiming to avoid undesirable behavior such as toxicity. In this work, we introduce Detoxification Generator (DETOXIGEN), an inference-time algorithm that steers the generation away from unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model (generator) and a detoxifier. The detoxifier is trained intentionally on the toxic data representative of the undesirable attribute, encouraging it to generate text in that style exclusively. During the actual generation, we use the trained detoxifier to produce undesirable tokens for the generator to contrast against at each decoding step. This approach directly informs the generator to avoid generating tokens that the detoxifier considers highly likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators. We find that it significantly outperforms previous approaches in detoxification metrics while not compromising on the generation quality. Moreover, the detoxifier is obtained by soft prompt-tuning using the same backbone language model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra weights from the virtual tokens of the detoxifier to be loaded into GPU memory while decoding, making it a promising lightweight, practical, and parameter-efficient detoxification strategy.",
  "keywords": [
    "the actual generation",
    "extra",
    "efficient",
    "field",
    "the generation",
    "detoxifier",
    "we",
    "generators",
    "parameter",
    "ensemble",
    "the generator",
    "the generated text",
    "the field",
    "natural",
    "it"
  ],
  "url": "https://aclanthology.org/2024.hucllm-1.3/",
  "provenance": {
    "collected_at": "2025-06-05 11:06:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}