{
  "id": "2020.acl-demos.30",
  "title": "DIALOGPT} : Large-Scale Generative Pre-training for Conversational Response Generation",
  "authors": [
    "Zhang, Yizhe  and\nSun, Siqi  and\nGalley, Michel  and\nChen, Yen-Chun  and\nBrockett, Chris  and\nGao, Xiang  and\nGao, Jianfeng  and\nLiu, Jingjing  and\nDolan, Bill"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
  "abstract": "We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",
  "keywords": [
    "generative",
    "transformer",
    "automatic and human evaluation",
    "single-turn dialogue settings",
    "generation",
    "147m conversation-like exchanges",
    "neural",
    "model",
    "human",
    "neural response generation",
    "conversational",
    "conversation",
    "we",
    "dialogue",
    "2017 dialogpt"
  ],
  "url": "https://aclanthology.org/2020.acl-demos.30/",
  "provenance": {
    "collected_at": "2025-06-05 07:53:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}