{
  "id": "2023.bionlp-1.54",
  "title": "K}now{L}ab at {R}ad{S}um23: comparing pre-trained language models in radiology report summarization",
  "authors": [
    "Wu, Jinge  and\nShi, Daqian  and\nHasan, Abul  and\nWu, Honghan"
  ],
  "year": "2023",
  "venue": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
  "abstract": "This paper presents our contribution to the RadSum23 shared task organized as part of the BioNLP 2023. We compared state-of-the-art generative language models in generating high-quality summaries from radiology reports. A two-stage fine-tuning approach was introduced for utilizing knowledge learnt from different datasets. We evaluated the performance of our method using a variety of metrics, including BLEU, ROUGE, bertscore, CheXbert, and RadGraph. Our results revealed the potentials of different models in summarizing radiology reports and demonstrated the effectiveness of the two-stage fine-tuning approach. We also discussed the limitations and future directions of our work, highlighting the need for better understanding the architecture designâ€™s effect and optimal way of fine-tuning accordingly in automatic clinical summarizations.",
  "keywords": [
    "variety",
    "bleu",
    "summarization",
    "chexbert",
    "we",
    "summarizations",
    "bertscore",
    "bleu rouge bertscore chexbert",
    "pre-trained language models",
    "a variety",
    "generative",
    "tuning",
    "the bionlp",
    "a two-stage fine-tuning approach",
    "metrics"
  ],
  "url": "https://aclanthology.org/2023.bionlp-1.54/",
  "provenance": {
    "collected_at": "2025-06-05 10:22:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}