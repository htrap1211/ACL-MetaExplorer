{
  "id": "2021.findings-acl.188",
  "title": "M}ini{LM}v2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers",
  "authors": [
    "Wang, Wenhui  and\nBao, Hangbo  and\nHuang, Shaohan  and\nDong, Li  and\nWei, Furu"
  ],
  "year": "2021",
  "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
  "abstract": "Correct abstract if needed. Retain XML formatting tags such as <tex-math>.",
  "keywords": [
    "tex",
    "pretrained transformers",
    "lm",
    "transformers",
    "ini",
    "self",
    "attention",
    "head",
    "multi",
    "xml",
    "tex-math",
    "relation",
    "tags",
    "math",
    "formatting tags"
  ],
  "url": "https://aclanthology.org/2021.findings-acl.188/",
  "provenance": {
    "collected_at": "2025-06-05 08:12:38",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}