{
  "id": "2024.acl-long.673",
  "title": "V}oice{C}raft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
  "authors": [
    "Peng, Puyuan  and\nHuang, Po-Yao  and\nLi, Shang-Wen  and\nMohamed, Abdelrahman  and\nHarwath, David"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALL-E and the popular commercial model XTTS v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named . We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web. Data, code, and model weights are available at https://github.com/jasonppy/VoiceCraft",
  "keywords": [
    "code",
    "background noise",
    "we",
    "zero-shot tts",
    "shot",
    "neural",
    "token",
    "decoder",
    "sequence",
    "background",
    "text",
    "speech editing evaluation",
    "transformer",
    "generation",
    "language"
  ],
  "url": "https://aclanthology.org/2024.acl-long.673/",
  "provenance": {
    "collected_at": "2025-06-05 10:43:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}