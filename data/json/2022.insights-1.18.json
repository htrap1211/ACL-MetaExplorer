{
  "id": "2022.insights-1.18",
  "title": "Challenges in including extra-linguistic context in pre-trained language models",
  "authors": [
    "Sorodoc, Ionut  and\nAina, Laura  and\nBoleda, Gemma"
  ],
  "year": "2022",
  "venue": "Proceedings of the Third Workshop on Insights from Negative Results in NLP",
  "abstract": "To successfully account for language, computational models need to take into account both the linguistic context (the content of the utterances) and the extra-linguistic context (for instance, the participants in a dialogue). We focus on a referential task that asks models to link entity mentions in a TV show to the corresponding characters, and design an architecture that attempts to account for both kinds of context. In particular, our architecture combines a previously proposed specialized module (an “entity library”) for character representation with transfer learning from a pre-trained language model. We find that, although the model does improve linguistic contextualization, it fails to successfully integrate extra-linguistic information about the participants in the dialogue. Our work shows that it is very challenging to incorporate extra-linguistic information into pre-trained language models.",
  "keywords": [
    "work",
    "language",
    "pre-trained language models",
    "model",
    "extra",
    "it",
    "a dialogue",
    "information",
    "a pre-trained language model",
    "we",
    "transfer",
    "dialogue",
    "pre",
    "entity",
    "the dialogue"
  ],
  "url": "https://aclanthology.org/2022.insights-1.18/",
  "provenance": {
    "collected_at": "2025-06-05 08:43:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}