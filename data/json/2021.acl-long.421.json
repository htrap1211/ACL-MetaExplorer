{
  "id": "2021.acl-long.421",
  "title": "Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation",
  "authors": [
    "Zhang, Bo  and\nZhang, Xiaoming  and\nLiu, Yun  and\nCheng, Lei  and\nLi, Zhoujun"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments. We propose a generic framework namedCross-domain Knowledge Distillation(CdKD) without needing any source data. CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain. As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance. Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting.",
  "keywords": [
    "we",
    "the gradient information",
    "classification",
    "cross",
    "information",
    "transfer",
    "cross-domain text classification",
    "text",
    "generic",
    "knowledge",
    "model",
    "gradient",
    "network",
    "time",
    "the transfer performance experiments"
  ],
  "url": "https://aclanthology.org/2021.acl-long.421/",
  "provenance": {
    "collected_at": "2025-06-05 08:05:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}