{
  "id": "2023.nlp4convai-1.6",
  "title": "c{TBLS}: Augmenting Large Language Models with Conversational Tables",
  "authors": [
    "Sundar, Anirudh S.  and\nHeck, Larry"
  ],
  "year": "2023",
  "venue": "Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023)",
  "abstract": "Optimizing accuracy and performance while eliminating hallucinations of open-domain conversational large language models (LLMs) is an open research challenge. A particularly promising direction is to augment and ground LLMs with information from structured sources. This paper introduces Conversational Tables cTBLS, a three-step architecture to retrieve and generate dialogue responses grounded on retrieved tabular information. cTBLS uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 125% relative improvement over the retriever in the previous state-of-the-art system on the HyrbiDialogue dataset. cTBLS then uses a shared process between encoder and decoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking combined with a GPT-3.5 LLM response generator to yield a 2x relative improvement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the time (coherency, fluency) and judge informativeness to be 4x better than the previous state-of-the-art.",
  "keywords": [
    "hyrbidialogue",
    "conversational tables",
    "dialogue",
    "rouge scores",
    "dialogue responses",
    "llm",
    "the hyrbidialogue dataset",
    "dense table retrieval",
    "encoder and decoder models",
    "gpt-3",
    "decoder",
    "retrieval",
    "information",
    "llms",
    "the retriever"
  ],
  "url": "https://aclanthology.org/2023.nlp4convai-1.6/",
  "provenance": {
    "collected_at": "2025-06-05 10:25:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}