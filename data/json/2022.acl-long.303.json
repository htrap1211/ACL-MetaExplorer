{
  "id": "2022.acl-long.303",
  "title": "F}i{NER}: Financial Numeric Entity Recognition for {XBRL} Tagging",
  "authors": [
    "Loukas, Lefteris  and\nFergadiotis, Manos  and\nChalkidis, Ilias  and\nSpyropoulou, Eirini  and\nMalakasiotis, Prodromos  and\nAndroutsopoulos, Ion  and\nPaliouras, Georgios"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139, a dataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction datasets, FiNER-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms BERT’s performance, allowing word-level BILSTMs to perform better. To improve BERT’s performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes. We also experiment with FIN-BERT, an existing BERT model for the financial domain, and release our own BERT (SEC-BERT), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on XBRL tagging.",
  "keywords": [
    "an existing bert model",
    "extraction",
    "f i ner",
    "publicly traded companies",
    "we",
    "fin-bert",
    "token",
    "our own bert sec-bert",
    "tag",
    "word",
    "word-level bilstms",
    "analysis",
    "bert s performance",
    "ner",
    "fin"
  ],
  "url": "https://aclanthology.org/2022.acl-long.303/",
  "provenance": {
    "collected_at": "2025-06-05 08:28:23",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}