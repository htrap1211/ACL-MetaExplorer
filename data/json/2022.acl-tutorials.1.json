{
  "id": "2022.acl-tutorials.1",
  "title": "A Gentle Introduction to Deep Nets and Opportunities for the Future",
  "authors": [
    "Church, Kenneth  and\nKordoni, Valia  and\nMarcus, Gary  and\nDavis, Ernest  and\nMa, Yanjun  and\nChen, Zeyu"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts",
  "abstract": "The first half of this tutorial will make deep nets more accessible to a broader audience, following “Deep Nets for Poets” and “A Gentle Introduction to Fine-Tuning.” We will also introduce GFT (general fine tuning), a little language for fine tuning deep nets with short (one line) programs that are as easy to code as regression in statistics packages such as R using glm (general linear models). Based on the success of these methods on a number of benchmarks, one might come away with the impression that deep nets are all we need. However, we believe the glass is half-full: while there is much that can be done with deep nets, there is always more to do. The second half of this tutorial will discuss some of these opportunities.",
  "keywords": [
    "deep",
    "tuning",
    "general",
    "language",
    "opportunities",
    "audience",
    "these opportunities",
    "r",
    "a broader audience",
    "fine",
    "all",
    "we",
    "glm general linear models",
    "fine-tuning",
    "that"
  ],
  "url": "https://aclanthology.org/2022.acl-tutorials.1/",
  "provenance": {
    "collected_at": "2025-06-05 08:34:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}