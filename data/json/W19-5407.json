{
  "id": "W19-5407",
  "title": "QE} {BERT}: Bilingual {BERT} Using Multi-task Learning for Neural Quality Estimation",
  "authors": [
    "Kim, Hyun  and\nLim, Joon-Ho  and\nKim, Hyun-Ki  and\nNa, Seung-Hoon"
  ],
  "year": "2019",
  "venue": "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
  "abstract": "For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses multi-task learning for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.",
  "keywords": [
    "qe bert bilingual bert",
    "processing",
    "i",
    "language",
    "translation",
    "neural",
    "natural",
    "the translation quality estimation",
    "bert",
    "model",
    "word",
    "learning",
    "purposed bert",
    "translation quality estimation",
    "that"
  ],
  "url": "https://aclanthology.org/W19-5407/",
  "provenance": {
    "collected_at": "2025-06-05 02:08:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}