{
  "id": "2024.cmcl-1.8",
  "title": "Large language models fail to derive atypicality inferences in a human-like manner",
  "authors": [
    "Kurch, Charlotte  and\nRyzhova, Margarita  and\nDemberg, Vera"
  ],
  "year": "2024",
  "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
  "abstract": "Recent studies have claimed that large language models (LLMs) are capable of drawing pragmatic inferences (Qiu et al., 2023; Hu et al., 2022; Barattieri di San Pietro et al., 2023). The present paper sets out to test LLM’s abilities on atypicality inferences, a type of pragmatic inference that is triggered through informational redundancy. We test several state-of-the-art LLMs in a zero-shot setting and find that LLMs fail to systematically fail to derive atypicality inferences. Our robustness analysis indicates that when inferences are seemingly derived in a few-shot settings, these results can be attributed to shallow pattern matching and not pragmatic inferencing. We also analyse the performance of the LLMs at the different derivation steps required for drawing atypicality inferences – our results show that models have access to script knowledge and can use it to identify redundancies and accommodate the atypicality inference. The failure instead seems to stem from not reacting to the subtle maxim of quantity violations introduced by the informationally redundant utterances.",
  "keywords": [
    "a few-shot settings",
    "the llms",
    "we",
    "llm",
    "shot",
    "llm s abilities",
    "it",
    "redundancies",
    "analysis",
    "manner",
    "llms",
    "abilities",
    "barattieri",
    "large language models llms",
    "knowledge"
  ],
  "url": "https://aclanthology.org/2024.cmcl-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 11:05:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}