{
  "id": "2022.findings-acl.49",
  "title": "Domain Generalisation of {NMT}: Fusing Adapters with Leave-One-Domain-Out Training",
  "authors": [
    "Vu, Thuy-Trang  and\nKhadivi, Shahram  and\nPhung, Dinh  and\nHaffari, Gholamreza"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Generalising to unseen domains is under-explored and remains a challenge in neural machine translation. Inspired by recent research in parameter-efficient transfer learning from pretrained models, this paper proposes a fusion-based generalisation method that learns to combine domain-specific parameters. We propose a leave-one-domain-out training strategy to avoid information leaking to address the challenge of not knowing the test domain during training time. Empirical results on three language pairs show that our proposed fusion method outperforms other baselines up to +0.8 BLEU score on average.",
  "keywords": [
    "domain generalisation",
    "language",
    "neural",
    "leave-one-domain-out training generalising",
    "machine",
    "a fusion-based generalisation method",
    "efficient",
    "generalisation",
    "neural machine translation",
    "bleu",
    "generalising",
    "information",
    "we",
    "transfer",
    "fusion"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.49/",
  "provenance": {
    "collected_at": "2025-06-05 08:35:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}