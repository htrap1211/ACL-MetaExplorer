{
  "id": "2024.findings-acl.484",
  "title": "Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks",
  "authors": [
    "Mishra, Aditi  and\nRahman, Sajjadur  and\nMitra, Kushan  and\nKim, Hannah  and\nHruschka, Estevam"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. However, their ability to generate rationales for knowledge-intensive tasks (KITs) remains under-explored. Generating rationales for KIT solutions, such as commonsense multiple-choice QA, requires external knowledge to support predictions and refute alternate options. In this work, we consider the task of generating retrieval-augmented rationalization of KIT model predictions via external knowledge guidance within a few-shot setting. Surprisingly, crowd-workers preferred LLM-generated rationales over existing crowd-sourced rationales, generated in a similar knowledge-guided setting, on aspects such as factuality, sufficiency, and convincingness. However, fine-grained evaluation of such rationales highlights the need for further improvements in conciseness, novelty, and domain invariance. Additionally, through an expert-sourced study evaluating the reliability of the rationales, we demonstrate that humansâ€™ trust in LLM-generated rationales erodes when communicated faithfully, i.e., without taking model prediction accuracy into account. We find that even instrumenting simple guardrails can be effective for reliable rationalization.",
  "keywords": [
    "however fine-grained evaluation",
    "sufficiency",
    "we",
    "shot",
    "under-explored generating rationales",
    "llm-generated rationales erodes",
    "retrieval",
    "a few-shot setting",
    "llms",
    "i",
    "factuality sufficiency",
    "text",
    "large language models llms",
    "retrieval-augmented rationalization",
    "invariance"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.484/",
  "provenance": {
    "collected_at": "2025-06-05 10:55:13",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}