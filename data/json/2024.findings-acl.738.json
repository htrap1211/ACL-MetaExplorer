{
  "id": "2024.findings-acl.738",
  "title": "MM}-{LLM}s: Recent Advances in {M}ulti{M}odal Large Language Models",
  "authors": [
    "Zhang, Duzhen  and\nYu, Yahan  and\nDong, Jiahua  and\nLi, Chenxing  and\nSu, Dan  and\nChu, Chenhui  and\nYu, Dong"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a [real-time tracking website](https://mm-llms.github.io/) for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.",
  "keywords": [
    "field",
    "the mm-llms domain",
    "we",
    "selected mm-llms",
    "llm",
    "training",
    "the field",
    "mm-llms",
    "m",
    "126 mm-llms",
    "llms",
    "cost-effective training strategies",
    "strategies",
    "general design formulations",
    "general"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.738/",
  "provenance": {
    "collected_at": "2025-06-05 10:58:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}