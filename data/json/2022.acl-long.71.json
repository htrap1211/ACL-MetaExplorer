{
  "id": "2022.acl-long.71",
  "title": "Multi-Granularity Structural Knowledge Distillation for Language Model Compression",
  "authors": [
    "Liu, Chang  and\nTao, Chongyang  and\nFeng, Jiazhan  and\nZhao, Dongyan"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.",
  "keywords": [
    "the rich semantics",
    "granularities",
    "semantic",
    "we",
    "semantics",
    "token",
    "dependencies",
    "rich",
    "language model compression",
    "text",
    "their simple dependencies",
    "knowledge",
    "language",
    "model",
    "multi"
  ],
  "url": "https://aclanthology.org/2022.acl-long.71/",
  "provenance": {
    "collected_at": "2025-06-05 08:25:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}