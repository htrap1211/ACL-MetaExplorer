{
  "id": "2024.acl-long.165",
  "title": "A}uto{A}ct: Automatic Agent Learning from Scratch for {QA} via Self-Planning",
  "authors": [
    "Qiao, Shuofei  and\nZhang, Ningyu  and\nFang, Runnan  and\nLuo, Yujie  and\nZhou, Wangchunshu  and\nJiang, Yuchen  and\nLv, Chengfei  and\nChen, Huajun"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Language agents have achieved considerable performance on various complex question-answering tasks by planning with external tools. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework for QA that does not rely on large-scale annotated data and synthetic planning trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutoAct generally outperforming that of others.",
  "keywords": [
    "end",
    "field",
    "question",
    "we",
    "planning trajectories",
    "synthetic planning trajectories",
    "different llms",
    "self",
    "information",
    "learning",
    "yields",
    "analysis",
    "llms",
    "gpt-4",
    "that autoact yields"
  ],
  "url": "https://aclanthology.org/2024.acl-long.165/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}