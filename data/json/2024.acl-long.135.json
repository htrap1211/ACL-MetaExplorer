{
  "id": "2024.acl-long.135",
  "title": "Making Long-Context Language Models Better Multi-Hop Reasoners",
  "authors": [
    "Li, Yanyang  and\nLiang, Shuo  and\nLyu, Michael  and\nWang, Liwei"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts. In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning. We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience. Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy. Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant.",
  "keywords": [
    "tuning",
    "multiple nlp applications",
    "language",
    "nlp",
    "model",
    "reasoners",
    "modeling",
    "long-context language models",
    "capabilities",
    "chatgpt",
    "resilience",
    "reasoning capabilities",
    "we",
    "language models lms",
    "proprietary lms"
  ],
  "url": "https://aclanthology.org/2024.acl-long.135/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}