{
  "id": "2024.bionlp-1.16",
  "title": "Domain-specific or Uncertainty-aware models: Does it really make a difference for biomedical text classification?",
  "authors": [
    "Sinha, Aman  and\nMickus, Timothee  and\nClausel, Marianne  and\nConstant, Mathieu  and\nCoubez, Xavier"
  ],
  "year": "2024",
  "venue": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
  "abstract": "The success of pretrained language models (PLMs) across a spate of use-cases has led to significant investment from the NLP community towards building domain-specific foundational models. On the other hand, in mission critical settings such as biomedical applications, other aspects also factor in—chief of which is a model’s ability to produce reasonable estimates of its own uncertainty. In the present study, we discuss these two desiderata through the lens of how they shape the entropy of a model’s output probability distribution. We find that domain specificity and uncertainty awareness can often be successfully combined, but the exact task at hand weighs in much more strongly.",
  "keywords": [
    "language",
    "nlp",
    "model",
    "text",
    "it",
    "the nlp community",
    "chief",
    "we",
    "pretrained language models plms",
    "classification",
    "biomedical text classification",
    "entropy",
    "biomedical applications",
    "applications",
    "specificity"
  ],
  "url": "https://aclanthology.org/2024.bionlp-1.16/",
  "provenance": {
    "collected_at": "2025-06-05 11:04:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}