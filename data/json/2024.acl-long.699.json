{
  "id": "2024.acl-long.699",
  "title": "XFT}: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts",
  "authors": [
    "Ding, Yifeng  and\nLiu, Jiawei  and\nWei, Yuxiang  and\nZhang, Lingming"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at https://github.com/ise-uiuc/xft.",
  "keywords": [
    "code",
    "we",
    "generalizability",
    "code instruction tuning",
    "llm",
    "training",
    "instruction",
    "oss",
    "e",
    "instruction tuning",
    "tuning",
    "normalization",
    "-",
    "code instruction tuning codes",
    "its generalizability xft"
  ],
  "url": "https://aclanthology.org/2024.acl-long.699/",
  "provenance": {
    "collected_at": "2025-06-05 10:43:56",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}