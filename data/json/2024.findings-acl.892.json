{
  "id": "2024.findings-acl.892",
  "title": "Set the Clock: Temporal Alignment of Pretrained Language Models",
  "authors": [
    "Zhao, Bowen  and\nBrumbaugh, Zander  and\nWang, Yizhong  and\nHajishirzi, Hannaneh  and\nSmith, Noah"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call “temporal alignment.” To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments demonstrate that aligning LLaMa2 to the year 2022 can enhance its performance by up to 62% according to that year’s answers. This improvement occurs even without explicitly mentioning time information, indicating the possibility of aligning models’ internal sense of time after pretraining. Finally, we find that alignment to a historical time is also possible, with up to2.8×the performance of the unaligned LM in 2010 if finetuning models to that year. These findings hint at the sophistication of LMs’ internal knowledge organization and the necessity of tuning them properly.",
  "keywords": [
    "earlier knowledge e g",
    "this alignment",
    "that alignment",
    "we",
    "information",
    "the clock temporal alignment",
    "text",
    "g",
    "work",
    "alignment",
    "general",
    "earlier",
    "knowledge",
    "language",
    "time"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.892/",
  "provenance": {
    "collected_at": "2025-06-05 11:00:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}