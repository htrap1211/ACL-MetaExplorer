{
  "id": "2024.acl-long.25",
  "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences",
  "authors": [
    "Wang, Xiyao  and\nZhou, Yuhang  and\nLiu, Xiaoyu  and\nLu, Hongjin  and\nXu, Yuancheng  and\nHe, Feihong  and\nYoon, Jaehong  and\nLu, Taixi  and\nLiu, Fuxiao  and\nBertasius, Gedas  and\nBansal, Mohit  and\nYao, Huaxiu  and\nHuang, Furong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs’ sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs’ sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of co-occurring behaviors, and the compounding impact of behavioral hallucinations.",
  "keywords": [
    "a gpt-4 assisted method",
    "variety",
    "gpt-4v",
    "we",
    "current",
    "mllm reasoning performance",
    "large language models mllms",
    "information",
    "a variety",
    "visual",
    "mllm",
    "analysis",
    "abilities",
    "object",
    "gpt-4"
  ],
  "url": "https://aclanthology.org/2024.acl-long.25/",
  "provenance": {
    "collected_at": "2025-06-05 10:34:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}