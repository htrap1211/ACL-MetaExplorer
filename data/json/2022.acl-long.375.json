{
  "id": "2022.acl-long.375",
  "title": "$\\infty$-former: Infinite Memory Transformer",
  "authors": [
    "Martins, Pedro Henrique  and\nMarinho, Zita  and\nMartins, Andre"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the∞-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the∞-former’s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important,∞-former maintains “sticky memories,” being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the∞-former’s ability to retain information from long sequences.",
  "keywords": [
    "a continuous-space attention mechanism",
    "transformer",
    "transformers",
    "document grounded dialogue generation",
    "precision",
    "language",
    "generation",
    "sticky memories",
    "memories",
    "the vanilla transformer",
    "efficient",
    "modeling",
    "information",
    "long-term memories",
    "all"
  ],
  "url": "https://aclanthology.org/2022.acl-long.375/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}