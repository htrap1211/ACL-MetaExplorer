{
  "id": "2023.acl-long.733",
  "title": "CAT}: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning",
  "authors": [
    "Wang, Weiqi  and\nFang, Tianqing  and\nXu, Baixuan  and\nBo, Chun Yi Louis  and\nSong, Yangqiu  and\nChen, Lei"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about “meditation,” while is knowledgeable about “singing,” he can still infer that “meditation makes people relaxed” from the existing knowledge that “singing makes people relaxed” by first conceptualizing “singing” as a “relaxing event” and then instantiating that event to “meditation.”This process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose CAT (Contextualized ConceptuAlization and InsTantiation),a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptualization tasks, and the acquired abstract commonsense knowledge can significantly improve commonsense inference modeling. Our code, data, and fine-tuned models are publicly available at [https://github.com/HKUST-KnowComp/CAT](https://github.com/HKUST-KnowComp/CAT).",
  "keywords": [
    "code",
    "we",
    "learning",
    "cat",
    "he",
    "process",
    "knowledge",
    "methodologies",
    "human",
    "modeling",
    "performances",
    "meditation",
    "two conceptualization tasks",
    "state",
    "a contextualized conceptualization"
  ],
  "url": "https://aclanthology.org/2023.acl-long.733/",
  "provenance": {
    "collected_at": "2025-06-05 09:45:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}