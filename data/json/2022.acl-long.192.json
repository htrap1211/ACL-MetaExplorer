{
  "id": "2022.acl-long.192",
  "title": "Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource {NER",
  "authors": [
    "Lee, Dong-Ho  and\nKadakia, Akshen  and\nTan, Kangmin  and\nAgarwal, Mahak  and\nFeng, Xinyu  and\nShibuya, Takashi  and\nMitani, Ryosuke  and\nSekiya, Toshiyuki  and\nPujara, Jay  and\nRen, Xiang"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates. Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence. Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning. We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use. Results on in-domain learning and domain adaptation show that the modelâ€™s performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances). We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.",
  "keywords": [
    "we",
    "prompt-based learning",
    "shot",
    "classification",
    "token",
    "dependencies",
    "e",
    "learning",
    "ner",
    "low-resource ner recent advances",
    "prompt",
    "text",
    "few-shot text classification",
    "-",
    "learner"
  ],
  "url": "https://aclanthology.org/2022.acl-long.192/",
  "provenance": {
    "collected_at": "2025-06-05 08:26:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}