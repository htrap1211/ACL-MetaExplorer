{
  "id": "2023.acl-long.119",
  "title": "A}t{TG}en: Attribute Tree Generation for Real-World Attribute Joint Extraction",
  "authors": [
    "Li, Yanzeng  and\nXue, Bingcong  and\nZhang, Ruoyu  and\nZou, Lei"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Attribute extraction aims to identify attribute names and the corresponding values from descriptive texts, which is the foundation for extensive downstream applications such as knowledge graph construction, search engines, and e-Commerce. In previous studies, attribute extraction is generally treated as a classification problem for predicting attribute types or a sequence tagging problem for labeling attribute values, where two paradigms, i.e., closed-world and open-world assumption, are involved. However, both of these paradigms have limitations in terms of real-world applications. And prior studies attempting to integrate these paradigms through ensemble, pipeline, and co-training models, still face challenges like cascading errors, high computational overhead, and difficulty in training. To address these existing problems, this paper presents Attribute Tree, a unified formulation for real-world attribute extraction application, where closed-world, open-world, and semi-open attribute extraction tasks are modeled uniformly. Then a text-to-tree generation model, AtTGen, is proposed to learn annotations from different scenarios efficiently and consistently. Experiments demonstrate that our proposed paradigm well covers various scenarios for real-world applications, and the model achieves state-of-the-art, outperforming existing methods by a large margin on three datasets. Our code, pretrained model, and datasets are available athttps://github.com/lsvih/AtTGen.",
  "keywords": [
    "code",
    "a unified formulation",
    "extraction",
    "prior studies",
    "graph",
    "training",
    "classification",
    "ensemble",
    "attribute tree generation",
    "unified",
    "e",
    "sequence",
    "efficiently and consistently experiments",
    "previous studies",
    "text"
  ],
  "url": "https://aclanthology.org/2023.acl-long.119/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}