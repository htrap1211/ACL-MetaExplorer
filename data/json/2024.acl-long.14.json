{
  "id": "2024.acl-long.14",
  "title": "Inference to the Best Explanation in Large Language Models",
  "authors": [
    "Dalal, Dhairya  and\nValentino, Marco  and\nFreitas, Andre  and\nBuitelaar, Paul"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposesIBE-Eval, a framework inspired by philosophical accounts onInference to the Best Explanation (IBE)to advance the interpretation and evaluation of LLMs’ explanations.IBE-Evalestimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including:consistency,parsimony,coherence, anduncertainty. Extensive experiments are conducted onCausal Question Answering (CQA), whereIBE-Evalis tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal thatIBE-Evalcan successfully identify the best explanation with up to 77% accuracy (≈ 27%above random), improving upon a GPT 3.5-as-a-Judge baseline (≈+17%) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and thatIBE-Evalis significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.",
  "keywords": [
    "a gpt",
    "opportunities",
    "efficient",
    "question",
    "natural",
    "model-specific variances llm-generated explanations",
    "cqa whereibe-evalis",
    "llms",
    "-",
    "large language models llms",
    "variances",
    "eval",
    "cqa",
    "accuracy",
    "process"
  ],
  "url": "https://aclanthology.org/2024.acl-long.14/",
  "provenance": {
    "collected_at": "2025-06-05 10:34:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}