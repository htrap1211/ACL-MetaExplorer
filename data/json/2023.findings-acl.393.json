{
  "id": "2023.findings-acl.393",
  "title": "Distilling Calibrated Knowledge for Stance Detection",
  "authors": [
    "Li, Yingjie  and\nCaragea, Cornelia"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Stance detection aims to determine the position of an author toward a target and provides insights into peopleâ€™s views on controversial topics such as marijuana legalization. Despite recent progress in this task, most existing approaches use hard labels (one-hot vectors) during training, which ignores meaningful signals among categories offered by soft labels. In this work, we explore knowledge distillation for stance detection and present a comprehensive analysis. Our contributions are: 1) we propose to use knowledge distillation over multiple generations in which a student is taken as a new teacher to transfer knowledge to a new fresh student; 2) we propose a novel dynamic temperature scaling for knowledge distillation to calibrate teacher predictions in each generation step. Extensive results on three stance detection datasets show that knowledge distillation benefits stance detection and a teacher is able to transfer knowledge to a student more smoothly via calibrated guiding signals. We publicly release our code to facilitate future research.",
  "keywords": [
    "code",
    "each generation",
    "we",
    "training",
    "multiple generations",
    "views",
    "analysis",
    "categories",
    "one-hot vectors",
    "generations",
    "soft",
    "hot",
    "work",
    "knowledge",
    "generation"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.393/",
  "provenance": {
    "collected_at": "2025-06-05 10:13:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}