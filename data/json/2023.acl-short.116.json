{
  "id": "2023.acl-short.116",
  "title": "Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data",
  "authors": [
    "Li, Yufei  and\nYu, Xiao  and\nLiu, Yanchi  and\nChen, Haifeng  and\nLiu, Cong"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves as filtering noisy instances and facilitating the model to converge fast at the early stage. During bootstrap learning, we propose self-ensembling as a regularizer to alleviate inter-model uncertainty produced by noisy labels. We further define probability variance of joint tagging probabilities to estimate inner-model parametric uncertainty, which is used to select and build up new reliable training instances for the next iteration. Experimental results on two large datasets reveal that our approach outperforms existing strong baselines and related methods.",
  "keywords": [
    "extraction",
    "we",
    "variance",
    "training",
    "self",
    "inner",
    "learning",
    "joint tagging probabilities",
    "early",
    "inner-model parametric uncertainty",
    "probability variance",
    "model",
    "probabilities",
    "entity",
    "approach"
  ],
  "url": "https://aclanthology.org/2023.acl-short.116/",
  "provenance": {
    "collected_at": "2025-06-05 09:49:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}