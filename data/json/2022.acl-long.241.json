{
  "id": "2022.acl-long.241",
  "title": "Image Retrieval from Contextual Descriptions",
  "authors": [
    "Krojer, Benno  and\nAdlakha, Vaibhav  and\nVineet, Vibhav  and\nGoyal, Yash  and\nPonti, Edoardo  and\nReddy, Siva"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe.Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.",
  "keywords": [
    "we",
    "syntax",
    "current",
    "cross",
    "retrieval",
    "current vision-and-language models",
    "visual",
    "-",
    "an accuracy",
    "encoders",
    "accuracy",
    "bi",
    "language",
    "model",
    "human"
  ],
  "url": "https://aclanthology.org/2022.acl-long.241/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}