{
  "id": "2023.americasnlp-1.17",
  "title": "C}hat{GPT} is not a good indigenous translator",
  "authors": [
    "Stap, David  and\nAraabi, Ali"
  ],
  "year": "2023",
  "venue": "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
  "abstract": "This report investigates the continuous challenges of Machine Translation (MT) systems on indigenous and extremely low-resource language pairs. Despite the notable achievements of Large Language Models (LLMs) that excel in various tasks, their applicability to low-resource languages remains questionable. In this study, we leveraged the AmericasNLP competition to evaluate the translation performance of different systems for Spanish to 11 indigenous languages from South America. Our team, LTLAmsterdam, submitted a total of four systems including GPT-4, a bilingual model, fine-tuned M2M100, and a combination of fine-tuned M2M100 with $k$NN-MT. We found that even large language models like GPT-4 are not well-suited for extremely low-resource languages. Our results suggest that fine-tuning M2M100 models can offer significantly better performance for extremely low-resource translation.",
  "keywords": [
    "the translation performance",
    "language",
    "even large language models",
    "fine-tuning m2m100 models",
    "americasnlp",
    "model",
    "machine",
    "c hat gpt",
    "llms",
    "machine translation mt systems",
    "large language models llms",
    "extremely low-resource translation",
    "gpt-4",
    "gpt",
    "fine"
  ],
  "url": "https://aclanthology.org/2023.americasnlp-1.17/",
  "provenance": {
    "collected_at": "2025-06-05 10:21:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}