{
  "id": "2023.acl-long.454",
  "title": "F}i{D}-{ICL}: A Fusion-in-Decoder Approach for Efficient In-Context Learning",
  "authors": [
    "Ye, Qinyuan  and\nBeltagy, Iz  and\nPeters, Matthew  and\nRen, Xiang  and\nHajishirzi, Hannaneh"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Large pre-trained models are capable of few-shot in-context learning (ICL), i.e., performing a new task by prepending a few demonstrations before the test input. However, the concatenated demonstrations are often excessively long and induce additional computation. Inspired by fusion-in-decoder (FiD) models which efficiently aggregate more passages and thus outperforms concatenation-based models in open-domain QA, we hypothesize that similar techniques can be applied to improve the efficiency and end-task performance of ICL. To verify this, we present a comprehensive study on applying three fusion methods—concatenation-based (early fusion), FiD (intermediate), and ensemble-based (late)—to ICL. We adopt a meta-learning setup where a model is first trained to perform ICL on a mixture of tasks using one selected fusion method, then evaluated on held-out tasks for ICL. Results on 11 held-out tasks show that FiD-ICL matches or outperforms the other two fusion methods. Additionally, we show that FiD-ICL (1) is 10x faster at inference time compared to concat-based and ensemble-based ICL, as we can easily pre-compute the representations of in-context examples and reuse them; (2) enables scaling up to meta-training 3B-sized models, which would fail for concat-based ICL.",
  "keywords": [
    "end",
    "efficient",
    "efficiency",
    "we",
    "fusion",
    "shot",
    "training",
    "ensemble",
    "efficient in-context",
    "decoder",
    "open-domain qa",
    "i",
    "early",
    "concat-based and ensemble-based icl",
    "model"
  ],
  "url": "https://aclanthology.org/2023.acl-long.454/",
  "provenance": {
    "collected_at": "2025-06-05 09:41:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}