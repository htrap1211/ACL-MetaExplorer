{
  "id": "2022.findings-acl.234",
  "title": "Modular and Parameter-Efficient Multimodal Fusion with Prompting",
  "authors": [
    "Liang, Sheng  and\nZhao, Mengjie  and\nSchuetze, Hinrich"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings. We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.",
  "keywords": [
    "the modalities",
    "processing",
    "prompt",
    "model",
    "it",
    "efficient",
    "vectors",
    "modalities",
    "-",
    "prompt vectors",
    "we",
    "efficient and flexible methods",
    "pre",
    "fusion",
    "parameter"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.234/",
  "provenance": {
    "collected_at": "2025-06-05 08:38:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}