{
  "id": "2024.acl-long.545",
  "title": "S}ym{KGQA}: Few-Shot Knowledge Graph Question Answering via Symbolic Program Generation and Execution",
  "authors": [
    "Agarwal, Prerna  and\nKumar, Nishant  and\nBedathur, Srikanta"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Semantic Parsing of natural language questions into their executable logical form (LF) has shown state-of-the-art (SOTA) performance for Knowledge Graph Question Answering (KGQA). However, these methods are not applicable for real-world applications, due to lack of KG-specific training data. Recent advances in the capabilities of Large Language Models (LLMs) has led towards generating low-level LFs such as SPARQL and S-Expression in a few-shot setting. Unfortunately, these methods: (1) are limited to the knowledge of underlying LLM about the LF, (2) performs inferior for the harder complex benchmarks such as KQA Pro, (3) suffers while grounding the generated LF to a specific Knowledge Graph. Recently, a new LF called KoPL has been introduced that explicitly models complex reasoning process step-by-step in a symbolic manner and has shown SOTA on KQA Pro in fully-supervised setting. Inspired by this, we propose SymKGQA framework that generates step-by-step Symbolic LF i.e., KoPL in a few-shot in-context learning setting using LLM. Our framework is not dependent on pre-trained information of LLM about KoPL. We further build a Retrieval-Augmented Generation based Question-Aware Contextual KoPL (QUACK) resolver to ground the generated LF. Our experiments with different LLMs and few-shot settings demonstrate that SymKGQA outperforms all other few-shot and even many of the fully-supervised KGQA approaches.",
  "keywords": [
    "a symbolic manner",
    "symkgqa",
    "parsing",
    "kgqa",
    "all other few-shot",
    "question",
    "form",
    "semantic",
    "we",
    "kg-specific training data",
    "graph",
    "underlying llm",
    "llm",
    "shot",
    "training"
  ],
  "url": "https://aclanthology.org/2024.acl-long.545/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}