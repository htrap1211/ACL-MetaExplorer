{
  "id": "2021.acl-long.518",
  "title": "Learning Dense Representations of Phrases at Scale",
  "authors": [
    "Lee, Jinhyuk  and\nSung, Mujeen  and\nKang, Jaewoo  and\nChen, Danqi"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.",
  "keywords": [
    "current phrase retrieval models",
    "a phrase retrieval problem",
    "question",
    "previous phrase retrieval models",
    "we",
    "current",
    "training",
    "scale open-domain question answering",
    "a query-side fine-tuning strategy",
    "retrieval",
    "open-domain qa",
    "learning",
    "transfer",
    "transfer learning",
    "tuning"
  ],
  "url": "https://aclanthology.org/2021.acl-long.518/",
  "provenance": {
    "collected_at": "2025-06-05 08:06:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}