{
  "id": "2021.acl-long.87",
  "title": "Selecting Informative Contexts Improves Language Model Fine-tuning",
  "authors": [
    "Antonello, Richard  and\nBeckage, Nicole  and\nTurek, Javier  and\nHuth, Alexander"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a validation metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning â€” we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.",
  "keywords": [
    "language model fine-tuning",
    "pretrained secondary learners",
    "the overall energy footprint",
    "validation",
    "efficient",
    "efficiency",
    "we",
    "efficient and effective fine-tuning",
    "training",
    "fine-tuning",
    "natural",
    "generality",
    "the overall training efficiency",
    "information",
    "a median perplexity"
  ],
  "url": "https://aclanthology.org/2021.acl-long.87/",
  "provenance": {
    "collected_at": "2025-06-05 08:00:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}