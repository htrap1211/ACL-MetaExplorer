{
  "id": "2023.findings-acl.529",
  "title": "Unsupervised Summarization Re-ranking",
  "authors": [
    "Ravaut, Mathieu  and\nJoty, Shafiq  and\nChen, Nancy"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).",
  "keywords": [
    "zero-shot performance",
    "objectives",
    "unsupervised summarization",
    "task-specific pre-training objectives",
    "four widely-adopted summarization benchmarks",
    "a very high variance",
    "an unsupervised manner",
    "downstream summarization tasks",
    "chatgpt",
    "abstractive summarization models",
    "we",
    "summarization",
    "rouge",
    "transfer",
    "pre"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.529/",
  "provenance": {
    "collected_at": "2025-06-05 10:15:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}