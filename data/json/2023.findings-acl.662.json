{
  "id": "2023.findings-acl.662",
  "title": "Learning from a Friend: Improving Event Extraction via Self-Training with Feedback from {A}bstract {M}eaning {R}epresentation",
  "authors": [
    "Xu, Zhiyang  and\nLee, Jay Yoon  and\nHuang, Lifu"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by comparing it to the Abstract Meaning Representation (AMR) graph of the same sentence. Specifically, STF consists of (1) a base event extraction model trained on existing event annotations and then applied to large-scale unlabeled corpora to predict new event mentions as pseudo training samples, and (2) a novel scoring model that takes in each new predicted event trigger, an argument, its argument role, as well as their paths in the AMR graph to estimate a compatibility score indicating the correctness of the pseudo label. The compatibility scores further act as feedback to encourage or discourage the model learning on the pseudo labels during self-training. Experimental results on three benchmark datasets, including ACE05-E, ACE05-E+, and ERE, demonstrate the effectiveness of the STF framework on event extraction, especially event argument extraction, with significant performance gain over the base event extraction models and strong baselines. Our experimental analysis further shows that STF is a generic framework as it can be applied to improve most, if not all, event extraction models by leveraging large-scale unlabeled data, even when high-quality AMR graph annotations are not available.",
  "keywords": [
    "friend",
    "feedback",
    "extraction",
    "we",
    "graph",
    "training",
    "it",
    "self",
    "analysis",
    "generic",
    "ere",
    "a friend",
    "model",
    "a generic framework",
    "act"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.662/",
  "provenance": {
    "collected_at": "2025-06-05 10:17:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}