{
  "id": "W18-2716",
  "title": "M}arian: Cost-effective High-Quality Neural Machine Translation in {C}++",
  "authors": [
    "Junczys-Dowmunt, Marcin  and\nHeafield, Kenneth  and\nHoang, Hieu  and\nGrundkiewicz, Roman  and\nAue, Anthony"
  ],
  "year": "2018",
  "venue": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
  "abstract": "This paper describes the submissions of the “Marian” team to the WNMT 2018 shared task. We investigate combinations of teacher-student training, low-precision matrix products, auto-tuning and other methods to optimize the Transformer model on GPU and CPU. By further integrating these methods with the new averaging attention networks, a recently introduced faster Transformer variant, we create a number of high-quality, high-performance models on the GPU and CPU, dominating the Pareto frontier for this shared task.",
  "keywords": [
    "transformer",
    "tuning",
    "the transformer model",
    "precision",
    "translation",
    "neural",
    "attention networks",
    "the pareto frontier",
    "machine",
    "model",
    "c",
    "frontier",
    "attention",
    "we",
    "training"
  ],
  "url": "https://aclanthology.org/W18-2716/",
  "provenance": {
    "collected_at": "2025-06-05 00:23:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}