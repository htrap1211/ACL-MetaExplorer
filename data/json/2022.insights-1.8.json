{
  "id": "2022.insights-1.8",
  "title": "Is {BERT} Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification",
  "authors": [
    "Zhu, Dawei  and\nHedderich, Michael A.  and\nZhai, Fangzhou  and\nAdelani, David Ifeoluwa  and\nKlakow, Dietrich"
  ],
  "year": "2022",
  "venue": "Proceedings of the Third Workshop on Insights from Negative Results in NLP",
  "abstract": "Incorrect labels in training data occur when human annotators make mistakes or when the data is generated via weak or distant supervision. It has been shown that complex noise-handling techniques - by modeling, cleaning or filtering the noisy instances - are required to prevent models from fitting this label noise. However, we show in this work that, for text classification tasks with modern NLP models like BERT, over a variety of noise types, existing noise-handling methods do not always improve its performance, and may even deteriorate it, suggesting the need for further investigation. We also back our observations with a comprehensive analysis.",
  "keywords": [
    "work",
    "variety",
    "bert robust",
    "text classification incorrect labels",
    "nlp",
    "bert",
    "text",
    "it",
    "human",
    "modeling",
    "we",
    "text classification tasks",
    "a variety",
    "modern nlp models",
    "analysis"
  ],
  "url": "https://aclanthology.org/2022.insights-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 08:43:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}