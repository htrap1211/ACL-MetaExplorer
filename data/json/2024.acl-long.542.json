{
  "id": "2024.acl-long.542",
  "title": "C}o{ELM}: Construction-Enhanced Language Modeling",
  "authors": [
    "Xu, Lvxiaowei  and\nGong, Zhilin  and\nDai, Jianhua  and\nWang, Tianxiang  and\nCai, Ming  and\nPeng, Jiawei"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent studies have shown that integrating constructional information can improve the performance of pre-trained language models (PLMs) in natural language understanding. However, exploration into leveraging constructional information to enhance generative language models for natural language generation has been limited. Additionally, probing studies indicate that PLMs primarily grasp the syntactic structure of constructions but struggle to capture their semantics. In this work, we encode constructions as inductive biases to explicitly embed constructional semantics and guide the generation process. We begin by presenting a construction grammar induction framework designed to automatically identify constructions from corpora. Subsequently, we propose the Construction-Enhanced Language Model (CoELM). It introduces a construction-guided language modeling approach that employs a dynamic sequence reassembly strategy during pre-training. Extensive experiments have demonstrated the superiority of CoELM across various benchmarks.",
  "keywords": [
    "generative",
    "work",
    "process",
    "pre-training extensive experiments",
    "language",
    "the generation process",
    "generation",
    "natural",
    "model",
    "studies",
    "semantics",
    "it",
    "constructional semantics",
    "inductive biases",
    "modeling"
  ],
  "url": "https://aclanthology.org/2024.acl-long.542/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}