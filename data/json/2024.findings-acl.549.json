{
  "id": "2024.findings-acl.549",
  "title": "On the Vulnerability of Safety Alignment in Open-Access {LLM}s",
  "authors": [
    "Yi, Jingwei  and\nYe, Rui  and\nChen, Qisi  and\nZhu, Bin  and\nChen, Siheng  and\nLian, Defu  and\nSun, Guangzhong  and\nXie, Xing  and\nWu, Fangzhao"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large language models (LLMs) possess immense capabilities but are susceptible to malicious exploitation. To mitigate the risk, safety alignment is employed to align LLMs with ethical standards. However, safety-aligned LLMs may remain vulnerable to carefully crafted jailbreak attacks, but these attacks often face high rejection rates and limited harmfulness. In this paper, we expose the vulnerabilities of safety alignment in open-access LLMs, which can significantly enhance the success rate and harmfulness of jailbreak attacks. Through reverse alignment, achieved by accessing model parameters, we show the feasibility of efficiently fine-tuning LLMs to undermine their inherent safeguards. We investigate two types of reverse alignment techniques: reverse supervised fine-tuning (RSFT) and reverse preference optimization (RPO). RSFT operates by supervising the fine-tuning of LLMs to reverse their inherent values. We also explore how to prepare data needed for RSFT. RPO optimizes LLMs to enhance their preference for harmful content, reversing the modelsâ€™ safety alignment. Our extensive experiments reveal that open-access high-performance LLMs can be adeptly reverse-aligned to output harmful content, even in the absence of manually curated malicious datasets. Our research acts as a whistleblower for the community, emphasizing the need to pay more attention to safety of open-accessing LLMs. It also underscores the limitations of current safety alignment approaches and calls for research on robust safety alignment methods to counteract malicious fine-tuning attacks.",
  "keywords": [
    "reverse alignment techniques",
    "reverse alignment",
    "safety-aligned llms",
    "rate",
    "we",
    "the vulnerabilities",
    "current",
    "llm",
    "robust safety alignment methods",
    "it",
    "efficiently fine-tuning llms",
    "supervised fine-tuning rsft",
    "more attention",
    "vulnerable",
    "open-accessing llms"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.549/",
  "provenance": {
    "collected_at": "2025-06-05 10:56:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}