{
  "id": "2021.acl-long.343",
  "title": "A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations",
  "authors": [
    "Tao, Chongyang  and\nChen, Changyu  and\nFeng, Jiazhan  and\nWen, Ji-Rong  and\nYan, Rui"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans. However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching. To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model. The former two tasks could help the model in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.",
  "keywords": [
    "background knowledge",
    "conversations",
    "knowledge-grounded conversations",
    "we",
    "dialogue",
    "training",
    "it",
    "the background documents",
    "unified",
    "retrieval",
    "recently many studies",
    "learning",
    "large-scale dialogues",
    "background",
    "a retrieval-based dialogue system"
  ],
  "url": "https://aclanthology.org/2021.acl-long.343/",
  "provenance": {
    "collected_at": "2025-06-05 08:04:01",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}