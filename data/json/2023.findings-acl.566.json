{
  "id": "2023.findings-acl.566",
  "title": "CIF}-{PT}: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training",
  "authors": [
    "Dong, Linhao  and\nAn, Zhecheng  and\nWu, Peihao  and\nZhang, Jun  and\nLu, Lu  and\nZejun, Ma"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Speech or text representation generated by pre-trained models contains modal-specific information that could be combined for benefiting spoken language understanding (SLU) tasks. In this work, we propose a novel pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training (CIF-PT). It relies on a simple but effective frame-to-token alignment: continuous integrate-and-fire (CIF) to bridge the representations between speech and text. It jointly performs speech-to-text training and language model distillation through CIF as the pre-training (PT). Evaluated on SLU benchmark SLURP dataset, CIF-PT outperforms the state-of-the-art model by 1.94% of accuracy and 2.71% of SLU-F1 on the tasks of intent classification and slot filling, respectively. We also observe the cross-modal representation extracted by CIF-PT obtains better performance than other neural interfaces for the tasks of SLU, including the dominant speech representation learned from self-supervised pre-training.",
  "keywords": [
    "work",
    "alignment",
    "cross",
    "intent classification",
    "the pre-training pt",
    "language",
    "neural",
    "model",
    "text",
    "it",
    "a novel pre-training paradigm",
    "token",
    "self",
    "-",
    "information"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.566/",
  "provenance": {
    "collected_at": "2025-06-05 10:16:19",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}