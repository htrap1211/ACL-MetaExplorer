{
  "id": "2024.findings-acl.193",
  "title": "Likelihood-based Mitigation of Evaluation Bias in Large Language Models",
  "authors": [
    "Ohi, Masanari  and\nKaneko, Masahiro  and\nKoike, Ryuto  and\nLoem, Mengsay  and\nOkazaki, Naoaki"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics.However, the likelihood, a measure of LLMâ€™s plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure.It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods.In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators.We also propose a method to mitigate the likelihood bias.Our method utilizes highly biased instances as few-shot examples for in-context learning.Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias.Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.",
  "keywords": [
    "bias",
    "the likelihood bias",
    "evaluation bias",
    "we",
    "llm",
    "shot",
    "natural",
    "it",
    "a likelihood bias",
    "word",
    "biased",
    "llm s plausibility",
    "automated metrics",
    "likelihood bias",
    "llms"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.193/",
  "provenance": {
    "collected_at": "2025-06-05 10:51:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}