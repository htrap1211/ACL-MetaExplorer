{
  "id": "2021.acl-short.135",
  "title": "S}im{CLS}: A Simple Framework for Contrastive Learning of Abstractive Summarization",
  "authors": [
    "Liu, Yixin  and\nLiu, Pengfei"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "In this paper, we present a conceptually simple while empirically powerful framework for abstractive summarization, SimCLS, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing top-scoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results:https://github.com/yixinL7/SimCLS. Results of our proposed models have been deployed into ExplainaBoard platform, which allows researchers to understand our systems in a more fine-grained way.",
  "keywords": [
    "summarization",
    "we",
    "text generation",
    "cnn",
    "sequence",
    "learning",
    "the learning objective",
    "i",
    "s",
    "text",
    "objective",
    "metrics",
    "abstractive summarization",
    "evaluation metrics",
    "abstractive summarization simcls"
  ],
  "url": "https://aclanthology.org/2021.acl-short.135/",
  "provenance": {
    "collected_at": "2025-06-05 08:08:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}