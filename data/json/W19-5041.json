{
  "id": "W19-5041",
  "title": "Pentagon at {MEDIQA} 2019: Multi-task Learning for Filtering and Re-ranking Answers using Language Inference and Question Entailment",
  "authors": [
    "Pugaliya, Hemant  and\nSaxena, Karan  and\nGarg, Shefali  and\nShalini, Sheetal  and\nGupta, Prashant  and\nNyberg, Eric  and\nMitamura, Teruko"
  ],
  "year": "2019",
  "venue": "Proceedings of the 18th BioNLP Workshop and Shared Task",
  "abstract": "Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have quickly become the state of the art, bypassing previous deep and shallow learning methods by a large margin. More recently, pre-trained models from large related datasets have been able to perform well on many downstream tasks by just fine-tuning on domain-specific datasets (similar to transfer learning). However, using powerful models on non-trivial tasks, such as ranking and large document classification, still remains a challenge due to input size limitations of parallel architecture and extremely small datasets (insufficient for fine-tuning). In this work, we introduce an end-to-end system, trained in a multi-task setting, to filter and re-rank answers in the medical domain. We use task-specific pre-trained models as deep feature extractors. Our model achieves the highest Spearmanâ€™s Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on the ACL-BioNLP workshop MediQA Question Answering shared-task.",
  "keywords": [
    "deep",
    "end",
    "question",
    "we",
    "classification",
    "fine-tuning",
    "entailment parallel deep learning",
    "fine-tuned bert",
    "mediqa 2019 multi-task learning",
    "learning",
    "transfer",
    "tuning",
    "bert",
    "bionlp",
    "fine"
  ],
  "url": "https://aclanthology.org/W19-5041/",
  "provenance": {
    "collected_at": "2025-06-05 00:56:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}