{
  "id": "2022.acl-long.552",
  "title": "Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs",
  "authors": [
    "Shang, Chao  and\nWang, Guangtao  and\nQi, Peng  and\nHuang, Jing"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Question answering over temporal knowledge graphs (KGs) efficiently uses facts contained in a temporal KG, which records entity relations and when they occur in time, to answer natural language questions (e.g., “Who was the president of the US before Obama?”). These questions often involve three time-related challenges that previous work fail to adequately address: 1) questions often do not specify exact timestamps of interest (e.g., “Obama” instead of 2000); 2) subtle lexical differences in time relations (e.g., “before” vs “after”); 3) off-the-shelf temporal KG embeddings that previous work builds on ignore the temporal order of timestamps, which is crucial for answering temporal-order related questions. In this paper, we propose a time-sensitive question answering (TSQA) framework to tackle these problems. TSQA features a timestamp estimation module to infer the unwritten timestamp from the question. We also employ a time-sensitive KG encoder to inject ordering information into the temporal KG embeddings that TSQA is based on. With the help of techniques to reduce the search space for potential answers, TSQA significantly outperforms the previous state of the art on a new benchmark for question answering over temporal KGs, especially achieving a 32% (absolute) error reduction on complex questions that require multiple steps of reasoning over facts in the temporal KG.",
  "keywords": [
    "question",
    "we",
    "natural",
    "information",
    "kgs",
    "the temporal kg",
    "reduction",
    "a temporal kg",
    "work",
    "embeddings",
    "e g",
    "knowledge",
    "language",
    "temporal knowledge graphs",
    "encoder"
  ],
  "url": "https://aclanthology.org/2022.acl-long.552/",
  "provenance": {
    "collected_at": "2025-06-05 08:31:44",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}