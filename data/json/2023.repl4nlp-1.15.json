{
  "id": "2023.repl4nlp-1.15",
  "title": "Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling",
  "authors": [
    "Mohammadshahi, Alireza  and\nHenderson, James"
  ],
  "year": "2023",
  "venue": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
  "abstract": "Recent models have shown that incorporating syntactic knowledge into the semantic role labelling (SRL) task leads to a significant improvement. In this paper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model, which encodes the syntactic structure using a novel way to input graph relations as embeddings, directly into the self-attention mechanism of Transformer. This approach adds a soft bias towards attention patterns that follow the syntactic structure but also allows the model to use this information to learn alternative patterns. We evaluate our model on both span-based and dependency-based SRL datasets, and outperform previous alternative methods in both in-domain and out-of-domain settings, on CoNLL 2005 and CoNLL 2009 datasets.",
  "keywords": [
    "embeddings",
    "transformer",
    "attention patterns",
    "knowledge",
    "bias",
    "semantic role",
    "model",
    "self",
    "information",
    "the self-attention mechanism",
    "dependency",
    "semantic",
    "attention",
    "the semantic role",
    "we"
  ],
  "url": "https://aclanthology.org/2023.repl4nlp-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 10:26:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}