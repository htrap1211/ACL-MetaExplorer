{
  "id": "2024.findings-acl.631",
  "title": "DORY}: Deliberative Prompt Recovery for {LLM",
  "authors": [
    "Gao, Lirong  and\nPeng, Ru  and\nZhang, Yiming  and\nZhao, Junbo"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Prompt recovery in large language models (LLMs) is crucial for understanding how LLMs work and addressing concerns regarding privacy, copyright, etc. The trend towards inference-only APIs complicates this task by restricting access to essential outputs for recovery. To tackle this challenge, we extract prompt-related information from limited outputs and identify a strong(negative) correlation between output probability-based uncertainty and the success of prompt recovery.This finding led to the development of Deliberative PrOmpt RecoverY (DORY), our novel approach that leverages uncertainty to recover prompts accurately. DORY involves reconstructing drafts from outputs, refining these with hints, and filtering out noise based on uncertainty. Our evaluation shows that DORY outperforms existing baselines across diverse LLMs and prompt benchmarks, improving performance by approximately 10.82% and establishing a new state-of-the-art record in prompt recovery tasks. Significantly, DORY operates using a single LLM without any external resources or model, offering a cost-effective, user-friendly prompt recovery solution.",
  "keywords": [
    "prompt benchmarks",
    "we",
    "llm",
    "deliberative prompt recovery",
    "friendly",
    "llm prompt recovery",
    "information",
    "llms",
    "prompt",
    "large language models llms",
    "our evaluation",
    "prompts",
    "language",
    "model",
    "a single llm"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.631/",
  "provenance": {
    "collected_at": "2025-06-05 10:57:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}