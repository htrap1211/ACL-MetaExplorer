{
  "id": "2023.findings-acl.528",
  "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
  "authors": [
    "Nie, Ercong  and\nLiang, Sheng  and\nSchmid, Helmut  and\nSch{\\\"u}tze, Hinrich"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in unlabeled (+5.1%) and labeled settings (+16.3%). PARC also outperforms finetuning by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs.",
  "keywords": [
    "cross",
    "prompts",
    "families",
    "zero-shot performance",
    "prompt",
    "language",
    "natural",
    "semantically similar sentences",
    "topic",
    "retrieval",
    "cross-lingual retrieval",
    "we",
    "transfer",
    "6 language families",
    "retrieval crosslingually parc"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.528/",
  "provenance": {
    "collected_at": "2025-06-05 10:15:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}