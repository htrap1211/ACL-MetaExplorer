{
  "id": "2024.smm4h-1.2",
  "title": "CTYUN}-{AI}@{SMM}4{H}-2024: Knowledge Extension Makes Expert Models",
  "authors": [
    "Fan, Yuming  and\nYang, Dongming  and\nCao, Lina"
  ],
  "year": "2024",
  "venue": "Proceedings of the 9th Social Media Mining for Health Research and Applications (SMM4H 2024) Workshop and Shared Tasks",
  "abstract": "This paper explores the potential of social media as a rich source of data for understanding public health trends and behaviors, particularly focusing on emotional well-being and the impact of environmental factors. We employed large language models (LLMs) and developed a suite of knowledge extension techniques to analyze social media content related to mental health issues, specifically examining 1) effects of outdoor spaces on social anxiety symptoms in Reddit,2) tweets reporting childrenâ€™s medical disorders, and 3) self-reported ages in posts of Twitter and Reddit. Our knowledge extension approach encompasses both supervised data (i.e., sample augmentation and cross-task fine-tuning) and unsupervised data (i.e., knowledge distillation and cross-task pre-training), tackling the inherent challenges of sample imbalance and informality of social media language. The effectiveness of our approach is demonstrated by the superior performance across multiple tasks (i.e., Task 3, 5 and 6) at the SMM4H-2024. Notably, we achieved the best performance in all three tasks, underscoring the utility of our models in real-world applications.",
  "keywords": [
    "cross-task pre-training",
    "we",
    "training",
    "cross",
    "self",
    "rich",
    "llms",
    "tuning",
    "large language models llms",
    "knowledge",
    "anxiety",
    "language",
    "pre",
    "social anxiety symptoms",
    "approach"
  ],
  "url": "https://aclanthology.org/2024.smm4h-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 11:10:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}