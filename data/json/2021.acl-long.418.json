{
  "id": "2021.acl-long.418",
  "title": "Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators",
  "authors": [
    "Liu, Peiyu  and\nGao, Ze-Feng  and\nZhao, Wayne Xin  and\nXie, Zhi-Yuan  and\nLu, Zhong-Yi  and\nWen, Ji-Rong"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics. It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters). With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm for MPO-based approximation over stacked network architectures. Our approach can be applied to the original or the compressed PLMs in a general way, which derives a lighter network and significantly reduces the parameters to be fine-tuned. Extensive experiments have demonstrated the effectiveness of the proposed approach in model compression, especially the reduction in fine-tuning parameters (91% reduction on average). The code to reproduce the results of this paper can be found athttps://github.com/RUCAIBox/MPOP.",
  "keywords": [
    "code",
    "pre-trained language model compression",
    "we",
    "it",
    "information",
    "core",
    "reduction",
    "tuning",
    "fine-tuning parameters",
    "an optimization",
    "fine",
    "a general way",
    "general",
    "language",
    "model"
  ],
  "url": "https://aclanthology.org/2021.acl-long.418/",
  "provenance": {
    "collected_at": "2025-06-05 08:05:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}