{
  "id": "2023.acl-long.138",
  "title": "Can {NLI} Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?",
  "authors": [
    "Xu, Jiashu  and\nMa, Mingyu Derek  and\nChen, Muhao"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches. We present NBR, which converts biomedical RE as natural language inference formulation through indirect supervision. By converting relations to natural language hypotheses, NBR is capable of exploiting semantic cues to alleviate annotation scarcity. By incorporating a ranking-based loss that implicitly calibrates abstinent instances, NBR learns a clearer decision boundary and is instructed to abstain on uncertain instances. Extensive experiments on three widely-used biomedical RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in both full-set and low-resource regimes. Our analysis demonstrates that indirect supervision benefits biomedical RE even when a domain gap exists, and combining NLI knowledge with biomedical knowledge leads to the best performance gains.",
  "keywords": [
    "a multi-class classification task",
    "extraction",
    "semantic",
    "we",
    "poor generalization",
    "classification",
    "natural",
    "loss",
    "analysis",
    "generalization",
    "semantic cues",
    "knowledge",
    "language",
    "class",
    "pre"
  ],
  "url": "https://aclanthology.org/2023.acl-long.138/",
  "provenance": {
    "collected_at": "2025-06-05 09:37:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}