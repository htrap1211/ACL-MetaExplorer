{
  "id": "2023.findings-acl.492",
  "title": "Scaling Laws for {BERT} in Low-Resource Settings",
  "authors": [
    "Urbizu, Gorka  and\nSan Vicente, I{\\~n}aki  and\nSaralegi, Xabier  and\nAgerri, Rodrigo  and\nSoroa, Aitor"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners. Previous work has researched the scaling laws of such models, but optimal ratios of model parameters, dataset size, and computation costs focused on the large scale. In contrast, we analyze the effect those variables have on the performance of language models in constrained settings, by building three lightweight BERT models (16M/51M/124M parameters) trained over a set of small corpora (5M/25M/125M words).We experiment on four languages of different linguistic characteristics (Basque, Spanish, Swahili and Finnish), and evaluate the models on MLM and several NLU tasks. We conclude that the power laws for parameters, data and compute for low-resource settings differ from the optimal scaling laws previously inferred, and data requirements should be higher. Our insights are consistent across all the languages we study, as well as across the MLM and downstream tasks. Furthermore, we experimentally establish when the cost of using a Transformer-based approach is worth taking, instead of favouring other computationally lighter solutions.",
  "keywords": [
    "we",
    "training",
    "nlp practitioners",
    "three lightweight bert models",
    "a transformer-based approach",
    "bert",
    "language models",
    "practitioners",
    "work",
    "transformer",
    "language",
    "nlp",
    "model",
    "large language models",
    "approach"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.492/",
  "provenance": {
    "collected_at": "2025-06-05 10:15:18",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}