{
  "id": "2024.acl-short.29",
  "title": "R}ec{GPT}: Generative Pre-training for Text-based Recommendation",
  "authors": [
    "Ngo, Hoang  and\nNguyen, Dat Quoc"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "We present the first domain-adapted and fully-trained large language model, RecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for text-based recommendation. Experimental results on rating prediction and sequential recommendation tasks show that our model, RecGPT-7B-Instruct, outperforms previous strong baselines. We are releasing our RecGPT models as well as their pre-training and fine-tuning datasets to facilitate future research and downstream applications in text-based recommendation. Public “huggingface” links to our RecGPT models and datasets are available at: https://github.com/VinAIResearch/RecGPT",
  "keywords": [
    "instruction",
    "generative",
    "instruct",
    "tuning",
    "language",
    "generative pre",
    "model",
    "text",
    "-",
    "r",
    "its instruction-following variant recgpt-7b-instruct",
    "gpt",
    "recgpt-7b-instruct",
    "fine",
    "we"
  ],
  "url": "https://aclanthology.org/2024.acl-short.29/",
  "provenance": {
    "collected_at": "2025-06-05 10:46:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}