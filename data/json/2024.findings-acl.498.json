{
  "id": "2024.findings-acl.498",
  "title": "Question Translation Training for Better Multilingual Reasoning",
  "authors": [
    "Zhu, Wenhao  and\nHuang, Shujian  and\nYuan, Fei  and\nShe, Shuaijie  and\nChen, Jiajun  and\nBirch, Alexandra"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMsâ€™ multilingual reasoning abilities. Experimental results on LLaMA2-13B show that question alignment leads to consistent improvements over the translate-training approach: an average improvement of 11.3% and 16.1% accuracy across ten languages on the MGSM and MSVAMP multilingual reasoning benchmarks.",
  "keywords": [
    "chain",
    "that question alignment",
    "the llms",
    "question",
    "we",
    "training",
    "translation",
    "instruction",
    "question alignment",
    "16 1 accuracy",
    "llms",
    "abilities",
    "multilingual reasoning abilities",
    "question translation training",
    "text"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.498/",
  "provenance": {
    "collected_at": "2025-06-05 10:55:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}