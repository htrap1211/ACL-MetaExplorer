{
  "id": "2024.kallm-1.1",
  "title": "Multi-hop Database Reasoning with Virtual Knowledge Graph",
  "authors": [
    "Son, Juhee  and\nSeonwoo, Yeon  and\nYoon, Seunghyun  and\nThorne, James  and\nOh, Alice"
  ],
  "year": "2024",
  "venue": "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
  "abstract": "Application of LLM to database queries on natural language sentences has demonstrated impressive results in both single and multi-hop scenarios.In the existing methodologies, the requirement to re-encode query vectors at each stage for processing multi-hop queries presents a significant bottleneck to the inference speed.This paper proposes VKGFR (Virtual Knowledge Graph based Fact Retriever) that leverages large language models to extract representations corresponding to a sentenceâ€™s knowledge graph, significantly enhancing inference speed for multi-hop reasoning without performance loss.Given that both the queries and natural language database sentences can be structured as a knowledge graph, we suggest extracting a Virtual Knowledge Graph (VKG) representation from sentences with LLM.Over the pre-constructed VKG, our VKGFR conducts retrieval with a tiny model structure, showing performance improvements with higher computational efficiency. We evaluate VKGFR on the WikiNLDB and MetaQA dataset, designed for multi-hop database reasoning over text. The results indicate 13x faster inference speed on the WikiNLDB dataset without performance loss.",
  "keywords": [
    "encode",
    "efficiency",
    "we",
    "graph",
    "metaqa",
    "both the queries",
    "vkgfr",
    "llm",
    "vkg representation",
    "natural",
    "queries",
    "retrieval",
    "loss",
    "vkg",
    "natural language"
  ],
  "url": "https://aclanthology.org/2024.kallm-1.1/",
  "provenance": {
    "collected_at": "2025-06-05 11:07:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}