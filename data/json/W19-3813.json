{
  "id": "W19-3813",
  "title": "MS}net: A {BERT}-based Network for Gendered Pronoun Resolution",
  "authors": [
    "Wang, Zili"
  ],
  "year": "2019",
  "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
  "abstract": "The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing. For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT. This model is a type of mention score classifier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities. In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set. Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task. The code in this paper is available at:https://github.com/ziliwang/MSnet-for-Gendered-Pronoun-Resolution",
  "keywords": [
    "code",
    "bias",
    "validation",
    "classifier",
    "semantic",
    "an attention mechanism",
    "the triple-wise semantic similarity",
    "training",
    "the pre-trained bert model",
    "the entities",
    "the pre-trained bert",
    "cross",
    "neural",
    "natural",
    "net"
  ],
  "url": "https://aclanthology.org/W19-3813/",
  "provenance": {
    "collected_at": "2025-06-05 00:59:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}