{
  "id": "2022.findings-acl.71",
  "title": "M}o{E}fication: Transformer Feed-forward Layers are Mixtures of Experts",
  "authors": [
    "Zhang, Zhengyan  and\nLin, Yankai  and\nLiu, Zhiyuan  and\nLi, Peng  and\nSun, Maosong  and\nZhou, Jie"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained fromhttps://github.com/thunlp/MoEfication.",
  "keywords": [
    "code",
    "transformers",
    "we",
    "fication",
    "the inner mechanism",
    "it",
    "inner",
    "feed",
    "ratio",
    "work",
    "transformer",
    "knowledge",
    "model",
    "human",
    "pre"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.71/",
  "provenance": {
    "collected_at": "2025-06-05 08:35:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}