{
  "id": "2023.findings-acl.780",
  "title": "Common Law Annotations: Investigating the Stability of Dialog System Output Annotations",
  "authors": [
    "Lee, Seunggun  and\nDeLucia, Alexandra  and\nNangia, Nikita  and\nGanedi, Praneeth  and\nGuan, Ryan  and\nLi, Rubing  and\nNgaw, Britney  and\nSinghal, Aditya  and\nVaidya, Shalaka  and\nYuan, Zijun  and\nZhang, Lining  and\nSedoc, Jo{\\~a}o"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Metrics for Inter-Annotator Agreement (IAA), like Cohenâ€™s Kappa, are crucial for validating annotated datasets. Although high agreement is often used to show the reliability of annotation procedures, it is insufficient to ensure or reproducibility. While researchers are encouraged to increase annotator agreement, this can lead to specific and tailored annotation guidelines. We hypothesize that this may result in diverging annotations from different groups. To study this, we first propose the Lee et al. Protocol (LEAP), a standardized and codified annotation protocol. LEAP strictly enforces transparency in the annotation process, which ensures reproducibility of annotation guidelines. Using LEAP to annotate a dialog dataset, we empirically show that while research groups may create reliable guidelines by raising agreement, this can cause divergent annotations across different research groups, thus questioning the validity of the annotations. Therefore, we caution NLP researchers against using reliability as a proxy for reproducibility and validity.",
  "keywords": [
    "process",
    "nlp",
    "it",
    "metrics",
    "dialog",
    "nlp researchers",
    "we",
    "insufficient",
    "groups",
    "annotator agreement",
    "this",
    "cohen",
    "reliable",
    "high agreement",
    "inter-annotator agreement"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.780/",
  "provenance": {
    "collected_at": "2025-06-05 10:19:13",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}