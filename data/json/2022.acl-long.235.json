{
  "id": "2022.acl-long.235",
  "title": "Direct Speech-to-Speech Translation With Discrete Units",
  "authors": [
    "Lee, Ann  and\nChen, Peng-Jen  and\nWang, Changhan  and\nGu, Jiatao  and\nPopuri, Sravya  and\nMa, Xutai  and\nPolyak, Adam  and\nAdi, Yossi  and\nHe, Qing  and\nTang, Yun  and\nPino, Juan  and\nHsu, Wei-Ning"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech. When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features. When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages.",
  "keywords": [
    "language",
    "generation",
    "6 7 bleu",
    "model",
    "text",
    "self",
    "bleu",
    "encoder",
    "we",
    "sequence",
    "training",
    "intermediate text generation",
    "translation",
    "speech",
    "that"
  ],
  "url": "https://aclanthology.org/2022.acl-long.235/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}