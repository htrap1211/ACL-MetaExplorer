{
  "id": "2022.acl-long.294",
  "title": "RST} Discourse Parsing with Second-Stage {EDU}-Level Pre-training",
  "authors": [
    "Yu, Nan  and\nZhang, Meishan  and\nFu, Guohong  and\nZhang, Min"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Pre-trained language models (PLMs) have shown great potentials in natural language processing (NLP) including rhetorical structure theory (RST) discourse parsing. Current PLMs are obtained by sentence-level pre-training, which is different from the basic processing unit, i.e. element discourse unit (EDU).To this end, we propose a second-stage EDU-level pre-training approach in this work, which presents two novel tasks to learn effective EDU representations continually based on well pre-trained language models. Concretely, the two tasks are (1) next EDU prediction (NEP) and (2) discourse marker prediction (DMP).We take a state-of-the-art transition-based neural parser as baseline, and adopt it with a light bi-gram EDU modification to effectively explore the EDU-level pre-trained EDU representation. Experimental results on a benckmark dataset show that our method is highly effective,leading a 2.1-point improvement in F1-score. All codes and pre-trained models will be released publicly to facilitate future studies.",
  "keywords": [
    "end",
    "we",
    "current",
    "f1-score all codes",
    "training",
    "natural language processing nlp",
    "neural",
    "natural",
    "it",
    "future studies",
    "processing",
    "well pre-trained language models",
    "-",
    "edu",
    "work"
  ],
  "url": "https://aclanthology.org/2022.acl-long.294/",
  "provenance": {
    "collected_at": "2025-06-05 08:28:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}