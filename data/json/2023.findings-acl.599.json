{
  "id": "2023.findings-acl.599",
  "title": "Constructing Code-mixed {U}niversal {D}ependency Forest for Unbiased Cross-lingual Relation Extraction",
  "authors": [
    "Fei, Hao  and\nZhang, Meishan  and\nZhang, Min  and\nChua, Tat-Seng"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Latest efforts on cross-lingual relation extraction (XRE) aggressively leverage the language-consistent structural features from the universal dependency (UD) resource, while they may largely suffer from biased transfer (e.g., either target-biased or source-biased) due to the inevitable linguistic disparity between languages. In this work, we investigate an unbiased UD- based XRE transfer by constructing a type of code-mixed UD forest. We first translate the sentence of the source language to the parallel target-side language, for both of which we parse the UD tree respectively. Then, we merge the source-/target-side UD structures as a unified code-mixed UD forest. With such forest features, the gaps of UD-based XRE between the training and predicting phases can be effectively closed. We conduct experiments on the ACE XRE benchmark datasets, where the results demonstrate that the proposed code-mixed UD forests help unbiased UD-based XRE transfer, with which we achieve significant XRE performance gains.",
  "keywords": [
    "cross",
    "work",
    "code",
    "forest",
    "language",
    "unbiased",
    "extraction",
    "the universal dependency",
    "ependency",
    "unified",
    "an unbiased ud-",
    "unbiased ud-based xre transfer",
    "dependency",
    "we",
    "transfer"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.599/",
  "provenance": {
    "collected_at": "2025-06-05 10:16:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}