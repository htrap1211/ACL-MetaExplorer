{
  "id": "2024.findings-acl.95",
  "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
  "authors": [
    "Ding, Ruomeng  and\nZhang, Chaoyun  and\nWang, Lu  and\nXu, Yong  and\nMa, Minghua  and\nZhang, Wei  and\nQin, Si  and\nRajmohan, Saravan  and\nLin, Qingwei  and\nZhang, Dongmei"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "This paper introduce a novel thought prompting approach called ”Everything of Thoughts” (XoT) for Large Language Models (LLMs) to defy the law of ”Penrose triangle” of existing thought paradigms, to achieve three key perspectives in thought generation simultaneously: performance, efficiency, and flexibility. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge and planning capability into thoughts, thereby enhancing LLMs’ decision-making capabilities. Through the MCTS-LLM collaborative thought revision framework, XoT autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to utilize flexible cognitive mappings for solving problems with multiple solutions.We evaluate XoT on several challenging problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches in various dimensions, showcasing its remarkable proficiency in addressing complex problems across diverse domains. The data and code are available at https://github.com/microsoft/Everything-of-Thoughts-XoT.",
  "keywords": [
    "code",
    "decision-making capabilities",
    "efficiency",
    "we",
    "llm",
    "thought generation",
    "learning",
    "minimal llm interactions",
    "llms",
    "prompting approach",
    "proficiency",
    "its remarkable proficiency",
    "reinforcement",
    "knowledge",
    "generation"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.95/",
  "provenance": {
    "collected_at": "2025-06-05 10:49:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}