{
  "id": "2024.arabicnlp-1.89",
  "title": "T}eam{\\_}{Z}ero at {S}tance{E}val2024: Frozen {PLM}s for {A}rabic Stance Detection",
  "authors": [
    "Galal, Omar  and\nKaseb, Abdelrahman"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "This research explores the effectiveness of using pre-trained language models (PLMs) as feature extractors for Arabic stance detection on social media, focusing on topics like women empowerment, COVID-19 vaccination, and digital transformation. By leveraging sentence transformers to extract embeddings and incorporating aggregation architectures on top of BERT, we aim to achieve high performance without the computational expense of fine-tuning. Our approach demonstrates significant resource and time savings while maintaining competitive performance, scoring an F1-score of 78.62 on the test set. This study highlights the potential of PLMs in enhancing stance detection in Arabic social media analysis, offering a resource-efficient alternative to traditional fine-tuning methods.",
  "keywords": [
    "t",
    "top",
    "transformers",
    "efficient",
    "traditional fine-tuning methods",
    "we",
    "pre-trained language models plms",
    "analysis",
    "tuning",
    "an f1-score",
    "bert",
    "ero",
    "fine",
    "a resource-efficient alternative",
    "embeddings"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.89/",
  "provenance": {
    "collected_at": "2025-06-05 11:03:23",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}