{
  "id": "2021.acl-long.20",
  "title": "Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction",
  "authors": [
    "Cui, Li  and\nYang, Deqing  and\nYu, Jiaxin  and\nHu, Chengwei  and\nCheng, Jiayang  and\nYi, Jingjie  and\nXiao, Yanghua"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many real-world applications. As a typical task of continual learning, continual relation extraction (CRE) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously. Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them. However, most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks. To fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation. Specifically, the prototype embedding for a specific relation is computed based on memorized samples of this relation, which is collected by K-means algorithm. The prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings, which ensures the modelâ€™s stable understanding on all observed relations when learning a new task. Compared with previous CRE models, our model utilizes the memory information sufficiently and efficiently, resulting in enhanced CRE performance. Our experiments show that the proposed model outperforms the state-of-the-art CRE models and has great advantage in avoiding catastrophic forgetting. The code and datasets are released onhttps://github.com/fd2014cl/RP-CRE.",
  "keywords": [
    "code",
    "extraction",
    "efficiency",
    "we",
    "current",
    "increasing attention",
    "information",
    "learning",
    "subsequent sample embeddings",
    "sample embeddings",
    "embeddings",
    "model",
    "entities",
    "attention",
    "network"
  ],
  "url": "https://aclanthology.org/2021.acl-long.20/",
  "provenance": {
    "collected_at": "2025-06-05 07:59:42",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}