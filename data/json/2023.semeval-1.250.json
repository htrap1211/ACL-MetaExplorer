{
  "id": "2023.semeval-1.250",
  "title": "SKAM} at {S}em{E}val-2023 Task 10: Linguistic Feature Integration and Continuous Pretraining for Online Sexism Detection and Classification",
  "authors": [
    "Kondragunta, Murali Manohar  and\nChen, Amber  and\nSlot, Karlo  and\nWeering, Sanne  and\nCaselli, Tommaso"
  ],
  "year": "2023",
  "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
  "abstract": "Sexism has been prevalent online. In this paper, we explored the effect of explicit linguistic features and continuous pretraining on the performance of pretrained language models in sexism detection. While adding linguistic features did not improve the performance of the model, continuous pretraining did slightly boost the performance of the model in Task B from a mean macro-F1 score of 0.6156 to 0.6246. The best mean macro-F1 score in Task A was achieved by a finetuned HateBERT model using regular pretraining (0.8331). We observed that the linguistic features did not improve the modelâ€™s performance. At the same time, continuous pretraining proved beneficial only for nuanced downstream tasks like Task-B.",
  "keywords": [
    "language",
    "model",
    "pretrained language models",
    "regular",
    "we",
    "mean",
    "a",
    "hatebert",
    "classification",
    "time",
    "a finetuned hatebert model",
    "a mean macro-f1 score",
    "pretraining",
    "task-b",
    "sexism"
  ],
  "url": "https://aclanthology.org/2023.semeval-1.250/",
  "provenance": {
    "collected_at": "2025-06-05 10:30:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}