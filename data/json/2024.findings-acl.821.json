{
  "id": "2024.findings-acl.821",
  "title": "Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model",
  "authors": [
    "Zhang, Hongbin  and\nChen, Kehai  and\nBai, Xuefeng  and\nXiang, Yang  and\nZhang, Min"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Large language models (LLMs) have showcased their remarkable capabilities to handle various downstream tasks, including multilingual machine translation ability. Despite their impressive performance, decoder-only LLMs lack an explicit alignment between source and target contexts, leading to translation that may not faithfully represent the original content. To address this, we propose three learning strategies to encourage LLMs to pay more attention to the source context during translation: 1) adjusting attention weights on the source context by adaptive attention re-weighting; 2) suppressing the irrelevant target prefix using contrastive decoding; 3) avoiding excessive reliance on the target prefix through target-constrained tuning. To verify the effectiveness of our model, we curate a new dataset specifically focusing on unfaithful translations generated by LLMs. Experimental results on both human-collected and general test sets verify the effectiveness of our model across multiple language pairs. Further human evaluation demonstrates the efficacy of our method in reducing hallucinatory translation and improving the fidelity of translations.",
  "keywords": [
    "decoder-only llms",
    "large language model",
    "three learning strategies",
    "attention weights",
    "we",
    "hallucinatory translation",
    "translation",
    "further human evaluation",
    "adaptive attention",
    "decoder",
    "an explicit alignment",
    "their remarkable capabilities",
    "more attention",
    "llms",
    "tuning"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.821/",
  "provenance": {
    "collected_at": "2025-06-05 10:59:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}