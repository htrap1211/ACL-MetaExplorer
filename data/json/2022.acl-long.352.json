{
  "id": "2022.acl-long.352",
  "title": "KNN}-Contrastive Learning for Out-of-Domain Intent Classification",
  "authors": [
    "Zhou, Yunhua  and\nLiu, Peiju  and\nQiu, Xipeng"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The Out-of-Domain (OOD) intent classification is a basic and challenging task for dialogue systems. Previous methods commonly restrict the region (in feature space) of In-domain (IND) intent features to be compact or simply-connected implicitly, which assumes no OOD intents reside, to learn discriminative semantic features. Then the distribution of the IND intent features is often assumed to obey a hypothetical distribution (Gaussian mostly) and samples outside this distribution are regarded as OOD samples. In this paper, we start from the nature of OOD intent classification and explore its optimization objective. We further propose a simple yet effective method, named KNN-contrastive learning. Our approach utilizes k-nearest neighbors (KNN) of IND intents to learn discriminative semantic features that are more conducive to OOD detection. Notably, the density-based novelty detection algorithm is so well-grounded in the essence of our method that it is reasonable to use it as the OOD detection algorithm without making any requirements for the feature distribution. Extensive experiments on four public datasets show that our approach can not only enhance the OOD detection performance substantially but also improve the IND intent classification while requiring no restrictions on feature distribution.",
  "keywords": [
    "the ind intent classification",
    "semantic",
    "we",
    "dialogue",
    "discriminative semantic features",
    "classification",
    "it",
    "dialogue systems",
    "its optimization",
    "learning",
    "objective",
    "optimization",
    "the density-based novelty detection",
    "approach",
    "essence"
  ],
  "url": "https://aclanthology.org/2022.acl-long.352/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}