{
  "id": "2021.acl-long.48",
  "title": "COSY}: {CO}unterfactual {SY}ntax for Cross-Lingual Understanding",
  "authors": [
    "Yu, Sicheng  and\nZhang, Hao  and\nNiu, Yulei  and\nSun, Qianru  and\nJiang, Jing"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance. However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task. We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages. Our approach, called COunterfactual SYntax (COSY), includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones. Our results show that COSY achieves the state-of-the-art performance for both tasks, without using auxiliary training data.",
  "keywords": [
    "mbert",
    "not only the semantics",
    "question",
    "we",
    "syntax",
    "shot",
    "training",
    "mbert and xlm-r",
    "cross",
    "natural",
    "syntax-aware networks",
    "semantics",
    "e g multilingual-bert",
    "information",
    "the syntax"
  ],
  "url": "https://aclanthology.org/2021.acl-long.48/",
  "provenance": {
    "collected_at": "2025-06-05 08:00:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}