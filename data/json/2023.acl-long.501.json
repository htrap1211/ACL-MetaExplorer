{
  "id": "2023.acl-long.501",
  "title": "F}orm{N}et{V}2: Multimodal Graph Contrastive Learning for Form Document Information Extraction",
  "authors": [
    "Lee, Chen-Yu  and\nLi, Chun-Liang  and\nZhang, Hao  and\nDozat, Timothy  and\nPerot, Vincent  and\nSu, Guolong  and\nZhang, Xiang  and\nSohn, Kihyuk  and\nGlushnev, Nikolay  and\nWang, Renshen  and\nAinslie, Joshua  and\nLong, Shangbang  and\nQin, Siyang  and\nFujii, Yasuhisa  and\nHua, Nan  and\nPfister, Tomas"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with a more compact model size.",
  "keywords": [
    "form document information extraction",
    "all modalities",
    "n",
    "extraction",
    "form",
    "we",
    "self-supervised pre-training techniques",
    "graph",
    "other modalities",
    "training",
    "edge",
    "natural",
    "self",
    "loss",
    "information"
  ],
  "url": "https://aclanthology.org/2023.acl-long.501/",
  "provenance": {
    "collected_at": "2025-06-05 09:42:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}