{
  "id": "2022.wassa-1.8",
  "title": "Uncertainty Regularized Multi-Task Learning",
  "authors": [
    "Meshgi, Kourosh  and\nMirzaei, Maryam Sadat  and\nSekine, Satoshi"
  ],
  "year": "2022",
  "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\\&} Social Media Analysis",
  "abstract": "By sharing parameters and providing task-independent shared features, multi-task deep neural networks are considered one of the most interesting ways for parallel learning from different tasks and domains. However, fine-tuning on one task may compromise the performance of other tasks or restrict the generalization of the shared learned features. To address this issue, we propose to use task uncertainty to gauge the effect of the shared feature changes on other tasks and prevent the model from overfitting or over-generalizing. We conducted an experiment on 16 text classification tasks, and findings showed that the proposed method consistently improves the performance of the baseline, facilitates the knowledge transfer of learned features to unseen data, and provides explicit control over the generalization of the shared model.",
  "keywords": [
    "deep",
    "tuning",
    "knowledge",
    "neural",
    "16 text classification tasks",
    "model",
    "text",
    "the generalization",
    "deep neural networks",
    "generalization",
    "we",
    "learning",
    "transfer",
    "over-generalizing",
    "generalizing"
  ],
  "url": "https://aclanthology.org/2022.wassa-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 08:46:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}