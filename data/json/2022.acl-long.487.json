{
  "id": "2022.acl-long.487",
  "title": "Integrating Vectorized Lexical Constraints for Neural Machine Translation",
  "authors": [
    "Wang, Shuo  and\nTan, Zhixing  and\nLiu, Yang"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Lexically constrained neural machine translation (NMT), which controls the generation of NMT models with pre-specified constraints, is important in many practical scenarios. Due to the representation gap between discrete constraints and continuous vectors in NMT models, most existing works choose to construct synthetic data or modify the decoding algorithm to impose lexical constraints, treating the NMT model as a black box. In this work, we propose to open this black box by directly integrating the constraints into NMT models. Specifically, we vectorize source and target constraints into continuous keys and values, which can be utilized by the attention modules of NMT models. The proposed integration method is based on the assumption that the correspondence between keys and values in attention modules is naturally suitable for modeling constraint pairs. Experimental results show that our method consistently outperforms several representative baselines on four language pairs, demonstrating the superiority of integrating vectorized lexical constraints.",
  "keywords": [
    "the generation",
    "we",
    "translation",
    "neural",
    "attention modules",
    "vectorized lexical constraints",
    "work",
    "pre-specified constraints",
    "specified",
    "generation",
    "language",
    "model",
    "machine",
    "vectors",
    "modeling"
  ],
  "url": "https://aclanthology.org/2022.acl-long.487/",
  "provenance": {
    "collected_at": "2025-06-05 08:30:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}