{
  "id": "2023.findings-acl.125",
  "title": "Incorporating Graph Information in Transformer-based {AMR} Parsing",
  "authors": [
    "Vasylenko, Pavlo  and\nHuguet Cabot, Pere Llu{\\'i}s  and\nMart{\\'i}nez Lorenzo, Abelardo Carlos  and\nNavigli, Roberto"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at [http://www.github.com/sapienzanlp/LeakDistill](http://www.github.com/sapienzanlp/LeakDistill).",
  "keywords": [
    "code",
    "parsing",
    "embed",
    "semantic",
    "we",
    "graph",
    "current",
    "word-to-node alignment",
    "training",
    "the encoder",
    "self",
    "a semantic graph abstraction",
    "information",
    "word",
    "text"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.125/",
  "provenance": {
    "collected_at": "2025-06-05 09:54:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}