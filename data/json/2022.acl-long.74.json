{
  "id": "2022.acl-long.74",
  "title": "Semi-supervised Domain Adaptation for Dependency Parsing with Dynamic Matching Network",
  "authors": [
    "Li, Ying  and\nLi, Shuaike  and\nZhang, Min"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Supervised parsing models have achieved impressive results on in-domain texts. However, their performances drop drastically on out-of-domain texts due to the data distribution shift. The shared-private model has shown its promising advantages for alleviating this problem via feature separation, whereas prior works pay more attention to enhance shared features but neglect the in-depth relevance of specific ones. To address this issue, we for the first time apply a dynamic matching network on the shared-private model for semi-supervised cross-domain dependency parsing. Meanwhile, considering the scarcity of target-domain labeled data, we leverage unlabeled data from two aspects, i.e., designing a new training strategy to improve the capability of the dynamic matching network and fine-tuning BERT to obtain domain-related contextualized representations. Experiments on benchmark datasets show that our proposed model consistently outperforms various baselines, leading to new state-of-the-art results on all domains. Detailed analysis on different matching strategies demonstrates that it is essential to learn suitable matching weights to emphasize useful features and ignore useless or even harmful ones. Besides, our proposed model can be directly extended to multi-source domain adaptation and achieves best performances among various baselines, further verifying the effectiveness and robustness.",
  "keywords": [
    "parsing",
    "we",
    "training",
    "cross",
    "semi-supervised cross-domain dependency parsing",
    "it",
    "fine-tuning bert",
    "more attention",
    "analysis",
    "i",
    "bert",
    "strategies",
    "different matching strategies",
    "model",
    "dependency"
  ],
  "url": "https://aclanthology.org/2022.acl-long.74/",
  "provenance": {
    "collected_at": "2025-06-05 08:25:18",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}