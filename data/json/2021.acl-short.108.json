{
  "id": "2021.acl-short.108",
  "title": "Robust Transfer Learning with Pretrained Language Models through Adapters",
  "authors": [
    "Han, Wenjuan  and\nPang, Bo  and\nWu, Ying Nian"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.",
  "keywords": [
    "transformer",
    "language",
    "random",
    "nlp",
    "bert",
    "model",
    "fine-tuning iterations",
    "it",
    "pretrained language models",
    "layer",
    "most nlp tasks",
    "we",
    "transfer",
    "sequence",
    "vulnerable"
  ],
  "url": "https://aclanthology.org/2021.acl-short.108/",
  "provenance": {
    "collected_at": "2025-06-05 08:08:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}