{
  "id": "2022.acl-long.66",
  "title": "Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues",
  "authors": [
    "Dong, Qingxiu  and\nQin, Ziwei  and\nXia, Heming  and\nFeng, Tian  and\nTong, Shoujie  and\nMeng, Haoran  and\nXu, Lin  and\nWei, Zhongyu  and\nZhan, Weidong  and\nChang, Baobao  and\nLi, Sujian  and\nLiu, Tianyu  and\nSui, Zhifang"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "It is a common practice for recent works in vision language cross-modal reasoning to adopt a binary or multi-choice classification formulation taking as input a set of source image(s) and textual query. In this work, we take a sober look at such an “unconditional” formulation in the sense that no prior knowledge is specified with respect to the source image(s). Inspired by the designs of both visual commonsense reasoning and natural language inference tasks, we propose a new task termed “Premise-based Multi-modal Reasoning” (PMR) where a textual premise is the background presumption on each source image. The PMR dataset contains 15,360 manually annotated samples which are created by a multi-phase crowd-sourcing process. With selected high-quality movie screenshots and human-curated premise templates from 6 pre-defined categories, we ask crowd-source workers to write one true hypothesis and three distractors (4 choices) given the premise and image through a cross-check procedure.",
  "keywords": [
    "we",
    "classification",
    "cross",
    "natural",
    "it",
    "visual",
    "background",
    "categories",
    "work",
    "process",
    "knowledge",
    "language",
    "movie",
    "human",
    "the background presumption"
  ],
  "url": "https://aclanthology.org/2022.acl-long.66/",
  "provenance": {
    "collected_at": "2025-06-05 08:25:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}