{
  "id": "2024.acl-srw.48",
  "title": "C}heckers{GPT}: Learning World Models through Language Modeling",
  "authors": [
    "Joshi, Abhinav  and\nSharma, Vaibhav  and\nModi, Ashutosh"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
  "abstract": "Although Large Language Models (LLMs) have been trained using just the next token prediction objective, these have shown impressive performance on various tasks. Consequently, it has attracted research interests in this regard. While one line of work in the past has suggested that LLMs learn surface-level statistics from the dataset, another line of work emphasizes that the learned representations are effective for simulating the underlying world model, considering the causal relationship for the next token prediction. This phenomenon is often referred to as the emergence of a world model in sequence prediction tasks. Recent work has demonstrated this phenomenon in a simulated setting of board games like Othello and Chess. In this paper, we analyze the game of Checkers to find out the emergence of a world model in a language model. By training a GPT-style autoregressive language model using only the next character prediction objective, we find that the model does show a hint of learning a world model representation of the board positions. We perform our analysis on two datasets: 1) synthetic dataset, which comes from the checkers game tree, and 2) human gameplay dataset. With multiple models trained with different layer sizes, we find that increasing the parameter size does help learn better world model representation decoded by linear probes.",
  "keywords": [
    "layer",
    "we",
    "language modeling",
    "parameter",
    "it",
    "token",
    "sequence",
    "a language model",
    "analysis",
    "llms",
    "objective",
    "large language models llms",
    "work",
    "language",
    "model"
  ],
  "url": "https://aclanthology.org/2024.acl-srw.48/",
  "provenance": {
    "collected_at": "2025-06-05 10:48:18",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}