{
  "id": "2023.clinicalnlp-1.8",
  "title": "Navigating Data Scarcity: Pretraining for Medical Utterance Classification",
  "authors": [
    "Min, Do June  and\nPerez-Rosas, Veronica  and\nMihalcea, Rada"
  ],
  "year": "2023",
  "venue": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
  "abstract": "Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data management. In this paper, we study the effectiveness of dialog-aware pretraining objectives and multiphase training in using unlabeled data to improve LMs training for medical utterance classification. The objectives of pretraining for dialog awareness involve tasks that take into account the structure of conversations, including features such as turn-taking and the roles of speakers. The multiphase training process uses unannotated data in a sequence that prioritizes similarities and connections between different domains. We empirically evaluate these methods on conversational dialog classification tasks in the medical and counseling domains, and find that multiphase training can help achieve higher performance than standard pretraining or finetuning.",
  "keywords": [
    "objectives",
    "dialog-aware pretraining objectives",
    "conversations",
    "we",
    "training",
    "classification",
    "self",
    "learning",
    "sequence",
    "medical conversations",
    "the objectives",
    "text",
    "process",
    "similarities",
    "language"
  ],
  "url": "https://aclanthology.org/2023.clinicalnlp-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 10:23:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}