{
  "id": "2023.acl-long.388",
  "title": "Soft Language Clustering for Multilingual Model Pre-training",
  "authors": [
    "Zeng, Jiali  and\nJiang, Yufan  and\nYin, Yongjing  and\nJing, Yi  and\nMeng, Fandong  and\nLin, Binghuai  and\nCao, Yunbo  and\nZhou, Jie"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typologyfrom the source language or when pre-training data is limited in size. In this paper, we propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME, which include text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer.",
  "keywords": [
    "efficient",
    "sentence retrieval",
    "question",
    "we",
    "pre-training data",
    "large-size language models",
    "training",
    "classification",
    "cross",
    "it",
    "unsupervised sentence retrieval",
    "retrieval",
    "sequence",
    "transfer",
    "abilities"
  ],
  "url": "https://aclanthology.org/2023.acl-long.388/",
  "provenance": {
    "collected_at": "2025-06-05 09:40:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}