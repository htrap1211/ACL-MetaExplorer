{
  "id": "2024.cmcl-1.7",
  "title": "Do {LLM}s Agree with Humans on Emotional Associations to Nonsense Words?",
  "authors": [
    "Miyakawa, Yui  and\nMatsuhira, Chihaya  and\nKato, Hirotaka  and\nHirayama, Takatsugu  and\nKomamizu, Takahiro  and\nIde, Ichiro"
  ],
  "year": "2024",
  "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
  "abstract": "Understanding human perception of nonsense words is helpful to devise product and character names that match their characteristics. Previous studies have suggested the usefulness of Large Language Models (LLMs) for estimating such human perception, but they did not focus on its emotional aspects. Hence, this study aims to elucidate the relationship of emotions evoked by nonsense words between humans and LLMs. Using a representative LLM, GPT-4, we reproduce the procedure of an existing study to analyze evoked emotions of humans for nonsense words. A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans. Although the correlation is not very high, this demonstrates that GPT-4 may agree with humans on emotional associations to nonsense words. Considering that the previous study reported that the correlation among human annotators was about 0.68 on average and that between a regression model trained on the annotations for real words and humans was 0.17, GPT-4â€™s agreement with humans is notably strong.",
  "keywords": [
    "previous studies",
    "language",
    "model",
    "studies",
    "human",
    "large language models",
    "a representative llm gpt-4",
    "gpt-4",
    "we",
    "llm",
    "llms",
    "that",
    "intensity",
    "helpful",
    "usefulness"
  ],
  "url": "https://aclanthology.org/2024.cmcl-1.7/",
  "provenance": {
    "collected_at": "2025-06-05 11:05:32",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}