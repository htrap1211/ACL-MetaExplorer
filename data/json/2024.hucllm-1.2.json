{
  "id": "2024.hucllm-1.2",
  "title": "Human-Centered Design Recommendations for {LLM}-as-a-judge",
  "authors": [
    "Pan, Qian  and\nAshktorab, Zahra  and\nDesmond, Michael  and\nSantill{\\'a}n Cooper, Mart{\\'i}n  and\nJohnson, James  and\nNair, Rahul  and\nDaly, Elizabeth  and\nGeyer, Werner"
  ],
  "year": "2024",
  "venue": "Proceedings of the 1st Human-Centered Large Language Modeling Workshop",
  "abstract": "Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human’s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners’ preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.",
  "keywords": [
    "bleu",
    "we",
    "interviews",
    "llm",
    "human evaluation",
    "it",
    "practitioners preferences",
    "llms",
    "text",
    "metrics",
    "large language models llms",
    "rouge",
    "practitioners",
    "work",
    "reference"
  ],
  "url": "https://aclanthology.org/2024.hucllm-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 11:06:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}