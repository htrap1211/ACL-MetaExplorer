{
  "id": "2024.wassa-1.26",
  "title": "Comparing Pre-trained Human Language Models: Is it Better with Human Context as Groups, Individual Traits, or Both?",
  "authors": [
    "Soni, Nikita  and\nBalasubramanian, Niranjan  and\nSchwartz, H. Andrew  and\nHovy, Dirk"
  ],
  "year": "2024",
  "venue": "Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis",
  "abstract": "Pre-trained language models consider the context of neighboring words and documents but lack any author context of the human generating the text. However, language depends on the author’s states, traits, social, situational, and environmental attributes, collectively referred to as human context (Soni et al., 2024). Human-centered natural language processing requires incorporating human context into language models. Currently, two methods exist: pre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2) individual traits. Group attributes are simple but coarse — not all 45-year-olds write the same way — while individual traits allow for more personalized representations, but require more complex modeling and data. It is unclear which approach benefits what tasks. We compare pre-training models with human context via 1) group attributes, 2) individual users, and 3) a combined approach on five user- and document-level tasks. Our results show that there is no best approach, but that human-centered language modeling holds avenues for different methods.",
  "keywords": [
    "both pre-trained language models",
    "processing",
    "language",
    "generating",
    "natural",
    "text",
    "it",
    "human",
    "language models",
    "modeling",
    "pre-trained human language models",
    "pre-training models",
    "we",
    "human-centered language modeling",
    "pre"
  ],
  "url": "https://aclanthology.org/2024.wassa-1.26/",
  "provenance": {
    "collected_at": "2025-06-05 11:11:56",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}