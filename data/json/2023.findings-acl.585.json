{
  "id": "2023.findings-acl.585",
  "title": "A Multi-task Learning Framework for Quality Estimation",
  "authors": [
    "Deoghare, Sourabh  and\nChoudhary, Paramveer  and\nKanojia, Diptesh  and\nRanasinghe, Tharindu  and\nBhattacharyya, Pushpak  and\nOr{\\u{a}}san, Constantin"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Quality Estimation (QE) is the task of evaluating machine translation output in the absence of reference translation. Conventional approaches to QE involve training separate models at different levels of granularity viz., word-level, sentence-level, and document-level, which sometimes lead to inconsistent predictions for the same input. To overcome this limitation, we focus on jointly training a single model for sentence-level and word-level QE tasks in a multi-task learning framework. Using two multi-task learning-based QE approaches, we show that multi-task learning improves the performance of both tasks. We evaluate these approaches by performing experiments in different settings, viz., single-pair, multi-pair, and zero-shot. We compare the multi-task learning-based approach with baseline QE models trained on single tasks and observe an improvement of up to 4.28% in Pearsonâ€™s correlation (r) at sentence-level and 8.46% in F1-score at word-level, in the single-pair setting. In the multi-pair setting, we observe improvements of up to 3.04% at sentence-level and 13.74% at word-level; while in the zero-shot setting, we also observe improvements of up to 5.26% and 3.05%, respectively. We make the models proposed in this paper publically available.",
  "keywords": [
    "reference translation conventional approaches",
    "we",
    "shot",
    "translation",
    "f1-score",
    "machine translation output",
    "word",
    "learning",
    "the zero-shot setting",
    "reference",
    "single-pair multi-pair and zero-shot",
    "model",
    "machine",
    "multi",
    "approach"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.585/",
  "provenance": {
    "collected_at": "2025-06-05 10:16:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}