{
  "id": "2023.bionlp-1.67",
  "title": "IITR} at {B}io{L}ay{S}umm Task 1:Lay Summarization of {B}io{M}edical articles using Transformers",
  "authors": [
    "Reddy, Venkat praneeth  and\nReddy, Pinnapu Reddy Harshavardhan  and\nSumedh, Karanam Sai  and\nSharma, Raksha"
  ],
  "year": "2023",
  "venue": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
  "abstract": "Initially, we analyzed the datasets in a statistical way so as to learn about various sectionsâ€™ contributions to the final summary in both the pros and life datasets. We found that both the datasets have an Introduction and Abstract along with some initial parts of the results contributing to the summary. We considered only these sections in the next stage of analysis. We found the optimal length or no of sentences of each of the Introduction, abstract, and result which contributes best to the summary. After this statistical analysis, we took the pre-trained model Facebook/bart-base and fine-tuned it with both the datasets PLOS and eLife. While fine-tuning and testing the results we have used chunking because the text lengths are huge. So to not lose information due to the number of token constraints of the model, we used chunking. Finally, we saw the eLife model giving more accurate results than PLOS in terms of readability aspect, probably because the PLOS summary is closer to its abstract, we have considered the eLife model as our final model and tuned the hyperparameters. We are ranked 7th overall and 1st in readability",
  "keywords": [
    "transformers",
    "summarization",
    "we",
    "hyperparameters",
    "it",
    "token",
    "information",
    "no",
    "analysis",
    "tuning",
    "text",
    "umm",
    "fine",
    "model",
    "pre"
  ],
  "url": "https://aclanthology.org/2023.bionlp-1.67/",
  "provenance": {
    "collected_at": "2025-06-05 10:23:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}