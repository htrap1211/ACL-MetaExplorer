{
  "id": "2024.acl-demos.37",
  "title": "LLMB}ox: A Comprehensive Library for Large Language Models",
  "authors": [
    "Tang, Tianyi  and\nYiwen, Hu  and\nLi, Bingqian  and\nLuo, Wenyang  and\nQin, ZiJing  and\nSun, Haoxiang  and\nWang, Jiapeng  and\nXu, Shiyi  and\nCheng, Xiaoxue  and\nGuo, Geyang  and\nPeng, Han  and\nZheng, Bowen  and\nTang, Yiru  and\nMin, Yingqian  and\nChen, Yushuo  and\nChen, Jie  and\nZhao, Ranchi  and\nDing, Luran  and\nWang, Yuhao  and\nDong, Zican  and\nChunxuan, Xia  and\nLi, Junyi  and\nZhou, Kun  and\nZhao, Xin  and\nWen, Ji-Rong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
  "abstract": "To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits: (1) a unified data interface that supports the flexible implementation of various training strategies, (2) a comprehensive evaluation that covers extensive tasks, datasets, and models, and (3) more practical consideration, especially on user-friendliness and efficiency. With our library, users can easily reproduce existing methods, train new models, and conduct comprehensive performance comparisons. To rigorously test LLMBox, we conduct extensive experiments in a diverse coverage of evaluation settings, and experimental results demonstrate the effectiveness and efficiency of our library in supporting various implementations related to LLMs. The detailed introduction and usage guidance can be found athttps://github.com/RUCAIBox/LLMBox.",
  "keywords": [
    "a unified data interface",
    "efficiency",
    "we",
    "training",
    "llmbox",
    "unified",
    "user-friendliness",
    "a comprehensive evaluation",
    "llmb ox",
    "llms",
    "strategies",
    "evaluation settings",
    "various training strategies",
    "language",
    "llmb"
  ],
  "url": "https://aclanthology.org/2024.acl-demos.37/",
  "provenance": {
    "collected_at": "2025-06-05 10:47:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}