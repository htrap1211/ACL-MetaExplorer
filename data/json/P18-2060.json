{
  "id": "P18-2060",
  "title": "Neural Hidden {M}arkov Model for Machine Translation",
  "authors": [
    "Wang, Weiyue  and\nZhu, Derui  and\nAlkhouli, Tamer  and\nGan, Zixuan  and\nNey, Hermann"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a translation, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates NMT while replacing the attention component. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the forward-backward algorithm. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 German↔English and Chinese→English translation tasks.",
  "keywords": [
    "work",
    "alignment",
    "neural",
    "model",
    "machine",
    "encoder",
    "decoder",
    "neural network-based alignment",
    "attention",
    "network",
    "sequence",
    "we",
    "a translation",
    "the attention component",
    "translation"
  ],
  "url": "https://aclanthology.org/P18-2060/",
  "provenance": {
    "collected_at": "2025-06-05 00:17:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}