{
  "id": "2022.wassa-1.14",
  "title": "Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning",
  "authors": [
    "Zhang, Chiyu  and\nAbdul-Mageed, Muhammad"
  ],
  "year": "2022",
  "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\\&} Social Media Analysis",
  "abstract": "Masked language models (MLMs) are pre-trained with a denoising objective that is in a mismatch with the objective of downstream fine-tuning. We propose pragmatic masking and surrogate fine-tuning as two complementing strategies that exploit social cues to drive pre-trained representations toward a broad set of concepts useful for a wide class of social meaning tasks. We test our models on 15 different Twitter datasets for social meaning detection. Our methods achieve 2.34%F1over a competitive baseline, while outperforming domain-specific language models pre-trained on large datasets. Our methods also excel in few-shot learning: with only 5% of training data (severely few-shot), our methods enable an impressive 68.54% averageF1. The methods are also language agnostic, as we show in a zero-shot setting involving six datasets from three different languages.",
  "keywords": [
    "a denoising objective",
    "tuning",
    "surrogate fine-tuning",
    "language",
    "averagef1",
    "objective",
    "class",
    "the objective",
    "few-shot learning",
    "strategies",
    "fine",
    "we",
    "learning",
    "domain-specific language models",
    "pre"
  ],
  "url": "https://aclanthology.org/2022.wassa-1.14/",
  "provenance": {
    "collected_at": "2025-06-05 08:46:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}