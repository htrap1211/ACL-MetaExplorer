{
  "id": "2023.acl-long.421",
  "title": "C}on{FEDE}: Contrastive Feature Decomposition for Multimodal Sentiment Analysis",
  "authors": [
    "Yang, Jiuding  and\nYu, Yakun  and\nNiu, Di  and\nGuo, Weidong  and\nXu, Yu"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Multimodal Sentiment Analysis aims to predict the sentiment of video content. Recent research suggests that multimodal sentiment analysis critically depends on learning a good representation of multimodal information, which should contain both modality-invariant representations that are consistent across modalities as well as modality-specific representations. In this paper, we propose ConFEDE, a unified learning framework that jointly performs contrastive representation learning and contrastive feature decomposition to enhance the representation of multimodal information. It decomposes each of the three modalities of a video sample, including text, video frames, and audio, into a similarity feature and a dissimilarity feature, which are learned by a contrastive relation centered around the text. We conducted extensive experiments on CH-SIMS, MOSI and MOSEI to evaluate various state-of-the-art multimodal sentiment analysis methods. Experimental results show that ConFEDE outperforms all baselines on these datasets on a range of metrics.",
  "keywords": [
    "we",
    "a unified learning framework",
    "it",
    "the three modalities",
    "c",
    "unified",
    "information",
    "learning",
    "multimodal sentiment analysis",
    "analysis",
    "text",
    "metrics",
    "sentiment",
    "modalities",
    "a contrastive relation"
  ],
  "url": "https://aclanthology.org/2023.acl-long.421/",
  "provenance": {
    "collected_at": "2025-06-05 09:41:07",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}