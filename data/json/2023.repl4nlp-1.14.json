{
  "id": "2023.repl4nlp-1.14",
  "title": "Contrastive Loss is All You Need to Recover Analogies as Parallel Lines",
  "authors": [
    "Ri, Narutatsu  and\nLee, Fei-Tzin  and\nVerma, Nakul"
  ],
  "year": "2023",
  "venue": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
  "abstract": "While static word embedding models are known to represent linguistic analogies as parallel lines in high-dimensional space, the underlying mechanism as to why they result in such geometric structures remains obscure. We find that an elementary contrastive-style method employed over distributional information performs competitively with popular word embedding models on analogy recovery tasks, while achieving dramatic speedups in training time. Further, we demonstrate that a contrastive loss is sufficient to create these parallel structures in word embeddings, and establish a precise relationship between the co-occurrence statistics and the geometric structure of the resulting word embeddings.",
  "keywords": [
    "embeddings",
    "linguistic analogies",
    "loss",
    "information",
    "sufficient",
    "dimensional",
    "all",
    "analogies",
    "we",
    "the resulting word embeddings",
    "word",
    "word embeddings",
    "time",
    "training",
    "static word embedding models"
  ],
  "url": "https://aclanthology.org/2023.repl4nlp-1.14/",
  "provenance": {
    "collected_at": "2025-06-05 10:26:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}