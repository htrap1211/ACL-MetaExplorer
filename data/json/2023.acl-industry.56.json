{
  "id": "2023.acl-industry.56",
  "title": "Towards Building a Robust Toxicity Predictor",
  "authors": [
    "Bespalov, Dmitriy  and\nBhabesh, Sourav  and\nXiang, Yi  and\nZhou, Liutong  and\nQi, Yanjun"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
  "abstract": "Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. \\texttt{ToxicTrap} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow \\texttt{ToxicTrap} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98\\% attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.",
  "keywords": [
    "fast and effective generation",
    "classifiers",
    "little attention",
    "we",
    "sota text classifiers",
    "training",
    "recent nlp literature",
    "word",
    "vulnerable",
    "greedy based search strategies",
    "text",
    "strategies",
    "fast",
    "function",
    "language"
  ],
  "url": "https://aclanthology.org/2023.acl-industry.56/",
  "provenance": {
    "collected_at": "2025-06-05 09:52:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}