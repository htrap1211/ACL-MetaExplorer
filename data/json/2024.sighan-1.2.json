{
  "id": "2024.sighan-1.2",
  "title": "T}ele{C}hat: An Open-source Billingual Large Language Model",
  "authors": [
    "Wang, Zihan  and\nLiuxz2@chinatelecom.cn, Liuxz2@chinatelecom.cn  and\nLiusx14@chinatelecom.cn, Liusx14@chinatelecom.cn  and\nYao, Yitong  and\nHuangyy121@chinatelecom.cn, Huangyy121@chinatelecom.cn  and\nMengxiang, Li  and\nHe, Zhongjiang  and\nLiyx25@chinatelecom.cn, Liyx25@chinatelecom.cn  and\nPulw@chinatelecom.cn, Pulw@chinatelecom.cn  and\nXuhn@chinatelecom.cn, Xuhn@chinatelecom.cn  and\nWang, Chao  and\nSong, Shuangyong"
  ],
  "year": "2024",
  "venue": "Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)",
  "abstract": "In this paper, we presentTeleChat, a collection of large language models (LLMs) with parameters of 7 billion and 12 billion. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, encompassing trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including general dialogue generation, language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves state-of-the-art performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat-7B and TeleChat-12B, along with code and a portion of our filtered high-quality pretraining data, to the public community.",
  "keywords": [
    "code",
    "question",
    "we",
    "dialogue",
    "llms",
    "tuning",
    "mathematics reasoning code generation",
    "general",
    "knowledge",
    "language",
    "generation",
    "model",
    "human",
    "large language models",
    "hat"
  ],
  "url": "https://aclanthology.org/2024.sighan-1.2/",
  "provenance": {
    "collected_at": "2025-06-05 11:10:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}