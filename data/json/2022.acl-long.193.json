{
  "id": "2022.acl-long.193",
  "title": "Contextual Representation Learning beyond Masked Language Modeling",
  "authors": [
    "Fu, Zhiyi  and\nZhou, Wangchunshu  and\nXu, Jingjing  and\nZhou, Hao  and\nLi, Lei"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Currently, masked language modeling (e.g., BERT) is the prime choice to learn contextualized representations. Due to the pervasiveness, it naturally raises an interesting question: how do masked language models (MLMs) learn contextual representations? In this work, we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs. To address these problems, we propose TACO, a simple yet effective representation learning approach to directly model global semantics. To be specific, TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations. Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over MLM.",
  "keywords": [
    "work",
    "embeddings",
    "global semantics",
    "language",
    "bert",
    "contextual semantics",
    "semantics",
    "it",
    "sampled embeddings",
    "masked language modeling",
    "language models",
    "the efficiency",
    "modeling",
    "question",
    "efficiency"
  ],
  "url": "https://aclanthology.org/2022.acl-long.193/",
  "provenance": {
    "collected_at": "2025-06-05 08:26:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}