{
  "id": "2023.wassa-1.8",
  "title": "Emotions in Spoken Language - Do we need acoustics?",
  "authors": [
    "Probol, Nadine  and\nMieskes, Margot"
  ],
  "year": "2023",
  "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis",
  "abstract": "Work on emotion detection is often focused on textual data from i.e. Social Media. If multimodal data (i.e. speech) is analysed, the focus again is often placed on the transcription. This paper takes a closer look at how crucial acoustic information actually is for the recognition of emotions from multimodal data. To this end we use the IEMOCAP data, which is one of the larger data sets that provides transcriptions, audio recordings and manual emotion categorization. We build models for emotion classification using text-only, acoustics-only and combining both modalities in order to examine the influence of the various modalities on the final categorization. Our results indicate that using text-only models outperform acoustics-only models. But combining text-only and acoustic-only models improves the results. Additionally, we perform a qualitative analysis and find that a range of misclassifications are due to factors not related to the model, but to the data such as, recording quality, a challenging classification task and misclassifications that are unsurprising for humans.",
  "keywords": [
    "end",
    "the various modalities",
    "we",
    "classification",
    "both modalities",
    "information",
    "analysis",
    "text",
    "a challenging classification task",
    "emotion classification",
    "the iemocap data",
    "work",
    "misclassifications",
    "language",
    "model"
  ],
  "url": "https://aclanthology.org/2023.wassa-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 10:33:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}