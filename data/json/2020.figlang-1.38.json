{
  "id": "2020.figlang-1.38",
  "title": "Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media",
  "authors": [
    "Dong, Xiangjue  and\nLi, Changmao  and\nChoi, Jinho D."
  ],
  "year": "2020",
  "venue": "Proceedings of the Second Workshop on Figurative Language Processing",
  "abstract": "We present a transformer-based sarcasm detection model that accounts for the context from the entire conversation thread for more robust predictions. Our model uses deep transformer layers to perform multi-head attentions among the target utterance and the relevant context in the thread. The context-aware models are evaluated on two datasets from social media, Twitter and Reddit, and show 3.1% and 7.0% improvements over their baselines. Our best models give the F1-scores of 79.0% and 75.0% for the Twitter and Reddit datasets respectively, becoming one of the highest performing systems among 36 participants in this shared task.",
  "keywords": [
    "deep",
    "transformer",
    "conversation threads",
    "model",
    "the f1-scores",
    "the entire conversation thread",
    "attentions",
    "transformer-based context-aware sarcasm detection",
    "deep transformer layers",
    "conversation",
    "we",
    "multi-head attentions",
    "that",
    "our best models",
    "predictions"
  ],
  "url": "https://aclanthology.org/2020.figlang-1.38/",
  "provenance": {
    "collected_at": "2025-06-05 07:56:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}