{
  "id": "2022.acl-long.486",
  "title": "STEMM}: Self-learning with Speech-text Manifold Mixup for Speech Translation",
  "authors": [
    "Fang, Qingkai  and\nYe, Rong  and\nLi, Lei  and\nFeng, Yang  and\nWang, Mingxuan"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "How to learn a better speech representation for end-to-end speech-to-text translation (ST) with limited labeled data? Existing techniques often attempt to transfer powerful machine translation (MT) capabilities to ST, but neglect the representation discrepancy across modalities. In this paper, we propose the Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy. Specifically, we mix up the representation sequences of different modalities, and take both unimodal speech sequences and multimodal mixed sequences as input to the translation model in parallel, and regularize their output predictions with a self-learning framework. Experiments on MuST-C speech translation benchmark and further analysis show that our method effectively alleviates the cross-modal representation discrepancy, and achieves significant improvements over a strong baseline on eight translation directions.",
  "keywords": [
    "must-c speech translation benchmark",
    "cross",
    "end",
    "powerful machine translation",
    "eight translation directions",
    "model",
    "machine",
    "text",
    "modalities",
    "self",
    "capabilities",
    "we",
    "speech translation",
    "the translation model",
    "different modalities"
  ],
  "url": "https://aclanthology.org/2022.acl-long.486/",
  "provenance": {
    "collected_at": "2025-06-05 08:30:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}