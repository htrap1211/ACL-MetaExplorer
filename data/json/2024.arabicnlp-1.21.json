{
  "id": "2024.arabicnlp-1.21",
  "title": "CATT}: Character-based {A}rabic Tashkeel Transformer",
  "authors": [
    "Alasmary, Faris  and\nZaafarani, Orjuwan  and\nGhannam, Ahmad"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the comprehension of Arabic text by removing ambiguity and minimizing the risk of misinterpretations caused by its absence.It plays a crucial role in improving Arabic text processing, particularly in applications such as text-to-speech and machine translation.This paper introduces a new approach to training ATD models.First, we finetuned two transformers, encoder-only and encoder-decoder, that were initialized from a pretrained character-based BERT.Then, we applied the Noisy-Student approach to boost the performance of the best model.We evaluated our models alongside 11 commercial and open-source models using two manually labeled benchmark datasets: WikiNews and our CATT dataset.Our findings show that our top model surpasses all evaluated models by relative Diacritic Error Rates (DERs) of 30.83% and 35.21% on WikiNews and CATT, respectively, achieving state-of-the-art in ATD.In addition, we show that our model outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36%.We open-source our CATT models and benchmark dataset for the research community .",
  "keywords": [
    "transformers",
    "we",
    "gpt-4-turbo",
    "training",
    "translation",
    "it",
    "two transformers",
    "decoder",
    "der",
    "bert",
    "text",
    "gpt-4",
    "transformer",
    "machine",
    "model"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.21/",
  "provenance": {
    "collected_at": "2025-06-05 11:02:29",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}