{
  "id": "2024.findings-acl.813",
  "title": "F}resh{LLM}s: Refreshing Large Language Models with Search Engine Augmentation",
  "authors": [
    "Vu, Tu  and\nIyyer, Mohit  and\nWang, Xuezhi  and\nConstant, Noah  and\nWei, Jerry  and\nWei, Jason  and\nTar, Chris  and\nSung, Yun-Hsuan  and\nZhou, Denny  and\nLe, Quoc  and\nLuong, Thang"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Since most large language models (LLMs) are trained once and never updated, they struggle to dynamically adapt to our ever-changing world. In this work, we present FreshQA, a dynamic QA benchmark that tests a model’s ability to answer questions that may require reasoning over up-to-date world knowledge. We develop a two-mode human evaluation procedure to measure both correctness and hallucination, which we use to benchmark both closed and open-source LLMs by collecting >50K human judgments. We observe that all LLMs struggle to answer questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. In response, we develop FreshPrompt, a few-shot prompting method that curates and organizes relevant information from a search engine into an LLM’s prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. To facilitate future work, we additionally develop FreshEval, a reliable autorater for quick evaluation and comparison on FreshQA. Our latest results with FreshEval suggest that open-source LLMs such as Mixtral (Jiang et al., 2024), when combined with FreshPrompt, are competitive with closed-source and commercial systems on search-augmented QA.",
  "keywords": [
    "f resh llm",
    "all llms",
    "we",
    "llm",
    "shot",
    "self",
    "refreshing large language models",
    "a few-shot prompting method",
    "search-augmented qa",
    "information",
    "an llm s prompt",
    "llms",
    "freshqa",
    "et",
    "prompt"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.813/",
  "provenance": {
    "collected_at": "2025-06-05 10:59:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}