{
  "id": "2024.bionlp-1.78",
  "title": "HGP}-{NLP} at {B}io{L}ay{S}umm: Leveraging {L}o{RA} for Lay Summarization of Biomedical Research Articles using {S}eq2{S}eq Transformers",
  "authors": [
    "Malik, Hemang  and\nPradeep, Gaurav  and\nSeth, Pratinav"
  ],
  "year": "2024",
  "venue": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
  "abstract": "Lay summarization aims to generate summaries of technical articles for non-experts, enabling easy comprehension for a general audience. The technical language used in research often hinders effective communication of scientific knowledge, making it difficult for non-experts to understand. Automatic lay summarization can enhance access to scientific literature, promoting interdisciplinary knowledge sharing and public understanding. This has become especially important for biomedical articles, given the current global need for clear medical information. Large Language Models (LLMs), with their remarkable language understanding capabilities, are ideal for abstractive summarization, helping to make complex information accessible to the public. This paper details our submissions to the BioLaySumm 2024 Shared Task: Lay Summarization of Biomedical Research Articles. We fine-tune and evaluate sequence-to-sequence models like T5 across various training dataset settings and optimization methods such as LoRA for lay summarization. Our submission achieved the 53rd position overall.",
  "keywords": [
    "transformers",
    "audience",
    "b",
    "summarization",
    "automatic lay summarization",
    "current",
    "training",
    "it",
    "optimization methods",
    "information",
    "sequence",
    "lay",
    "scientific",
    "scientific literature",
    "shared task lay summarization"
  ],
  "url": "https://aclanthology.org/2024.bionlp-1.78/",
  "provenance": {
    "collected_at": "2025-06-05 11:04:58",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}