{
  "id": "2024.findings-acl.514",
  "title": "PPTC} Benchmark: Evaluating Large Language Models for {P}ower{P}oint Task Completion",
  "authors": [
    "Guo, Yiduo  and\nZhang, Zekai  and\nLiang, Yaobo  and\nZhao, Dongyan  and\nDuan, Nan"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMsâ€™ ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems .",
  "keywords": [
    "the evaluation",
    "we",
    "dialogue",
    "6 open-source llms",
    "llm",
    "3 closed llms",
    "shot",
    "instruction",
    "other llms",
    "natural",
    "it",
    "75 1 accuracy",
    "their zero-shot few-shot capabilities",
    "sequence",
    "just 6 session accuracy"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.514/",
  "provenance": {
    "collected_at": "2025-06-05 10:55:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}