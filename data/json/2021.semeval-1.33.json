{
  "id": "2021.semeval-1.33",
  "title": "H}umor{H}unter at {S}em{E}val-2021 Task 7: Humor and Offense Recognition with Disentangled Attention",
  "authors": [
    "Xie, Yubo  and\nLi, Junze  and\nPu, Pearl"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F-score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F-score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).",
  "keywords": [
    "deberta",
    "we",
    "the large deberta model",
    "e",
    "the attention scores",
    "relative position vectors",
    "bert",
    "text",
    "the evaluation phase",
    "language",
    "model",
    "their content vectors",
    "the pre-trained language models",
    "vectors",
    "fine-tuned the deberta model"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.33/",
  "provenance": {
    "collected_at": "2025-06-05 08:20:12",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}