{
  "id": "2023.acl-long.118",
  "title": "KALM}: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
  "authors": [
    "Feng, Shangbin  and\nTan, Zhaoxuan  and\nZhang, Wenqian  and\nLei, Zhenyu  and\nTsvetkov, Yulia"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts â€” from local (e.g., sentence), document-level, to global knowledge, to enable knowledge-rich and interpretable exchange across contexts. In addition, incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length. In light of these challenges, we propose KALM, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM firstly encodes long documents and knowledge graphs into the three knowledge-aware context representations. KALM then processes each context with context-specific layers. These context-specific layers are followed by a ContextFusion layer that facilitates knowledge exchange to derive an overarching document representation. Extensive experiments demonstrate that KALM achieves state-of-the-art performance on three long document understanding tasks across 6 datasets/settings. Further analyses reveal that the three knowledge-aware contexts are complementary and they all contribute to model performance, while the importance and information exchange patterns of different contexts vary on different tasks and datasets.",
  "keywords": [
    "question",
    "layer",
    "all",
    "we",
    "pre-trained language models lms",
    "it",
    "information",
    "knowledge graphs",
    "rich",
    "sequence",
    "a language model",
    "knowledge",
    "language",
    "model",
    "pre"
  ],
  "url": "https://aclanthology.org/2023.acl-long.118/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}