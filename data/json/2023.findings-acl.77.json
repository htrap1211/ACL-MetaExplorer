{
  "id": "2023.findings-acl.77",
  "title": "I} am {P}sy{AM}: Modeling Happiness with Cognitive Appraisal Dimensions",
  "authors": [
    "Liu, Xuan  and\nJaidka, Kokil"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "This paper proposes and evaluates PsyAM (https://anonymous.4open.science/r/BERT-PsyAM-10B9), a framework that incorporates adaptor modules in a sequential multi-task learning setup to generate high-dimensional feature representations of hedonic well-being (momentary happiness) in terms of its psychological underpinnings. PsyAM models emotion in text through its cognitive antecedents through auxiliary models that achieve multi-task learning through novel feature fusion methods. We show that BERT-PsyAM has cross-task validity and cross-domain generalizability through experiments with emotion-related tasks â€“ on new emotion tasks and new datasets, as well as against traditional methods and BERT baselines. We further probe the robustness of BERT-PsyAM through feature ablation studies, as well as discuss the qualitative inferences we can draw regarding the effectiveness of the framework for representing emotional states. We close with a discussion of a future agenda of psychology-inspired neural network architectures.",
  "keywords": [
    "feature ablation studies",
    "we",
    "generalizability",
    "fusion",
    "cross",
    "neural",
    "science",
    "learning",
    "psychology-inspired neural network",
    "bert-psyam",
    "i",
    "bert",
    "text",
    "dimensional",
    "cross-domain generalizability"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.77/",
  "provenance": {
    "collected_at": "2025-06-05 09:54:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}