{
  "id": "2022.dialdoc-1.19",
  "title": "TRUE}: Re-evaluating Factual Consistency Evaluation",
  "authors": [
    "Honovich, Or  and\nAharoni, Roee  and\nHerzig, Jonathan  and\nTaitelbaum, Hagai  and\nKukliansy, Doron  and\nCohen, Vered  and\nScialom, Thomas  and\nSzpektor, Idan  and\nHassidim, Avinatan  and\nMatias, Yossi"
  ],
  "year": "2022",
  "venue": "Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering",
  "abstract": "Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive study of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better methods.",
  "keywords": [
    "generation-and-answering-based approaches",
    "question",
    "we",
    "factual inconsistencies",
    "training",
    "increasing attention",
    "the example-level accuracy",
    "an example-level meta-evaluation protocol",
    "metric",
    "text",
    "evaluation cycles",
    "metrics",
    "such metrics",
    "accuracy",
    "work"
  ],
  "url": "https://aclanthology.org/2022.dialdoc-1.19/",
  "provenance": {
    "collected_at": "2025-06-05 08:41:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}