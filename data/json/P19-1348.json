{
  "id": "P19-1348",
  "title": "Generating Question Relevant Captions to Aid Visual Question Answering",
  "authors": [
    "Wu, Jialin  and\nHu, Zeyuan  and\nMooney, Raymond"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Visual question answering (VQA) and image captioning require a shared body of general knowledge connecting language and vision. We present a novel approach to better VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance (e.g. 68.4% in the Test-standard set using a single model) by simultaneously generating question-relevant captions.",
  "keywords": [
    "general",
    "knowledge",
    "language",
    "the vqa v2 challenge",
    "model",
    "general knowledge",
    "question",
    "gradient",
    "we",
    "generating question",
    "visual",
    "vqa",
    "better vqa performance",
    "g",
    "that"
  ],
  "url": "https://aclanthology.org/P19-1348/",
  "provenance": {
    "collected_at": "2025-06-05 00:39:01",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}