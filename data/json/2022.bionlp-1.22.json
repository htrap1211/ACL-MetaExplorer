{
  "id": "2022.bionlp-1.22",
  "title": "G}en{C}ompare{S}um: a hybrid unsupervised summarization method using salience",
  "authors": [
    "Bishop, Jennifer  and\nXie, Qianqian  and\nAnaniadou, Sophia"
  ],
  "year": "2022",
  "venue": "Proceedings of the 21st Workshop on Biomedical Language Processing",
  "abstract": "Text summarization (TS) is an important NLP task. Pre-trained Language Models (PLMs) have been used to improve the performance of TS. However, PLMs are limited by their need of labelled training data and by their attention mechanism, which often makes them unsuitable for use on long documents. To this end, we propose a hybrid, unsupervised, abstractive-extractive approach, in which we walk through a document, generating salient textual fragments representing its key points. We then select the most important sentences of the document by choosing the most similar sentences to the generated texts, calculated using BERTScore. We evaluate the efficacy of generating and using salient textual fragments to guide extractive summarization on documents from the biomedical and general scientific domains. We compare the performance between long and short documents using different generative text models, which are finetuned to generate relevant queries or document titles. We show that our hybrid approach out-performs existing unsupervised methods, as well as state-of-the-art supervised methods, despite not needing a vast amount of labelled training data.",
  "keywords": [
    "the generated texts",
    "end",
    "summarization",
    "we",
    "training",
    "bertscore",
    "salience text summarization ts",
    "their attention mechanism",
    "queries",
    "salience",
    "scientific",
    "generative",
    "text",
    "extractive summarization",
    "generated"
  ],
  "url": "https://aclanthology.org/2022.bionlp-1.22/",
  "provenance": {
    "collected_at": "2025-06-05 08:39:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}