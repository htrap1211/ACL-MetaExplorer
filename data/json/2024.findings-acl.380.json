{
  "id": "2024.findings-acl.380",
  "title": "Too Big to Fail: Larger Language Models are Disproportionately Resilient to Induction of Dementia-Related Linguistic Anomalies",
  "authors": [
    "Li, Changye  and\nSheng, Zhecheng  and\nCohen, Trevor  and\nPakhomov, Serguei"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "As artificial neural networks grow in complexity, understanding their inner workings becomes increasingly challenging, which is particularly important in healthcare applications. The intrinsic evaluation metrics of autoregressive neural language models (NLMs), perplexity (PPL), can reflect how “surprised” an NLM model is at novel input. PPL has been widely used to understand the behavior of NLMs. Previous findings show that changes in PPL when masking attention layers in pre-trained transformer-based NLMs reflect linguistic anomalies associated with Alzheimer’s disease dementia. Building upon this, we explore a novel bidirectional attention head ablation method that exhibits properties attributed to the concepts of cognitive and brain reserve in human brain studies, which postulate that people with more neurons in the brain and more efficient processing are more resilient to neurodegeneration. Our results show that larger GPT-2 models require a disproportionately larger share of attention heads to be masked/ablated to display degradation of similar magnitude to masking in smaller models. These results suggest that the attention mechanism in transformer models may present an analogue to the notions of cognitive and brain reserve and could potentially be used to model certain aspects of the progression of neurodegenerative disorders and aging.",
  "keywords": [
    "perplexity ppl",
    "efficient",
    "linguistic anomalies",
    "we",
    "resilient",
    "neurodegenerative disorders",
    "neural",
    "neurodegenerative",
    "properties",
    "inner",
    "processing",
    "transformer models",
    "their inner workings",
    "pre-trained transformer-based nlms",
    "dementia-related linguistic anomalies"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.380/",
  "provenance": {
    "collected_at": "2025-06-05 10:53:48",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}