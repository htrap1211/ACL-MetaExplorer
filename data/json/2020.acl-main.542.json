{
  "id": "2020.acl-main.542",
  "title": "Curriculum Learning for Natural Language Understanding",
  "authors": [
    "Xu, Benfeng  and\nZhang, Licheng  and\nMao, Zhendong  and\nWang, Quan  and\nXie, Hongtao  and\nZhang, Yongdong"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.",
  "keywords": [
    "language",
    "random",
    "pre-trained language models",
    "natural",
    "model",
    "human",
    "language models",
    "fine",
    "we",
    "natural language",
    "pre",
    "this idea",
    "great",
    "approach",
    "finetune"
  ],
  "url": "https://aclanthology.org/2020.acl-main.542/",
  "provenance": {
    "collected_at": "2025-06-05 07:49:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}