{
  "id": "2023.wassa-1.45",
  "title": "YNU}-{HPCC} at {WASSA}-2023 Shared Task 1: Large-scale Language Model with {L}o{RA} Fine-Tuning for Empathy Detection and Emotion Classification",
  "authors": [
    "Wang, Yukun  and\nWang, Jin  and\nZhang, Xuejie"
  ],
  "year": "2023",
  "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis",
  "abstract": "This paper describes the system for the YNU-HPCC team in WASSA-2023 Shared Task 1: Empathy Detection and Emotion Classification. This task needs to predict the empathy, emotion, and personality of the empathic reactions. This system is mainly based on the Decoding-enhanced BERT with disentangled attention (DeBERTa) model with parameter-efficient fine-tuning (PEFT) and the Robustly Optimized BERT Pretraining Approach (RoBERTa). Low-Rank Adaptation (LoRA) fine-tuning in PEFT is used to reduce the training parameters of large language models. Moreover, back translation is introduced to augment the training dataset. This system achieved relatively good results on the competitionâ€™s official leaderboard. The code of this system is available here.",
  "keywords": [
    "1 large-scale language model",
    "code",
    "tuning",
    "roberta",
    "language",
    "deberta",
    "back translation",
    "ra",
    "translation",
    "bert",
    "model",
    "disentangled attention deberta model",
    "efficient",
    "large language models",
    "emotion classification"
  ],
  "url": "https://aclanthology.org/2023.wassa-1.45/",
  "provenance": {
    "collected_at": "2025-06-05 10:33:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}