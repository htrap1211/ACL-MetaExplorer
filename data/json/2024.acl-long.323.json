{
  "id": "2024.acl-long.323",
  "title": "S}parse{F}low: Accelerating Transformers by Sparsifying Information Flows",
  "authors": [
    "Kim, Yeachan  and\nLee, SangKeun"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Transformers have become the de-facto standard for natural language processing. However, dense information flows within transformers pose significant challenges for real-time and resource-constrained devices, as computational complexity grows quadratically with sequence length. To counteract such dense information flows, we propose SparseFlow, a novel efficient method designed to sparsify the dense pathways of token representations across all transformer blocks. To this end, SparseFlow parameterizes the information flows linking token representations to transformer blocks. These parameterized information flows are optimized to be sparse, allowing only the salient information to pass through into the blocks. To validate the efficacy of SparseFlow, we conduct comprehensive experiments across diverse benchmarks (understanding and generation), scales (ranging from millions to billions), architectures (including encoders, decoders, and seq-to-seq models), and modalities (such as language-only and vision-language). The results convincingly demonstrate that sparsifying the dense information flows leads to substantial speedup gains without compromising task accuracy. For instance, SparseFlow reduces computational costs by half on average, without a significant loss in accuracy.",
  "keywords": [
    "transformers",
    "end",
    "only the salient information",
    "efficient",
    "we",
    "a novel efficient method",
    "natural",
    "generation scales",
    "task accuracy",
    "token",
    "loss",
    "information",
    "decoders",
    "sequence",
    "natural language processing"
  ],
  "url": "https://aclanthology.org/2024.acl-long.323/",
  "provenance": {
    "collected_at": "2025-06-05 10:38:44",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}