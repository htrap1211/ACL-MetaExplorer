{
  "id": "2023.acl-long.493",
  "title": "The {CRINGE} Loss: Learning what language not to model",
  "authors": [
    "Adolphs, Leonard  and\nGao, Tianyu  and\nXu, Jing  and\nShuster, Kurt  and\nSukhbaatar, Sainbayar  and\nWeston, Jason"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the “CRINGE” loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.",
  "keywords": [
    "work",
    "language",
    "generation",
    "model",
    "safe generation contradiction avoidance",
    "human",
    "loss",
    "we",
    "dialogue",
    "contrastive iterative negative generation",
    "open-domain dialogue",
    "standard language model training",
    "training",
    "that",
    "documents"
  ],
  "url": "https://aclanthology.org/2023.acl-long.493/",
  "provenance": {
    "collected_at": "2025-06-05 09:42:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}