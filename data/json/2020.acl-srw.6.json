{
  "id": "2020.acl-srw.6",
  "title": "Research on Task Discovery for Transfer Learning in Deep Neural Networks",
  "authors": [
    "Akdemir, Arda"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
  "abstract": "Deep neural network based machine learning models are shown to perform poorly on unseen or out-of-domain examples by numerous recent studies. Transfer learning aims to avoid overfitting and to improve generalizability by leveraging the information obtained from multiple tasks. Yet, the benefits of transfer learning depend largely on task selection and finding the right method of sharing. In this thesis, we hypothesize that current deep neural network based transfer learning models do not achieve their fullest potential for various tasks and there are still many task combinations that will benefit from transfer learning that are not considered by the current models. To this end, we started our research by implementing a novel multi-task learner with relaxed annotated data requirements and obtained a performance improvement on two NLP tasks. We will further devise models to tackle tasks from multiple areas of machine learning, such as Bioinformatics and Computer Vision, in addition to NLP.",
  "keywords": [
    "deep",
    "end",
    "neural",
    "nlp",
    "machine",
    "studies",
    "information",
    "deep neural networks",
    "machine learning",
    "-task learner",
    "network",
    "we",
    "learning",
    "transfer",
    "two nlp tasks"
  ],
  "url": "https://aclanthology.org/2020.acl-srw.6/",
  "provenance": {
    "collected_at": "2025-06-05 07:53:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}