{
  "id": "2024.langmol-1.8",
  "title": "S}ci{M}ind: A Multimodal Mixture-of-Experts Model for Advancing Pharmaceutical Sciences",
  "authors": [
    "Xiong, Zhaoping  and\nFang, Xintao  and\nChu, Haotian  and\nWan, Xiaozhe  and\nLiu, Liwei  and\nLi, Yameng  and\nXiang, Wenkai  and\nZheng, Mingyue"
  ],
  "year": "2024",
  "venue": "Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)",
  "abstract": "Large language models (LLMs) have made substantial strides, but their use in reliably tackling issues within specialized domains, particularly in interdisciplinary areas like pharmaceutical sciences, is hindered by data heterogeneity, knowledge complexity, unique objectives, and a spectrum of constraint conditions. In this area, diverse modalities such as nucleic acids, proteins, molecular structures, and natural language are often involved. We designed a specialized token set and introduced a new Mixture-of-Experts (MoEs) pre-training and fine-tuning strategy to unify these modalities in one model. With this strategy, weâ€™ve created a multi-modal mixture-of-experts foundational model for pharmaceutical sciences, named SciMind. This model has undergone extensive pre-training on publicly accessible datasets including nucleic acid sequences, protein sequences, molecular structure strings, and biomedical texts, and delivers good performance on biomedical text comprehension, promoter prediction, protein function prediction, molecular description, and molecular generation.",
  "keywords": [
    "objectives",
    "sciences",
    "we",
    "training",
    "natural",
    "token",
    "diverse modalities",
    "pharmaceutical sciences",
    "natural language",
    "m",
    "these modalities",
    "llms",
    "tuning",
    "text",
    "molecular generation"
  ],
  "url": "https://aclanthology.org/2024.langmol-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 11:07:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}