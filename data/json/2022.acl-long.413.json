{
  "id": "2022.acl-long.413",
  "title": "Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge",
  "authors": [
    "Zhang, Linhai  and\nHu, Xuemeng  and\nWang, Boyu  and\nZhou, Deyu  and\nZhang, Qian-Wen  and\nCao, Yunbo"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pre-trained language models (PLMs) into neural topic modeling. However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead. In this paper, we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pre-trained on a large corpus and then fine-tuned on the target dataset. Experiments have been conducted on three datasets and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs. Moreover, further study shows that the proposed approach greatly reduces the need for the huge size of training data.",
  "keywords": [
    "embeddings",
    "tuning",
    "knowledge",
    "language",
    "pre-trained word embeddings pwes",
    "neural",
    "topic modeling",
    "trained language models plms",
    "model",
    "topic",
    "modeling",
    "fine",
    "some topic modeling approaches",
    "word",
    "we"
  ],
  "url": "https://aclanthology.org/2022.acl-long.413/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}