{
  "id": "2023.acl-long.251",
  "title": "Back Translation for Speech-to-text Translation Without Transcripts",
  "authors": [
    "Fang, Qingkai  and\nFeng, Yang"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes available since numerous unwritten languages exist worldwide. In this paper, we aim to utilize large amounts of target-side monolingual data to enhance ST without transcripts. Motivated by the remarkable success of back translation in MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo ST data from monolingual target data. To ease the challenges posed by short-to-long generation and one-to-many mapping, we introduce self-supervised discrete units and achieve back translation by cascading a target-to-unit model and a unit-to-speech model. With our synthetic ST data, we achieve an average boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments show that our method is especially effective in low-resource scenarios.",
  "keywords": [
    "end",
    "a back translation algorithm",
    "bleu",
    "machine translation",
    "we",
    "training",
    "translation",
    "self",
    "boost",
    "st",
    "text",
    "short-to-long generation",
    "-",
    "e g",
    "generation"
  ],
  "url": "https://aclanthology.org/2023.acl-long.251/",
  "provenance": {
    "collected_at": "2025-06-05 09:38:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}