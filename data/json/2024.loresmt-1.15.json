{
  "id": "2024.loresmt-1.15",
  "title": "Learning-From-Mistakes Prompting for Indigenous Language Translation",
  "authors": [
    "Liao, You Cheng  and\nYu, Chen-Jui  and\nLin, Chi-Yi  and\nYun, He-Feng  and\nWang, Yen-Hsiang  and\nLi, Hsiao-Min  and\nFan, Yao-Chung"
  ],
  "year": "2024",
  "venue": "Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)",
  "abstract": "Using large language models, this paper presents techniques to improve extremely low-resourced indigenous language translations. Our approaches are grounded in the use of (1) the presence of a datastore consisting of a limited number of parallel translation examples, (2) the inherent capabilities of LLMs like GPT-3.5, and (3) a word-level translation dictionary. We harness the potential of LLMs and in-context learning techniques in such a setting for using LLM as universal translators for extremely low-resourced languages. Our methodology hinges on utilizing LLMs as language compilers for selected language pairs, hypothesizing that they could internalize syntactic structures to facilitate accurate translation. We introduce three techniques: KNN-Prompting with Retrieved Prompting Context, Chain-of-Thought Prompting, and Learning-from-Mistakes Prompting, with the last method addressing past errors. The evaluation results suggest that, even with limited corpora, LLMs, when paired with proper prompting, can effectively translate extremely low-resource languages.",
  "keywords": [
    "accurate translation",
    "the evaluation results",
    "a word-level translation dictionary",
    "language",
    "thought",
    "indigenous language translation",
    "the inherent capabilities",
    "chain",
    "large language models",
    "gpt-3",
    "capabilities",
    "proper prompting",
    "prompting",
    "we",
    "word"
  ],
  "url": "https://aclanthology.org/2024.loresmt-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 11:08:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}