{
  "id": "2022.acl-long.227",
  "title": "Life after {BERT}: What do Other Muppets Understand about Language?",
  "authors": [
    "Lialin, Vladislav  and\nZhao, Kevin  and\nShivagunde, Namrata  and\nRumshisky, Anna"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a modelâ€™s linguistic capabilities.",
  "keywords": [
    "work",
    "transformer",
    "families",
    "objectives",
    "a zero-shot fashion",
    "language",
    "bert",
    "gpt networks",
    "model",
    "objective",
    "albert",
    "existing pre-training objectives",
    "gpt",
    "capabilities",
    "pre-training objectives"
  ],
  "url": "https://aclanthology.org/2022.acl-long.227/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}