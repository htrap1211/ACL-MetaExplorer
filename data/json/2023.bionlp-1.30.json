{
  "id": "2023.bionlp-1.30",
  "title": "Evaluation of {C}hat{GPT} on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers",
  "authors": [
    "Jahan, Israt  and\nLaskar, Md Tahmid Rahman  and\nPeng, Chun  and\nHuang, Jimmy"
  ],
  "year": "2023",
  "venue": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
  "abstract": "ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPTâ€™s pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data.",
  "keywords": [
    "transformers",
    "end",
    "extraction",
    "chatgpt s pre",
    "question",
    "summarization",
    "we",
    "shot",
    "training",
    "classification",
    "fine-tuned generative transformers chatgpt",
    "biogpt",
    "it",
    "chatgpt",
    "zero-shot chatgpt"
  ],
  "url": "https://aclanthology.org/2023.bionlp-1.30/",
  "provenance": {
    "collected_at": "2025-06-05 10:22:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}