{
  "id": "2024.acl-long.289",
  "title": "P}re{FLMR}: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers",
  "authors": [
    "Lin, Weizhe  and\nMei, Jingbiao  and\nChen, Jinghong  and\nByrne, Bill"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to questions. We present an extensive training and evaluation framework, M2KR, for KB-VQA. M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of benchmark tasks for training and evaluating general-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks. We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose multi-modal retrievers.",
  "keywords": [
    "late",
    "the retrieval",
    "general",
    "knowledge",
    "language",
    "natural",
    "fine-grained late-interaction multi-modal retrievers",
    "retrievers",
    "retrieval",
    "question",
    "information",
    "general-purpose multi-modal retrievers",
    "we",
    "visual",
    "natural language"
  ],
  "url": "https://aclanthology.org/2024.acl-long.289/",
  "provenance": {
    "collected_at": "2025-06-05 10:38:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}