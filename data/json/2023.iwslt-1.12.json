{
  "id": "2023.iwslt-1.12",
  "title": "MT} Metrics Correlate with Human Ratings of Simultaneous Speech Translation",
  "authors": [
    "Mach{\\'a}{\\v{c}}ek, Dominik  and\nBojar, Ond{\\v{r}}ej  and\nDabre, Raj"
  ],
  "year": "2023",
  "venue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
  "abstract": "There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the metrics with translation as a reference is significantly higher than with simultaneous interpreting, and thus we recommend the former for reliable evaluation.",
  "keywords": [
    "bleu",
    "machine translation",
    "proxies",
    "we",
    "current",
    "mt evaluation metrics",
    "translation",
    "bertscore",
    "mt metrics",
    "the evaluations",
    "bleu chrf2 bertscore",
    "the metrics",
    "these metrics",
    "cr",
    "analysis"
  ],
  "url": "https://aclanthology.org/2023.iwslt-1.12/",
  "provenance": {
    "collected_at": "2025-06-05 10:24:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}