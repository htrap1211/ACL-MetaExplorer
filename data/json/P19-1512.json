{
  "id": "P19-1512",
  "title": "Improving Textual Network Embedding with Global Attention via Optimal Transport",
  "authors": [
    "Chen, Liqun  and\nWang, Guoyin  and\nTao, Chenyang  and\nShen, Dinghan  and\nCheng, Pengyu  and\nZhang, Xinyuan  and\nWang, Wenlin  and\nZhang, Yizhe  and\nCarin, Lawrence"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Constituting highly informative network embeddings is an essential tool for network analysis. It encodes network topology, along with other useful side information, into low dimensional node-based feature representations that can be exploited by statistical modeling. This work focuses on learning context-aware network embeddings augmented with text data. We reformulate the network embedding problem, and present two novel strategies to improve over traditional attention mechanisms: (i) a content-aware sparse attention module based on optimal transport; and (ii) a high-level attention parsing module. Our approach yields naturally sparse and self-normalized relational inference. It can capture long-term interactions between sequences, thus addressing the challenges faced by existing textual network embedding schemes. Extensive experiments are conducted to demonstrate our model can consistently outperform alternative state-of-the-art methods.",
  "keywords": [
    "global attention",
    "we",
    "traditional attention mechanisms",
    "it",
    "self",
    "information",
    "highly informative network embeddings",
    "yields",
    "analysis",
    "i",
    "two novel strategies",
    "text",
    "strategies",
    "our approach yields",
    "dimensional"
  ],
  "url": "https://aclanthology.org/P19-1512/",
  "provenance": {
    "collected_at": "2025-06-05 00:43:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}