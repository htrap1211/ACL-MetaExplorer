{
  "id": "2021.acl-long.503",
  "title": "BERTG}en: Multi-task Generation through {BERT",
  "authors": [
    "Mitzalis, Faidon  and\nCaglayan, Ozan  and\nMadhyastha, Pranava  and\nSpecia, Lucia"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGenâ€™s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.",
  "keywords": [
    "generative",
    "multi-task generation",
    "bertg",
    "that bertgen",
    "generation",
    "language",
    "bert",
    "model",
    "machine",
    "studies",
    "multimodal machine translation",
    "evaluations",
    "it",
    "bertgen s ability",
    "ablation studies"
  ],
  "url": "https://aclanthology.org/2021.acl-long.503/",
  "provenance": {
    "collected_at": "2025-06-05 08:06:10",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}