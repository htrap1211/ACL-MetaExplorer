{
  "id": "2023.acl-long.355",
  "title": "C}ontra{CLM}: Contrastive Learning For Causal Language Model",
  "authors": [
    "Jain, Nihal  and\nZhang, Dejiao  and\nAhmad, Wasi Uddin  and\nWang, Zijian  and\nNan, Feng  and\nLi, Xiaopeng  and\nTan, Ming  and\nNallapati, Ramesh  and\nRay, Baishakhi  and\nBhatia, Parminder  and\nMa, Xiaofei  and\nXiang, Bing"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.",
  "keywords": [
    "code",
    "variety",
    "accuracy",
    "language generation",
    "language",
    "generation",
    "model",
    "causal language models",
    "token",
    "encoder",
    "semantic",
    "we",
    "learning",
    "sequence",
    "a variety"
  ],
  "url": "https://aclanthology.org/2023.acl-long.355/",
  "provenance": {
    "collected_at": "2025-06-05 09:40:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}