{
  "id": "2020.acl-main.153",
  "title": "Self-Attention with Cross-Lingual Position Representation",
  "authors": [
    "Ding, Liang  and\nWang, Longyue  and\nTao, Dacheng"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs withcross-lingual position representationsto model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT’14 English⇒German, WAT’17 Japanese⇒English, and WMT’17 Chinese⇔English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information.",
  "keywords": [
    "alignments",
    "natural language processing tasks",
    "self-attention",
    "we",
    "cross-lingual scenarios machine translation",
    "translation",
    "cross",
    "natural",
    "self",
    "information",
    "latent",
    "word",
    "bilingual diagonal alignments",
    "processing",
    "translation quality"
  ],
  "url": "https://aclanthology.org/2020.acl-main.153/",
  "provenance": {
    "collected_at": "2025-06-05 07:44:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}