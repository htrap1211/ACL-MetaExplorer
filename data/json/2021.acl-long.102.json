{
  "id": "2021.acl-long.102",
  "title": "Common Sense Beyond {E}nglish: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning",
  "authors": [
    "Lin, Bill Yuchen  and\nLee, Seyeon  and\nQiao, Xiaoyang  and\nRen, Xiang"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method â€” multilingual contrastive pretraining (MCP). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R_L).",
  "keywords": [
    "popular multilingual language models",
    "cross",
    "multilingual language models",
    "general",
    "language",
    "csqa",
    "it",
    "2 7 accuracy",
    "-",
    "a language-general probing task",
    "x",
    "we",
    "accuracy",
    "nglish",
    "evaluating"
  ],
  "url": "https://aclanthology.org/2021.acl-long.102/",
  "provenance": {
    "collected_at": "2025-06-05 08:00:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}