{
  "id": "2023.acl-short.43",
  "title": "M}eta{VL}: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
  "authors": [
    "Monajatipoor, Masoud  and\nLi, Liunian Harold  and\nRouhsedaghat, Mozhdeh  and\nYang, Lin  and\nChang, Kai-Wei"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model while having ~20 times fewer parameters.",
  "keywords": [
    "cross",
    "vision-language models",
    "i",
    "language",
    "nlp",
    "model",
    "modalities",
    "language models",
    "cross modalities",
    "-",
    "encoder",
    "vqa ok",
    "we",
    "learning",
    "nlp tasks"
  ],
  "url": "https://aclanthology.org/2023.acl-short.43/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}