{
  "id": "2024.findings-acl.521",
  "title": "GAOKAO}-{MM}: A {C}hinese Human-Level Benchmark for Multimodal Models Evaluation",
  "authors": [
    "Zong, Yi  and\nQiu, Xipeng"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing datasets either focus solely on primary perception abilities and commonsense knowledge, or have a low level of text comprehension difficulty, which are insufficient to reflect the comprehensive capabilities of LVLMs, particularly in terms of Chinese language proficiency. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the modelâ€™s abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vision (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moderate distance towards Artificial General Intelligence (AGI) and provide insights facilitating the development of multilingual LVLMs. The dataset and evaluation code are available through: https://github.com/OpenMOSS/GAOKAO-MM",
  "keywords": [
    "code",
    "the accuracies",
    "the model s abilities",
    "chinese language proficiency",
    "all",
    "we",
    "artificial general intelligence agi",
    "analysis",
    "abilities",
    "the large vision-language models",
    "text",
    "primary perception abilities",
    "great abilities",
    "-",
    "gpt-4"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.521/",
  "provenance": {
    "collected_at": "2025-06-05 10:55:43",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}