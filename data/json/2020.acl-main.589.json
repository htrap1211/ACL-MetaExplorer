{
  "id": "2020.acl-main.589",
  "title": "Differentiable Window for Dynamic Local Attention",
  "authors": [
    "Nguyen, Thanh-Tung  and\nNguyen, Xuan-Phi  and\nJoty, Shafiq  and\nLi, Xiaoli"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions. We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways. We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling. Our experimental results demonstrate consistent and sizable improvements across all tasks.",
  "keywords": [
    "transformer",
    "general",
    "language",
    "dynamic local attention",
    "neural",
    "nlp",
    "machine",
    "standard attention modules",
    "attentions",
    "attention",
    "more focused attentions",
    "we",
    "nlp tasks",
    "general purpose component",
    "the transformer architecture"
  ],
  "url": "https://aclanthology.org/2020.acl-main.589/",
  "provenance": {
    "collected_at": "2025-06-05 07:50:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}