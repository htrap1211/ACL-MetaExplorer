{
  "id": "2020.acl-demos.16",
  "title": "The {M}icrosoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding",
  "authors": [
    "Liu, Xiaodong  and\nWang, Yu  and\nJi, Jianshu  and\nCheng, Hao  and\nZhu, Xueyun  and\nAwa, Emmanuel  and\nHe, Pengcheng  and\nChen, Weizhu  and\nPoon, Hoifung  and\nCao, Guihong  and\nGao, Jianfeng"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
  "abstract": "We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pre-trained models will be publicly available athttps://github.com/namisan/mt-dnn.",
  "keywords": [
    "deep",
    "variety",
    "transformers",
    "roberta",
    "objectives",
    "support",
    "efficient",
    "rnns",
    "we",
    "classification",
    "efficient production deployment",
    "neural",
    "natural",
    "it",
    "learning"
  ],
  "url": "https://aclanthology.org/2020.acl-demos.16/",
  "provenance": {
    "collected_at": "2025-06-05 07:52:54",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}