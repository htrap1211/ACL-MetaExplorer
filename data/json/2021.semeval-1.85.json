{
  "id": "2021.semeval-1.85",
  "title": "JUST}-{BLUE} at {S}em{E}val-2021 Task 1: Predicting Lexical Complexity using {BERT} and {R}o{BERT}a Pre-trained Language Models",
  "authors": [
    "Bani Yaseen, Tuqa  and\nIsmail, Qusai  and\nAl-Omari, Sarah  and\nAl-Sobh, Eslam  and\nAbdullah, Malak"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "Predicting the complexity level of a word or a phrase is considered a challenging task. It is even recognized as a crucial step in numerous NLP applications, such as text rearrangements and text simplification. Early research treated the task as a binary classification task, where the systems anticipated the existence of a wordâ€™s complexity (complex versus uncomplicated). Other studies had been designed to assess the level of word complexity using regression models or multi-labeling classification models. Deep learning models show a significant improvement over machine learning models with the rise of transfer learning and pre-trained language models. This paper presents our approach that won the first rank in the SemEval-task1 (sub stask1). We have calculated the degree of word complexity from 0-1 within a text. We have been ranked first place in the competition using the pre-trained language models Bert and RoBERTa, with a Pearson correlation score of 0.788.",
  "keywords": [
    "deep",
    "roberta",
    "we",
    "a pre-trained language models",
    "a binary classification task",
    "classification",
    "pre-trained language models",
    "it",
    "machine learning models",
    "numerous nlp applications",
    "word",
    "learning",
    "transfer",
    "transfer learning",
    "bert"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.85/",
  "provenance": {
    "collected_at": "2025-06-05 08:20:53",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}