{
  "id": "2022.acl-long.230",
  "title": "Adaptive Testing and Debugging of {NLP} Models",
  "authors": [
    "Ribeiro, Marco Tulio  and\nLundberg, Scott"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.",
  "keywords": [
    "work",
    "process",
    "language",
    "partnership",
    "nlp",
    "feedback",
    "model",
    "human",
    "class",
    "text",
    "we",
    "nlp models",
    "human feedback",
    "current",
    "bugs"
  ],
  "url": "https://aclanthology.org/2022.acl-long.230/",
  "provenance": {
    "collected_at": "2025-06-05 08:27:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}