{
  "id": "2021.acl-long.155",
  "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
  "authors": [
    "Qian, Lihua  and\nZhou, Hao  and\nBao, Yu  and\nWang, Mingxuan  and\nQiu, Lin  and\nZhang, Weinan  and\nYu, Yong  and\nLi, Lei"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8×-15× speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.",
  "keywords": [
    "work",
    "transformer",
    "single-pass parallel generation models",
    "glancing transformer glat",
    "language",
    "generation",
    "neural",
    "nat",
    "machine",
    "model",
    "word interdependency experiments",
    "high-quality translation",
    "glancing transformer",
    "the efficiency",
    "bleu"
  ],
  "url": "https://aclanthology.org/2021.acl-long.155/",
  "provenance": {
    "collected_at": "2025-06-05 08:01:30",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}