{
  "id": "2023.acl-long.447",
  "title": "P}ara{AMR}: A Large-Scale Syntactically Diverse Paraphrase Dataset by {AMR} Back-Translation",
  "authors": [
    "Huang, Kuan-Hao  and\nIyer, Varun  and\nHsu, I-Hung  and\nKumar, Anoop  and\nChang, Kai-Wei  and\nGalstyan, Aram"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity â€“ the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present ParaAMR, a large-scale syntactically diverse paraphrase dataset created by abstract meaning representation back-translation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of ParaAMR are syntactically more diverse compared to existing large-scale paraphrase datasets while preserving good semantic similarity. In addition, we show that ParaAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning. Our results thus showcase the potential of ParaAMR for improving various NLP applications.",
  "keywords": [
    "inefficient",
    "good semantic similarity",
    "semantic",
    "we",
    "various nlp applications",
    "syntax",
    "shot",
    "translation",
    "amr back-translation paraphrase generation",
    "natural language processing nlp",
    "human evaluation",
    "natural",
    "back-translation",
    "analysis",
    "processing"
  ],
  "url": "https://aclanthology.org/2023.acl-long.447/",
  "provenance": {
    "collected_at": "2025-06-05 09:41:28",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}