{
  "id": "2021.acl-short.40",
  "title": "PRAL}: A Tailored Pre-Training Model for Task-Oriented Dialog Generation",
  "authors": [
    "Gu, Jing  and\nWu, Qingyang  and\nWu, Chongruo  and\nShi, Weiyan  and\nYu, Zhou"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained models on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques: start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt PRALon three downstream tasks. The results show that PRAL outperforms or is on par with state-of-the-art models.",
  "keywords": [
    "task-oriented dialog tasks",
    "knowledge",
    "generation",
    "language",
    "model",
    "par",
    "dialog",
    "task-oriented conversational systems",
    "conversational",
    "language model pral",
    "we",
    "pre",
    "gpt-2",
    "pre-training performance",
    "various language generation tasks"
  ],
  "url": "https://aclanthology.org/2021.acl-short.40/",
  "provenance": {
    "collected_at": "2025-06-05 08:07:39",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}