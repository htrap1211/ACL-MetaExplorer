{
  "id": "2022.acl-long.424",
  "title": "MSP}: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators",
  "authors": [
    "Tan, Zhixing  and\nZhang, Xiangwen  and\nWang, Shuo  and\nLiu, Yang"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks. We present Multi-Stage Prompting, a simple and automatic approach for leveraging pre-trained language models to translation tasks. To better mitigate the discrepancy between pre-training and translation, MSP divides the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage. During each stage, we independently apply different continuous prompts for allowing pre-trained language models better shift to translation tasks. We conduct extensive experiments on three translation tasks. Experiments show that our method can significantly improve the translation performance of pre-trained language models.",
  "keywords": [
    "translation tasks",
    "prompts",
    "process",
    "pre-training and translation msp",
    "language",
    "the translation process",
    "the translation performance",
    "pre-trained language models",
    "three translation tasks experiments",
    "-",
    "we",
    "different continuous prompts",
    "pre",
    "training",
    "translation"
  ],
  "url": "https://aclanthology.org/2022.acl-long.424/",
  "provenance": {
    "collected_at": "2025-06-05 08:30:01",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}