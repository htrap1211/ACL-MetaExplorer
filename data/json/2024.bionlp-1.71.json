{
  "id": "2024.bionlp-1.71",
  "title": "HULAT}-{UC}3{M} at {B}iolay{S}umm: Adaptation of {B}io{BART} and Longformer models to summarizing biomedical documents",
  "authors": [
    "Gonzalez Sanchez, Adrian  and\nMart{\\'i}nez, Paloma"
  ],
  "year": "2024",
  "venue": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
  "abstract": "This article presents our submission to the Bio- LaySumm 2024 shared task: Lay Summarization of Biomedical Research Articles. The objective of this task is to generate summaries that are simplified in a concise and less technical way, in order to facilitate comprehension by non-experts users. A pre-trained BioBART model was employed to fine-tune the articles from the two journals, thereby generating two models, one for each journal. The submission achieved the 12th best ranking in the task, attaining a meritorious first place in the Relevance ROUGE-1 metric.",
  "keywords": [
    "uc",
    "summaries",
    "model",
    "objective",
    "metric",
    "the objective",
    "-",
    "umm",
    "fine",
    "summarization",
    "pre",
    "shared task lay summarization",
    "that",
    "documents",
    "hulat"
  ],
  "url": "https://aclanthology.org/2024.bionlp-1.71/",
  "provenance": {
    "collected_at": "2025-06-05 11:04:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}