{
  "id": "2023.acl-long.904",
  "title": "Prototype-Guided Pseudo Labeling for Semi-Supervised Text Classification",
  "authors": [
    "Yang, Weiyi  and\nZhang, Richong  and\nChen, Junfan  and\nWang, Lihong  and\nKim, Jaein"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Semi-supervised text classification (SSTC) aims at text classification with few labeled data and massive unlabeled data. Recent works achieve this task by pseudo-labeling methods, with the belief that the unlabeled and labeled data have identical data distribution, and assign the unlabeled data with pseudo-labels as additional supervision. However, existing pseudo-labeling methods usually suffer from ambiguous categorical boundary issues when training the pseudo-labeling phase, and simply select pseudo-labels without considering the unbalanced categorical distribution of the unlabeled data, making it difficult to generate reliable pseudo-labels for each category. We propose a novel semi-supervised framework, namely ProtoS2, with prototypical cluster separation (PCS) and prototypical-center data selection (CDS) technology to address the issue. Particularly, PCS exploits categorical prototypes to assimilate instance representations within the same category, thus emphasizing low-density separation for the pseudo-labeled data to alleviate ambiguous boundaries. Besides, CDS selects central pseudo-labeled data considering the categorical distribution, avoiding the model from biasing on dominant categories. Empirical studies and extensive analysis with four benchmarks demonstrate the effectiveness of the proposed model.",
  "keywords": [
    "ambiguous boundaries",
    "dominant categories empirical studies",
    "semi-supervised text classification sstc",
    "we",
    "classification",
    "cluster",
    "it",
    "text classification",
    "analysis",
    "categories",
    "the belief",
    "text",
    "semi-supervised text classification",
    "-",
    "model"
  ],
  "url": "https://aclanthology.org/2023.acl-long.904/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}