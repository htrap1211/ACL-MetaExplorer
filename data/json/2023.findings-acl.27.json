{
  "id": "2023.findings-acl.27",
  "title": "Prompt Tuning for Unified Multimodal Pretrained Models",
  "authors": [
    "Yang, Hao  and\nLin, Junyang  and\nYang, An  and\nWang, Peng  and\nZhou, Chang"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low computation costs and almost lossless performance. In this work, we explore the transfer of prompt tuning to multimodal pretrained models. Specifically, we implement prompt tuning to a unified sequence-to-sequence pretrained model by adding a sequence of learnable embeddings to each layer and finetuning the pretrained model on downstream task with only the learnable embeddings being optimized. Experimental results on a series of multimodal understanding and generation tasks demonstrate that our method OFA-PT can achieve comparable performance with finetuning across a series of multimodal generation and understanding tasks. Additionally, it significantly outperforms the unified multimodal pretrained model with other parameter-efficient tuning methods, e.g., Adapter, BitFit. etc. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning.",
  "keywords": [
    "pt",
    "efficient",
    "learnable embeddings",
    "series",
    "layer",
    "prompt length prompt depth",
    "multimodal generation",
    "we",
    "parameter",
    "natural",
    "it",
    "unified",
    "generation tasks",
    "sequence",
    "transfer"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.27/",
  "provenance": {
    "collected_at": "2025-06-05 09:53:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}