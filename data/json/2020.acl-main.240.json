{
  "id": "2020.acl-main.240",
  "title": "Masked Language Model Scoring",
  "authors": [
    "Salazar, Julian  and\nLiang, Davis  and\nNguyen, Toan Q.  and\nKirchhoff, Katrin"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring athttps://github.com/awslabs/mlm-scoring.",
  "keywords": [
    "variety",
    "low-resource translation",
    "roberta",
    "bias",
    "end",
    "wer",
    "bleu",
    "language model scoring",
    "we",
    "perplexities",
    "translation",
    "cross",
    "rescore translations",
    "to 1 7 bleu",
    "most nlp tasks"
  ],
  "url": "https://aclanthology.org/2020.acl-main.240/",
  "provenance": {
    "collected_at": "2025-06-05 07:45:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}