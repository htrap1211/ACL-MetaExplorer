{
  "id": "2024.bionlp-1.68",
  "title": "EL}i{RF}-{VRAIN} at {B}io{L}ay{S}umm: Boosting Lay Summarization Systems Performance with Ranking Models",
  "authors": [
    "Ahuir, Vicent  and\nTorres, Diego  and\nSegarra, Encarna  and\nHurtado, Llu{\\'i}s-F."
  ],
  "year": "2024",
  "venue": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
  "abstract": "This paper presents our contribution to the BioLaySumm 2024 shared task of the 23rd BioNLP Workshop. The task is to create a lay summary, given a biomedical research article and its technical summary. As the input to the system could be large, a Longformer Encoder-Decoder (LED) has been used. We continuously pre-trained a general domain LED model with biomedical data to adapt it to this specific domain. In the pre-training phase, several pre-training tasks were aggregated to inject linguistic knowledge and increase the abstractivity of the generated summaries. Since the distribution of samples between the two datasets, eLife and PLOS, is unbalanced, we fine-tuned two models: one for eLife and another for PLOS. To increase the quality of the lay summaries of the system, we developed a regression model that helps us rank the summaries generated by the summarization models. This regression model predicts the quality of the summary in three different aspects: Relevance, Readability, and Factuality. We present the results of our models and a study to measure the ranking capabilities of the regression model.",
  "keywords": [
    "a longformer encoder-decoder",
    "the lay summaries",
    "summarization",
    "we",
    "training",
    "the 23rd bionlp workshop",
    "it",
    "decoder",
    "the summaries",
    "several pre-training tasks",
    "the generated summaries",
    "lay",
    "the pre-training phase",
    "the ranking capabilities",
    "bionlp"
  ],
  "url": "https://aclanthology.org/2024.bionlp-1.68/",
  "provenance": {
    "collected_at": "2025-06-05 11:04:50",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}