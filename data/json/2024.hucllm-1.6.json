{
  "id": "2024.hucllm-1.6",
  "title": "Evaluating Large Language Models on Social Signal Sensitivity: An Appraisal Theory Approach",
  "authors": [
    "Wu, Zhen  and\nDutt, Ritam  and\nRose, Carolyn"
  ],
  "year": "2024",
  "venue": "Proceedings of the 1st Human-Centered Large Language Modeling Workshop",
  "abstract": "We present a framework to assess the sensitivity of Large Language Models (LLMs) to textually embedded social signals using an Appraisal Theory perspective. We report on an experiment that uses prompts encoding three dimensions of social signals: Affect, Judgment, and Appreciation. In response to the prompt, an LLM generates both an analysis (Insight) and a conversational Response, which are analyzed in terms of sensitivity to the signals. We quantitatively evaluate the output text through topical analysis of the Insight and predicted social intelligence scores of the Response in terms of empathy and emotional polarity. Key findings show that LLMs are more sensitive to positive signals. The personas impact Responses but not the Insight. We discuss how our framework can be extended to a broader set of social signals, personas, and scenarios to evaluate LLM behaviors under various conditions.",
  "keywords": [
    "prompts",
    "prompt",
    "language",
    "text",
    "large language models",
    "an llm",
    "the prompt",
    "conversational",
    "llm behaviors",
    "we",
    "a conversational response",
    "llm",
    "analysis",
    "llms",
    "that"
  ],
  "url": "https://aclanthology.org/2024.hucllm-1.6/",
  "provenance": {
    "collected_at": "2025-06-05 11:06:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}