{
  "id": "2022.acl-long.282",
  "title": "Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation",
  "authors": [
    "Cheng, Yong  and\nBapna, Ankur  and\nFirat, Orhan  and\nCao, Yuan  and\nWang, Pidong  and\nMacherey, Wolfgang"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs. The dominant inductive bias applied to these models is a shared vocabulary and a shared set of parameters across languages; the inputs and labels corresponding to examples drawn from different language pairs might still reside in distinct sub-spaces. In this paper, we introduce multilingual crossover encoder-decoder (mXEncDec) to fuse language pairs at an instance level. Our approach interpolates instances from different language pairs into joint ‘crossover examples’ in order to encourage sharing input and output spaces across languages. To ensure better fusion of examples in multilingual settings, we propose several techniques to improve example interpolation across dissimilar languages under heavy data imbalance. Experiments on a large-scale WMT multilingual dataset demonstrate that our approach significantly improves quality on English-to-Many, Many-to-English and zero-shot translation tasks (from +0.5 BLEU up to +5.5 BLEU points). Results on code-switching sets demonstrate the capability of our approach to improve model generalization to out-of-distribution multilingual examples. We also conduct qualitative and quantitative representation comparisons to analyze the advantages of our approach at the representation level.",
  "keywords": [
    "code",
    "bias",
    "bleu",
    "model generalization",
    "we",
    "fusion",
    "shot",
    "translation",
    "neural",
    "decoder",
    "the dominant inductive bias",
    "multilingual crossover encoder-decoder mxencdec",
    "-",
    "zero-shot",
    "generalization"
  ],
  "url": "https://aclanthology.org/2022.acl-long.282/",
  "provenance": {
    "collected_at": "2025-06-05 08:28:06",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}