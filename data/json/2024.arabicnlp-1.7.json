{
  "id": "2024.arabicnlp-1.7",
  "title": "Improving Language Models Trained on Translated Data with Continual Pre-Training and Dictionary Learning Analysis",
  "authors": [
    "Boughorbel, Sabri  and\nParvez, Md Rizwan  and\nHawasly, Majd"
  ],
  "year": "2024",
  "venue": "Proceedings of the Second Arabic Natural Language Processing Conference",
  "abstract": "Training LLMs in low resources languages usually utilizes machine translation (MT) data augmentation from English language. However, translation brings a number of challenges: there are large costs attached to translating and curating huge amounts of content with high-end machine translation solutions; the translated content carries over cultural biases; and if the translation is not faithful and accurate, the quality of the data degrades causing issues in the trained model. In this work, we investigate the role of translation and synthetic data in training language models. We translate TinyStories, a dataset of 2.2M short stories for 3-4 year old children, from English to Arabic using the open NLLB-3B MT model. We train a number of story generation models of size 1M-33M parameters using this data. We identify a number of quality and task-specific issues in the resulting models. To rectify these issues, we further pre-train the models with a small dataset of synthesized high-quality stories generated by a capable LLM in Arabic, representing 1% of the original training data. We show, using GPT-4 as a judge and dictionary learning analysis from mechanistic interpretability, that the suggested approach is a practical means to resolve some of the translation pitfalls. We illustrate the improvement through case studies of linguistic and cultural bias issues.",
  "keywords": [
    "bias",
    "end",
    "story generation models",
    "a capable llm",
    "we",
    "llm",
    "training",
    "translation",
    "means",
    "the translation pitfalls",
    "analysis",
    "llms",
    "the translation",
    "language models",
    "gpt-4"
  ],
  "url": "https://aclanthology.org/2024.arabicnlp-1.7/",
  "provenance": {
    "collected_at": "2025-06-05 11:02:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}