{
  "id": "2021.acl-long.428",
  "title": "B}andit{MTL}: Bandit-based Multi-task Learning for Text Classification",
  "authors": [
    "Mao, Yuren  and\nWang, Zekai  and\nLiu, Weiwei  and\nLin, Xuemin  and\nHu, Wenbin"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.",
  "keywords": [
    "the task variance",
    "text",
    "the generalization",
    "-",
    "multi-task text classification",
    "gradient",
    "regularization",
    "generalization",
    "learning",
    "variance",
    "analysis",
    "classification",
    "means",
    "unexplored",
    "multi"
  ],
  "url": "https://aclanthology.org/2021.acl-long.428/",
  "provenance": {
    "collected_at": "2025-06-05 08:05:10",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}