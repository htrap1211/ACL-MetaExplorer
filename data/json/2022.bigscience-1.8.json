{
  "id": "2022.bigscience-1.8",
  "title": "A Holistic Assessment of the Carbon Footprint of Noor, a Very Large {A}rabic Language Model",
  "authors": [
    "Lakim, Imad  and\nAlmazrouei, Ebtesam  and\nAbualhaol, Ibrahim  and\nDebbah, Merouane  and\nLaunay, Julien"
  ],
  "year": "2022",
  "venue": "Proceedings of BigScience Episode {\\#}5 -- Workshop on Challenges {\\&} Perspectives in Creating Large Language Models",
  "abstract": "As ever larger language models grow more ubiquitous, it is crucial to consider their environmental impact. Characterised by extreme size and resource use, recent generations of models have been criticised for their voracious appetite for compute, and thus significant carbon footprint. Although reporting of carbon impact has grown more common in machine learning papers, this reporting is usually limited to compute resources used strictly for training. In this work, we propose a holistic assessment of the footprint of an extreme-scale language model, Noor. Noor is an ongoing project aiming to develop the largest multi-task Arabic language models–with up to 13B parameters–leveraging zero-shot generalisation to enable a wide range of downstream tasks via natural language instructions. We assess the total carbon bill of the entire project: starting with data collection and storage costs, including research and development budgets, pretraining costs, future serving estimates, and other exogenous costs necessary for this international cooperation. Notably, we find that inference costs and exogenous factors can have a significant impact on total budget. Finally, we discuss pathways to reduce the carbon footprint of extreme-scale models.",
  "keywords": [
    "we",
    "shot",
    "training",
    "natural",
    "it",
    "generalisation",
    "zero-shot generalisation",
    "recent generations",
    "generations",
    "work",
    "ever larger language models",
    "language",
    "model",
    "machine",
    "machine learning papers"
  ],
  "url": "https://aclanthology.org/2022.bigscience-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 08:39:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}