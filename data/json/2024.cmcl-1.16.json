{
  "id": "2024.cmcl-1.16",
  "title": "Evaluating Grammatical Well-Formedness in Large Language Models: A Comparative Study with Human Judgments",
  "authors": [
    "Qiu, Zhuang  and\nDuan, Xufeng  and\nCai, Zhenguang"
  ],
  "year": "2024",
  "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
  "abstract": "Research in artificial intelligence has witnessed the surge of large language models (LLMs) demonstrating improved performance in various natural language processing tasks. This has sparked significant discussions about the extent to which large language models emulate human linguistic cognition and usage. This study delves into the representation of grammatical well-formedness in LLMs, which is a critical aspect of linguistic knowledge. In three preregistered experiments, we collected grammaticality judgment data for over 2400 English sentences with varying structures from ChatGPT and Vicuna, comparing them with human judgment data. The results reveal substantial alignment in the assessment of grammatical correctness between LLMs and human judgments, albeit with LLMs often showing more conservative judgments for grammatical correctness or incorrectness.",
  "keywords": [
    "alignment",
    "processing",
    "knowledge",
    "language",
    "natural",
    "human",
    "large language models",
    "large language models llms",
    "chatgpt",
    "we",
    "substantial alignment",
    "llms",
    "improved performance",
    "grammaticality",
    "comparative"
  ],
  "url": "https://aclanthology.org/2024.cmcl-1.16/",
  "provenance": {
    "collected_at": "2025-06-05 11:05:39",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}