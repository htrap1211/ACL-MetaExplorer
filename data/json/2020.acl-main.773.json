{
  "id": "2020.acl-main.773",
  "title": "Towards Robustifying {NLI} Models Against Lexical Dataset Biases",
  "authors": [
    "Zhou, Xiang  and\nBansal, Mohit"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.",
  "keywords": [
    "deep",
    "the label bias",
    "the bias",
    "bias",
    "word-overlapping bias",
    "we",
    "bag",
    "natural",
    "semantics",
    "high accuracy",
    "the dataset biases",
    "word",
    "learning",
    "biased",
    "the model bias"
  ],
  "url": "https://aclanthology.org/2020.acl-main.773/",
  "provenance": {
    "collected_at": "2025-06-05 07:52:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}