{
  "id": "2022.acl-long.9",
  "title": "U}ni{T}ran{S}e{R}: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System",
  "authors": [
    "Ma, Zhiyuan  and\nLi, Jianjun  and\nLi, Guohui  and\nCheng, Yongjing"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "As a more natural and intelligent interaction manner, multimodal task-oriented dialog system recently has received great attention and many remarkable progresses have been achieved. Nevertheless, almost all existing studies follow the pipeline to first learn intra-modal features separately and then conduct simple feature concatenation or attention-based feature fusion to generate responses, which hampers them from learning inter-modal interactions and conducting cross-modal feature alignment for generating more intention-aware responses. To address these issues, we propose UniTranSeR, a Unified Transformer Semantic Representation framework with feature alignment and intention reasoning for multimodal dialog systems. Specifically, we first embed the multimodal features into a unified Transformer semantic space to prompt inter-modal interactions, and then devise a feature alignment and intention reasoning (FAIR) layer to perform cross-modal entity alignment and fine-grained key-value reasoning, so as to effectively identify userâ€™s intention for generating more accurate responses. Experimental results verify the effectiveness of UniTranSeR, showing that it significantly outperforms state-of-the-art approaches on the representative MMD dataset.",
  "keywords": [
    "almost all existing studies",
    "layer",
    "semantic",
    "we",
    "feature alignment",
    "fusion",
    "great attention",
    "cross",
    "natural",
    "it",
    "unified",
    "multimodal task-oriented dialog system",
    "manner",
    "s",
    "cross-modal feature alignment"
  ],
  "url": "https://aclanthology.org/2022.acl-long.9/",
  "provenance": {
    "collected_at": "2025-06-05 08:24:25",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}