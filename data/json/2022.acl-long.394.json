{
  "id": "2022.acl-long.394",
  "title": "Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation",
  "authors": [
    "Moramarco, Francesco  and\nPapadopoulos Korfiatis, Alex  and\nPerera, Mark  and\nJuric, Damir  and\nFlann, Jack  and\nReiter, Ehud  and\nBelz, Anya  and\nSavkov, Aleksandar"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In recent years, machine learning models have rapidly become better at generating clinical consultation notes; yet, there is little work on how to properly evaluate the generated consultation notes to understand the impact they may have on both the clinician using them and the patientâ€™s clinical safety. To address this we present an extensive human evaluation study of consultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii) write their own notes, (iii) post-edit a number of automatically generated notes, and (iv) extract all the errors, both quantitative and qualitative. We then carry out a correlation study with 18 automatic quality metrics and the human judgements. We find that a simple, character-based Levenshtein distance metric performs on par if not better than common model-based metrics like BertScore. All our findings and annotations are open-sourced.",
  "keywords": [
    "work",
    "the generated consultation notes",
    "bertscore",
    "human evaluation",
    "i",
    "generation",
    "common model-based metrics",
    "automatically generated notes",
    "machine",
    "metric",
    "model",
    "18 automatic quality metrics",
    "human",
    "metrics",
    "machine learning models"
  ],
  "url": "https://aclanthology.org/2022.acl-long.394/",
  "provenance": {
    "collected_at": "2025-06-05 08:29:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}