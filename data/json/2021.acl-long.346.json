{
  "id": "2021.acl-long.346",
  "title": "Evaluation Examples are not Equally Informative: How should that change {NLP} Leaderboards?",
  "authors": [
    "Rodriguez, Pedro  and\nBarrow, Joe  and\nHoyle, Alexander Miserlis  and\nLalor, John P.  and\nJia, Robin  and\nBoyd-Graber, Jordan"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks.",
  "keywords": [
    "nlp",
    "the field",
    "model",
    "field",
    "latent",
    "subjects nlp models",
    "we",
    "nlp models",
    "nlp leaderboards leaderboards",
    "evaluation examples",
    "evaluation",
    "evaluation items examples",
    "that",
    "a bayesian leaderboard model",
    "informative examples"
  ],
  "url": "https://aclanthology.org/2021.acl-long.346/",
  "provenance": {
    "collected_at": "2025-06-05 08:04:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}