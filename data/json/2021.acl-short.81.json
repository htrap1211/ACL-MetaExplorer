{
  "id": "2021.acl-short.81",
  "title": "Improving Compositional Generalization in Classification Tasks via Structure Annotations",
  "authors": [
    "Kim, Juyong  and\nRavikumar, Pradeep  and\nAinslie, Joshua  and\nOntanon, Santiago"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  "abstract": "Compositional generalization is the ability to generalize systematically to a new data distribution by combining known components. Although humans seem to have a great ability to generalize compositionally, state-of-the-art neural models struggle to do so. In this work, we study compositional generalization in classification tasks and present two main contributions. First, we study ways to convert a natural language sequence-to-sequence dataset to a classification dataset that also requires compositional generalization. Second, we show that providing structural hints (specifically, providing parse trees and entity links as attention masks for a Transformer model) helps compositional generalization.",
  "keywords": [
    "work",
    "transformer",
    "language",
    "classification tasks",
    "neural",
    "natural",
    "model",
    "a classification dataset",
    "generalization",
    "attention",
    "we",
    "sequence",
    "attention masks",
    "a transformer model",
    "entity"
  ],
  "url": "https://aclanthology.org/2021.acl-short.81/",
  "provenance": {
    "collected_at": "2025-06-05 08:08:11",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}