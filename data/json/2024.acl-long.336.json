{
  "id": "2024.acl-long.336",
  "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing {LLM} Abilities",
  "authors": [
    "Stap, David  and\nHasler, Eva  and\nByrne, Bill  and\nMonz, Christof  and\nTran, Ke"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters.Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.",
  "keywords": [
    "the general translation quality",
    "machine translation",
    "we",
    "llm",
    "shot",
    "translation",
    "fine-tuning",
    "the fine-tuning data",
    "overall translation quality",
    "neural",
    "it",
    "llms",
    "abilities",
    "tuning",
    "document-level translation"
  ],
  "url": "https://aclanthology.org/2024.acl-long.336/",
  "provenance": {
    "collected_at": "2025-06-05 10:38:55",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}