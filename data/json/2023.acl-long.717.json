{
  "id": "2023.acl-long.717",
  "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
  "authors": [
    "Uppaal, Rheeya  and\nHu, Junjie  and\nLi, Yixuan"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive experiments demonstrate near-perfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming the fine-tuned counterpart.",
  "keywords": [
    "tuning",
    "objectives",
    "i",
    "language",
    "pre-trained language models",
    "model",
    "text",
    "question",
    "fine",
    "we",
    "pre",
    "several competitive fine-tuning objectives",
    "fine-tuning",
    "unexplored",
    "predictions"
  ],
  "url": "https://aclanthology.org/2023.acl-long.717/",
  "provenance": {
    "collected_at": "2025-06-05 09:45:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}