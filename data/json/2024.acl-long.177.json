{
  "id": "2024.acl-long.177",
  "title": "Open {K}o-{LLM} Leaderboard: Evaluating Large Language Models in {K}orean with {K}o-H5 Benchmark",
  "authors": [
    "Park, Chanjun  and\nKim, Hyeonwoo  and\nKim, Dahyun  and\nCho, SeongHwan  and\nKim, Sanghoon  and\nLee, Sukyung  and\nKim, Yungi  and\nLee, Hwalsuk"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.",
  "keywords": [
    "the korean llm community",
    "language",
    "support",
    "large language models",
    "the open ko-llm leaderboard",
    "k",
    "a robust evaluation framework",
    "we",
    "evaluation",
    "llm evaluation",
    "llm",
    "analysis",
    "llms",
    "that",
    "orean"
  ],
  "url": "https://aclanthology.org/2024.acl-long.177/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:42",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}