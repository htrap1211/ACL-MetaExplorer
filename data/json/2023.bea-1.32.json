{
  "id": "2023.bea-1.32",
  "title": "Automated evaluation of written discourse coherence using {GPT}-4",
  "authors": [
    "Naismith, Ben  and\nMulcaire, Phoebe  and\nBurstein, Jill"
  ],
  "year": "2023",
  "venue": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
  "abstract": "The popularization of large language models (LLMs) such as OpenAIâ€™s GPT-3 and GPT-4 have led to numerous innovations in the field of AI in education. With respect to automated writing evaluation (AWE), LLMs have reduced challenges associated with assessing writing quality characteristics that are difficult to identify automatically, such as discourse coherence. In addition, LLMs can provide rationales for their evaluations (ratings) which increases score interpretability and transparency. This paper investigates one approach to producing ratings by training GPT-4 to assess discourse coherence in a manner consistent with expert human raters. The findings of the study suggest that GPT-4 has strong potential to produce discourse coherence ratings that are comparable to human ratings, accompanied by clear rationales. Furthermore, the GPT-4 ratings outperform traditional NLP coherence metrics with respect to agreement with human ratings. These results have implications for advancing AWE technology for learning and assessment.",
  "keywords": [
    "field",
    "the field",
    "gpt-3",
    "their evaluations ratings",
    "learning",
    "manner",
    "the gpt-4 ratings",
    "llms",
    "traditional nlp coherence metrics",
    "a manner",
    "metrics",
    "large language models llms",
    "gpt-4",
    "language",
    "ai"
  ],
  "url": "https://aclanthology.org/2023.bea-1.32/",
  "provenance": {
    "collected_at": "2025-06-05 10:21:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}