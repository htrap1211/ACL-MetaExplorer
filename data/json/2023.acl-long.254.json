{
  "id": "2023.acl-long.254",
  "title": "Multi-modal Action Chain Abductive Reasoning",
  "authors": [
    "Li, Mengze  and\nWang, Tianbao  and\nXu, Jiahe  and\nHan, Kairong  and\nZhang, Shengyu  and\nZhao, Zhou  and\nMiao, Jiaxu  and\nZhang, Wenqiao  and\nPu, Shiliang  and\nWu, Fei"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Abductive Reasoning, has long been considered to be at the core ability of humans, which enables us to infer the most plausible explanation of incomplete known phenomena in daily life. However, such critical reasoning capability is rarely investigated for contemporary AI systems under such limited observations. To facilitate this research community, this paper sheds new light onAbductive Reasoningby studying a new vision-language task,Multi-modalAction chain abductiveReasoning (MAR), together with a large-scaleAbductive Reasoningdataset: Given an incomplete set of language described events, MAR aims to imagine the most plausible event by spatio-temporal grounding in past video and then infer the hypothesis of subsequent action chain that can best explain the language premise. To solve this task, we propose a strong baseline model that realizes MAR from two perspectives: (i) we first introduce the transformer, which learns to encode the observation to imagine the plausible event with explicitly interpretable event grounding in the video based on the commonsense knowledge recognition ability. (ii) To complete the assumption of a follow-up action chain, we design a novel symbolic module that can complete strict derivation of the progressive action chain layer by layer. We conducted extensive experiments on the proposed dataset, and the experimental study shows that the proposed model significantly outperforms existing video-language models in terms of effectiveness on our newly created MAR dataset.",
  "keywords": [
    "chain",
    "layer",
    "we",
    "existing video-language models",
    "core",
    "i",
    "the transformer",
    "action",
    "transformer",
    "knowledge",
    "language",
    "model",
    "us",
    "mar",
    "multi"
  ],
  "url": "https://aclanthology.org/2023.acl-long.254/",
  "provenance": {
    "collected_at": "2025-06-05 09:38:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}