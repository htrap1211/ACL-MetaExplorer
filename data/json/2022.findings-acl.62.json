{
  "id": "2022.findings-acl.62",
  "title": "A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction",
  "authors": [
    "Liu, Yang  and\nHu, Jinpeng  and\nWan, Xiang  and\nChang, Tsung-Hui"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each relation class implicitly with relation information, generally through designing complex network structures, like generating hybrid features, combining with contrastive learning or attention networks. We argue that relation information can be introduced more explicitly and effectively into the model. Thus, this paper proposes a direct addition approach to introduce relation information. Specifically, for each relation class, the relation representation is first generated by concatenating two views of relations (i.e., [CLS] token embedding and the mean value of embeddings of all tokens) and then directly added to the original prototype for both train and prediction. Experimental results on the benchmark dataset FewRel 1.0 show significant improvements and achieve comparable results to the state-of-the-art, which demonstrates the effectiveness of our proposed approach. Besides, further analyses verify that the direct addition is a much more effective way to integrate the relation representations and the original prototypes.",
  "keywords": [
    "extraction",
    "we",
    "shot",
    "training",
    "information",
    "views",
    "learning",
    "i",
    "two views",
    "embeddings",
    "attention networks",
    "model",
    "entities",
    "class",
    "train"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.62/",
  "provenance": {
    "collected_at": "2025-06-05 08:35:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}