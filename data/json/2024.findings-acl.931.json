{
  "id": "2024.findings-acl.931",
  "title": "emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation",
  "authors": [
    "Ma, Ziyang  and\nZheng, Zhisheng  and\nYe, Jiaxin  and\nLi, Jinchao  and\nGao, Zhifu  and\nZhang, ShiLiang  and\nChen, Xie"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "We propose emotion2vec, a universal speech emotion representation model. emotion2vec is pre-trained on open-source unlabeled emotion data through self-supervised online distillation, combining utterance-level loss and frame-level loss during pre-training. emotion2vec outperforms state-of-the-art pre-trained universal models and emotion specialist models by only training linear layers for the speech emotion recognition task on the mainstream IEMOCAP dataset. In addition, emotion2vec shows consistent improvements among 10 different languages of speech emotion recognition datasets. emotion2vec also shows excellent results on other emotion tasks, such as song emotion recognition, emotion prediction in conversation, and sentiment analysis. Comparison experiments, ablation experiments, and visualization comprehensively demonstrate the universal capability of the proposed emotion2vec. To the best of our knowledge, emotion2vec is the first universal representation model in various emotion-related tasks, filling a gap in the field.",
  "keywords": [
    "knowledge",
    "the field",
    "model",
    "pre-training emotion2vec",
    "self",
    "field",
    "the mainstream iemocap dataset",
    "loss",
    "conversation",
    "we",
    "pre",
    "iemocap",
    "visualization",
    "analysis",
    "training"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.931/",
  "provenance": {
    "collected_at": "2025-06-05 11:01:18",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}