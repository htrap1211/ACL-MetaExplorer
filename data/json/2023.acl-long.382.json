{
  "id": "2023.acl-long.382",
  "title": "Are Experts Needed? On Human Evaluation of Counselling Reflection Generation",
  "authors": [
    "Wu, Zixiu  and\nBalloccu, Simone  and\nReiter, Ehud  and\nHelaoui, Rim  and\nReforgiato Recupero, Diego  and\nRiboni, Daniele"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2.",
  "keywords": [
    "human evaluation",
    "client",
    "generation",
    "language",
    "such evaluations",
    "evaluation results",
    "human",
    "evaluations",
    "language models",
    "gpt-3",
    "the client",
    "the less powerful gpt-2",
    "reflection generation reflection",
    "we",
    "experts laypeople-based evaluation"
  ],
  "url": "https://aclanthology.org/2023.acl-long.382/",
  "provenance": {
    "collected_at": "2025-06-05 09:40:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}