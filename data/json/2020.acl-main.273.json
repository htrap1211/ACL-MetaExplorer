{
  "id": "2020.acl-main.273",
  "title": "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation",
  "authors": [
    "Yin, Yongjing  and\nMeng, Fandong  and\nSu, Jinsong  and\nZhou, Chulun  and\nYang, Zhengyuan  and\nZhou, Jie  and\nLuo, Jiebo"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",
  "keywords": [
    "the decoder",
    "a unified multi-modal graph",
    "neural machine translation",
    "multi-modal semantic units words",
    "semantic",
    "we",
    "our proposed encoder",
    "graph",
    "fusion",
    "translation",
    "neural",
    "semantic units",
    "unified",
    "decoder",
    "learning"
  ],
  "url": "https://aclanthology.org/2020.acl-main.273/",
  "provenance": {
    "collected_at": "2025-06-05 07:45:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}