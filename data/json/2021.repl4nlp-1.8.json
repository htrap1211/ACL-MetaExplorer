{
  "id": "2021.repl4nlp-1.8",
  "title": "Preserving Cross-Linguality of Pre-trained Models via Continual Learning",
  "authors": [
    "Liu, Zihan  and\nWinata, Genta Indra  and\nMadotto, Andrea  and\nFung, Pascale"
  ],
  "year": "2021",
  "venue": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
  "abstract": "Recently, fine-tuning pre-trained language models (e.g., multilingual BERT) to downstream cross-lingual tasks has shown promising results. However, the fine-tuning process inevitably changes the parameters of the pre-trained model and weakens its cross-lingual ability, which leads to sub-optimal performance. To alleviate this problem, we leverage continual learning to preserve the original cross-lingual ability of the pre-trained model when we fine-tune it to downstream tasks. The experimental result shows that our fine-tuning methods can better preserve the cross-lingual ability of the pre-trained model in a sentence retrieval task. Our methods also achieve better performance than other fine-tuning baselines on the zero-shot cross-lingual part-of-speech tagging and named entity recognition tasks.",
  "keywords": [
    "cross",
    "tuning",
    "process",
    "language",
    "bert",
    "model",
    "the fine-tuning process",
    "other fine-tuning baselines",
    "-",
    "retrieval",
    "a sentence retrieval task",
    "tagging",
    "our fine-tuning methods",
    "fine",
    "we"
  ],
  "url": "https://aclanthology.org/2021.repl4nlp-1.8/",
  "provenance": {
    "collected_at": "2025-06-05 08:19:24",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}