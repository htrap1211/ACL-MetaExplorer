{
  "id": "2024.acl-long.854",
  "title": "ICLEF}: In-Context Learning with Expert Feedback for Explainable Style Transfer",
  "authors": [
    "Saakyan, Arkadiy  and\nMuresan, Smaranda"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "While state-of-the-art large language models (LLMs) can excel at adapting text from one style to another, current work does not address the explainability of style transfer models. Recent work has explored generating textual explanations from larger teacher models and distilling them into smaller student models. One challenge with such approach is that LLM outputs may contain errors that require expertise to correct, but gathering and incorporating expert feedback is difficult due to cost and availability. To address this challenge, we propose ICLEF, a novel human-AI collaboration approach to model distillation that incorporates scarce expert human feedback by combining in-context learning and model self-critique. We show that our method leads to generation of high-quality synthetic explainable style transfer datasets for formality (E-GYAFC) and subjective bias (E-WNC). Via automatic and human evaluation, we show that specialized student models fine-tuned on our datasets outperform generalist teacher models on the explainable style transfer task in one-shot settings, and perform competitively compared to few-shot teacher models, highlighting the quality of the data and the role of expert feedback. In an extrinsic task of authorship attribution, we show that explanations generated by smaller models fine-tuned on E-GYAFC are more predictive of authorship than explanations generated by few-shot teacher models.",
  "keywords": [
    "bias",
    "feedback",
    "llm outputs",
    "we",
    "generalist teacher models",
    "current",
    "llm",
    "shot",
    "subjective bias",
    "automatic and human evaluation",
    "self",
    "few-shot teacher models",
    "e",
    "generating textual explanations",
    "learning"
  ],
  "url": "https://aclanthology.org/2024.acl-long.854/",
  "provenance": {
    "collected_at": "2025-06-05 10:46:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}