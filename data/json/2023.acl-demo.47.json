{
  "id": "2023.acl-demo.47",
  "title": "O}pen{ICL}: An Open-Source Framework for In-context Learning",
  "authors": [
    "Wu, Zhenyu  and\nWang, Yaoxiang  and\nYe, Jiacheng  and\nWu, Zhiyong  and\nFeng, Jiangtao  and\nXu, Jingjing  and\nQiao, Yu"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
  "abstract": "In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released athttps://github.com/Shark-NLP/OpenICL.",
  "keywords": [
    "parsing",
    "efficient",
    "large language model",
    "traditional fine-tuning methods",
    "increasing attentionand",
    "semantic",
    "we",
    "llm",
    "parameter",
    "translation",
    "classification",
    "friendly",
    "edge",
    "it",
    "unified"
  ],
  "url": "https://aclanthology.org/2023.acl-demo.47/",
  "provenance": {
    "collected_at": "2025-06-05 09:51:15",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}