{
  "id": "2024.acl-long.181",
  "title": "G}row{OVER}: How Can {LLM}s Adapt to Growing Real-World Knowledge?",
  "authors": [
    "Ko, Dayoon  and\nKim, Jinyoung  and\nChoi, Hahyeon  and\nKim, Gunhee"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In the real world, knowledge is constantly evolving, which can render existing knowledge-based datasets outdated. This unreliability highlights the critical need for continuous updates to ensure both accuracy and relevance in knowledge-intensive tasks. To address this, we propose GrowOVER-QA and GrowOVER-Dialogue, dynamic open-domain QA and dialogue benchmarks that undergo a continuous cycle of updates, keeping pace with the rapid evolution of knowledge. Our research indicates that retrieval-augmented language models (RaLMs) struggle with knowledge that has not been trained on or recently updated. Consequently, we introduce a novel retrieval-interactive language model framework, where the language model evaluates and reflects on its answers for further re-retrieval. Our exhaustive experiments demonstrate that our training-free framework significantly improves upon existing methods, performing comparably to or even surpassing continuously trained language models.",
  "keywords": [
    "the language model",
    "knowledge",
    "growover-dialogue dynamic open-domain qa",
    "retrieval-augmented language models ralms",
    "language",
    "model",
    "-",
    "retrieval",
    "dialogue benchmarks",
    "we",
    "dialogue",
    "growover-qa",
    "llm",
    "training",
    "continuously trained language models"
  ],
  "url": "https://aclanthology.org/2024.acl-long.181/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}