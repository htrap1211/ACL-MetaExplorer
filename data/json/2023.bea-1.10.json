{
  "id": "2023.bea-1.10",
  "title": "Difficulty-Controllable Neural Question Generation for Reading Comprehension using Item Response Theory",
  "authors": [
    "Uto, Masaki  and\nTomikawa, Yuto  and\nSuzuki, Ayaka"
  ],
  "year": "2023",
  "venue": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
  "abstract": "Question generation (QG) for reading comprehension, a technology for automatically generating questions related to given reading passages, has been used in various applications, including in education. Recently, QG methods based on deep neural networks have succeeded in generating fluent questions that are pertinent to given reading passages. One example of how QG can be applied in education is a reading tutor that automatically offers reading comprehension questions related to various reading materials. In such an application, QG methods should provide questions with difficulty levels appropriate for each learner’s reading ability in order to improve learning efficiency. Several difficulty-controllable QG methods have been proposed for doing so. However, conventional methods focus only on generating questions and cannot generate answers to them. Furthermore, they ignore the relation between question difficulty and learner ability, making it hard to determine an appropriate difficulty for each learner. To resolve these problems, we propose a new method for generating question–answer pairs that considers their difficulty, estimated using item response theory. The proposed difficulty-controllable generation is realized by extending two pre-trained transformer models: BERT and GPT-2.",
  "keywords": [
    "deep",
    "two pre-trained transformer models",
    "question",
    "deep neural networks",
    "efficiency",
    "we",
    "the proposed difficulty-controllable generation",
    "difficulty-controllable neural question generation",
    "answer",
    "neural",
    "it",
    "gpt-2",
    "learner ability",
    "bert",
    "each learner"
  ],
  "url": "https://aclanthology.org/2023.bea-1.10/",
  "provenance": {
    "collected_at": "2025-06-05 10:21:23",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}