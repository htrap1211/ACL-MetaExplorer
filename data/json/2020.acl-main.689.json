{
  "id": "2020.acl-main.689",
  "title": "Learning a Multi-Domain Curriculum for Neural Machine Translation",
  "authors": [
    "Wang, Wei  and\nTian, Ye  and\nNgiam, Jiquan  and\nYang, Yinfei  and\nCaswell, Isaac  and\nParekh, Zarana"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.",
  "keywords": [
    "neural",
    "machine",
    "machine translation",
    "we",
    "training",
    "translation",
    "multi",
    "this",
    "all domains",
    "batches",
    "the use",
    "data",
    "the multi-domain curriculum",
    "relevance",
    "a training curriculum"
  ],
  "url": "https://aclanthology.org/2020.acl-main.689/",
  "provenance": {
    "collected_at": "2025-06-05 07:51:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}