{
  "id": "2022.findings-acl.135",
  "title": "Morphosyntactic Tagging with Pre-trained Language Models for {A}rabic and its Dialects",
  "authors": [
    "Inoue, Go  and\nKhalifa, Salam  and\nHabash, Nizar"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8% in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario. Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect. Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.",
  "keywords": [
    "transformer",
    "different varieties",
    "tuning",
    "language",
    "pre-trained language models",
    "varieties",
    "tagging",
    "we",
    "strategic fine-tuning",
    "pre",
    "training",
    "existing systems",
    "our results",
    "external linguistic resources",
    "the-art"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.135/",
  "provenance": {
    "collected_at": "2025-06-05 08:36:41",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}