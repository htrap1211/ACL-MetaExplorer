{
  "id": "2022.acl-long.199",
  "title": "Variational Graph Autoencoding as Cheap Supervision for {AMR} Coreference Resolution",
  "authors": [
    "Li, Irene  and\nSong, Linfeng  and\nXu, Kun  and\nYu, Dong"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Coreference resolution over semantic graphs like AMRs aims to group the graph nodes that represent the same entity. This is a crucial step for making document-level formal semantic representations. With annotated data on AMR coreference resolution, deep learning approaches have recently shown great potential for this task, yet they are usually data hunger and annotations are costly. We propose a general pretraining method using variational graph autoencoder (VGAE) for AMR coreference resolution, which can leverage any general AMR corpus and even automatically parsed AMR data. Experiments on benchmarks show that the pretraining approach achieves performance gains of up to 6% absolute F1 points. Moreover, our model significantly improves on the previous state-of-the-art model by up to 11% F1.",
  "keywords": [
    "deep",
    "up to 11 f1",
    "general",
    "model",
    "amr coreference resolution",
    "semantic graphs",
    "deep learning approaches",
    "variational graph autoencoder vgae",
    "autoencoder",
    "a general pretraining method",
    "semantic",
    "we",
    "learning",
    "any general amr corpus",
    "graph"
  ],
  "url": "https://aclanthology.org/2022.acl-long.199/",
  "provenance": {
    "collected_at": "2025-06-05 08:26:59",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}