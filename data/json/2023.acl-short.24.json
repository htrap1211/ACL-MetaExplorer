{
  "id": "2023.acl-short.24",
  "title": "Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer",
  "authors": [
    "Lv, Xingtai  and\nDing, Ning  and\nQin, Yujia  and\nLiu, Zhiyuan  and\nSun, Maosong"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Recent studies show that large-scale pre-trained language models could be efficaciously adapted to particular tasks in a parameter-efficient manner. The trained lightweight set of parameters, such as adapters, can be easily stored and shared as a capability equipped with the corresponding models. Owning many lightweight parameters, we focus on transferring them between tasks to acquire an improvement in performance of new tasks, the key point of which is to obtain the similarity between tasks. In this paper, we explore 5 parameter-efficient weight ensembling methods to achieve such transferability and verify the effectiveness of them. These methods extract the information of datasets and trained lightweight parameters from different perspectives to obtain the similarity between tasks, and weight the existing lightweight parameters according to the comparability to acquire a suitable module for the initialization of new tasks. We apply them to three parameter-efficient tuning methods and test them on a wide set of downstream tasks. Experimental results show that our methods show an improvement of 5%~8% over baselines and could largely facilitate task-level knowledge transfer.",
  "keywords": [
    "efficient",
    "large-scale pre-trained language models",
    "we",
    "parameter",
    "parameter-efficient weight",
    "information",
    "transfer",
    "manner",
    "three parameter-efficient tuning methods",
    "tuning",
    "a parameter-efficient manner",
    "knowledge",
    "language",
    "studies",
    "pre"
  ],
  "url": "https://aclanthology.org/2023.acl-short.24/",
  "provenance": {
    "collected_at": "2025-06-05 09:48:34",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}