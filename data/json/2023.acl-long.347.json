{
  "id": "2023.acl-long.347",
  "title": "APOLLO}: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
  "authors": [
    "Sanyal, Soumya  and\nXu, Yichong  and\nWang, Shuohang  and\nYang, Ziyi  and\nPryzant, Reid  and\nYu, Wenhao  and\nZhu, Chenguang  and\nRen, Xiang"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.",
  "keywords": [
    "the semantics",
    "we",
    "training",
    "classification",
    "logiqa",
    "two self-supervised loss functions",
    "semantics",
    "it",
    "any general text corpus",
    "self",
    "loss",
    "a sentence-level classification loss",
    "processing",
    "text",
    "language models"
  ],
  "url": "https://aclanthology.org/2023.acl-long.347/",
  "provenance": {
    "collected_at": "2025-06-05 09:40:08",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}