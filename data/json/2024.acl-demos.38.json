{
  "id": "2024.acl-demos.38",
  "title": "L}lama{F}actory: Unified Efficient Fine-Tuning of 100+ Language Models",
  "authors": [
    "Zheng, Yaowei  and\nZhang, Richong  and\nZhang, Junhao  and\nYe, Yanhan  and\nLuo, Zheyan"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
  "abstract": "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks.",
  "keywords": [
    "the fine-tuning",
    "tuning",
    "edge",
    "language",
    "generation",
    "cutting-edge efficient training methods",
    "text",
    "it",
    "efficient",
    "large language models",
    "the efficiency",
    "modeling",
    "a unified framework",
    "unified",
    "100 language models"
  ],
  "url": "https://aclanthology.org/2024.acl-demos.38/",
  "provenance": {
    "collected_at": "2025-06-05 10:47:47",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}