{
  "id": "P18-1013",
  "title": "A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss",
  "authors": [
    "Hsu, Wan-Ting  and\nLin, Chieh-Kai  and\nLee, Ming-Ying  and\nMin, Kerui  and\nTang, Jing  and\nSun, Min"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.",
  "keywords": [
    "extractive and abstractive summarization",
    "a solid human evaluation",
    "end",
    "a unified model",
    "cnn",
    "our model sentence-level attention",
    "high rouge scores",
    "model",
    "sentence-level attention",
    "the word-level attention",
    "human",
    "attentions",
    "unified",
    "loss",
    "summarization"
  ],
  "url": "https://aclanthology.org/P18-1013/",
  "provenance": {
    "collected_at": "2025-06-05 00:09:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}