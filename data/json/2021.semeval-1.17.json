{
  "id": "2021.semeval-1.17",
  "title": "Zhestyatsky at {S}em{E}val-2021 Task 2: {R}e{LU} over Cosine Similarity for {BERT} Fine-tuning",
  "authors": [
    "Zhestiankin, Boris  and\nPonomareva, Maria"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "This paper presents our contribution to SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). Our experiments cover English (EN-EN) sub-track from the multilingual setting of the task. We experiment with several pre-trained language models and investigate an impact of different top-layers on fine-tuning. We find the combination of Cosine Similarity and ReLU activation leading to the most effective fine-tuning procedure. Our best model results in accuracy 92.7%, which is the fourth-best score in EN-EN sub-track.",
  "keywords": [
    "cross",
    "tuning",
    "language",
    "bert",
    "model",
    "activation",
    "-",
    "several pre-trained language models",
    "fine",
    "en",
    "we",
    "word",
    "pre",
    "relu",
    "fine-tuning"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.17/",
  "provenance": {
    "collected_at": "2025-06-05 08:19:59",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}