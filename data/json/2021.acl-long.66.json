{
  "id": "2021.acl-long.66",
  "title": "Adapting High-resource {NMT} Models to Translate Low-resource Related Languages without Parallel Data",
  "authors": [
    "Ko, Wei-Jen  and\nEl-Kishky, Ahmed  and\nRenduchintala, Adithya  and\nChaudhary, Vishrav  and\nGoyal, Naman  and\nGuzm{\\'a}n, Francisco  and\nFung, Pascale  and\nKoehn, Philipp  and\nDiab, Mona"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.",
  "keywords": [
    "work",
    "families",
    "objectives",
    "language",
    "machine",
    "other translation baselines",
    "high-quality machine translation systems",
    "back-translation",
    "we",
    "three different language families",
    "translation",
    "adversarial objectives",
    "major",
    "this work",
    "a low-resource language"
  ],
  "url": "https://aclanthology.org/2021.acl-long.66/",
  "provenance": {
    "collected_at": "2025-06-05 08:00:19",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}