{
  "id": "2024.findings-acl.671",
  "title": "CMMLU}: Measuring massive multitask language understanding in {C}hinese",
  "authors": [
    "Li, Haonan  and\nZhang, Yixuan  and\nKoto, Fajri  and\nYang, Yifei  and\nZhao, Hai  and\nGong, Yeyun  and\nDuan, Nan  and\nBaldwin, Timothy"
  ],
  "year": "2024",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
  "abstract": "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of even 60%, which is the pass mark for Chinese exams. This highlights that there is substantial room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the modelsâ€™ performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models for Chinese.",
  "keywords": [
    "knowledge",
    "language",
    "natural",
    "humanities",
    "large language models",
    "most existing llms",
    "a thorough evaluation",
    "sciences",
    "an accuracy",
    "capabilities",
    "form",
    "the humanities",
    "we",
    "the capabilities",
    "evaluation"
  ],
  "url": "https://aclanthology.org/2024.findings-acl.671/",
  "provenance": {
    "collected_at": "2025-06-05 10:57:46",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}