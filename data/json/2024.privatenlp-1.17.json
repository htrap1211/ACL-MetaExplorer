{
  "id": "2024.privatenlp-1.17",
  "title": "Reinforcement Learning-Driven {LLM} Agent for Automated Attacks on {LLM}s",
  "authors": [
    "Wang, Xiangwen  and\nPeng, Jie  and\nXu, Kaidi  and\nYao, Huaxiu  and\nChen, Tianlong"
  ],
  "year": "2024",
  "venue": "Proceedings of the Fifth Workshop on Privacy in Natural Language Processing",
  "abstract": "Recently, there has been a growing focus on conducting attacks on large language models (LLMs) to assess LLMsâ€™ safety. Yet, existing attack methods face challenges, including the need to access model weights or merely ensuring LLMs output harmful information without controlling the specific content of their output. Exactly control of the LLM output can produce more inconspicuous attacks which could reveal a new page for LLM security. To achieve this, we propose RLTA: the Reinforcement Learning Targeted Attack, a framework that is designed for attacking language models (LLMs) and is adaptable to both white box (weight accessible) and black box (weight inaccessible) scenarios. It is capable of automatically generating malicious prompts that trigger target LLMs to produce specific outputs. We demonstrate RLTA in two different scenarios: LLM trojan detection and jailbreaking. The comprehensive experimental results show the potential of RLTA in enhancing the security measures surrounding contemporary LLMs.",
  "keywords": [
    "we",
    "llm",
    "llm security",
    "it",
    "information",
    "learning",
    "the reinforcement learning",
    "reinforcement learning-driven llm agent",
    "llms safety",
    "llms",
    "llm trojan detection",
    "language models",
    "prompts",
    "reinforcement",
    "language"
  ],
  "url": "https://aclanthology.org/2024.privatenlp-1.17/",
  "provenance": {
    "collected_at": "2025-06-05 11:09:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}