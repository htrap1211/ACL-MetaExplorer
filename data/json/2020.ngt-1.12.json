{
  "id": "2020.ngt-1.12",
  "title": "Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation",
  "authors": [
    "Gordon, Mitchell  and\nDuh, Kevin"
  ],
  "year": "2020",
  "venue": "Proceedings of the Fourth Workshop on Neural Generation and Translation",
  "abstract": "We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting. While both domain adaptation and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher.",
  "keywords": [
    "general",
    "knowledge",
    "general-domain data",
    "language",
    "neural",
    "machine",
    "efficient",
    "neural machine translation",
    "machine translation",
    "we",
    "sequence",
    "translation",
    "both domain adaptation",
    "three language pairs",
    "interaction"
  ],
  "url": "https://aclanthology.org/2020.ngt-1.12/",
  "provenance": {
    "collected_at": "2025-06-05 07:57:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}