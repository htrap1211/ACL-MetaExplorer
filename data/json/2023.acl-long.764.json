{
  "id": "2023.acl-long.764",
  "title": "Learning In-context Learning for Named Entity Recognition",
  "authors": [
    "Chen, Jiawei  and\nLu, Yaojie  and\nLin, Hongyu  and\nLou, Jie  and\nJia, Wei  and\nDai, Dai  and\nWu, Hua  and\nCao, Boxi  and\nHan, Xianpei  and\nSun, Le"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function Lambda_instruction, demonstrations, text.M, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., (Lambda . M) (instruction, demonstrations) ->F where F will be a new entity extractor F: text -> entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor. Experimental results on 4 few-shot NER datasets show that our method can effectively inject in-context NER ability into PLMs and significantly outperforms the PLMs+fine-tuning counterparts.",
  "keywords": [
    "instruction",
    "ner",
    "a meta-function pre-training algorithm",
    "fine-tuning counterparts",
    "named entity recognition",
    "entities",
    "f",
    "text",
    "4 few-shot ner datasets",
    "we",
    "learning",
    "pre",
    "entity",
    "function",
    "training"
  ],
  "url": "https://aclanthology.org/2023.acl-long.764/",
  "provenance": {
    "collected_at": "2025-06-05 09:46:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}