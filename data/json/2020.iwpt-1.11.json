{
  "id": "2020.iwpt-1.11",
  "title": "Self-Training for Unsupervised Parsing with {PRPN",
  "authors": [
    "Mohananey, Anhad  and\nKann, Katharina  and\nBowman, Samuel R."
  ],
  "year": "2020",
  "venue": "Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies",
  "abstract": "Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like language modeling. In this work, we propose self-training for neural UP models: we leverage aggregated annotations predicted by copies of our model as supervision for future copies. To be able to use our modelâ€™s predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a), such that it can be trained in a semi-supervised fashion. We then add examples with parses predicted by our model to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1% F1 and the previous state of the art by 1.6% F1. In addition, we show that our architecture can also be helpful for semi-supervised parsing in ultra-low-resource settings.",
  "keywords": [
    "work",
    "copies",
    "semi-supervised parsing",
    "parsing",
    "language",
    "1 6 f1",
    "neural",
    "model",
    "it",
    "self",
    "modeling",
    "8 1 f1",
    "we",
    "future copies",
    "language modeling"
  ],
  "url": "https://aclanthology.org/2020.iwpt-1.11/",
  "provenance": {
    "collected_at": "2025-06-05 07:56:14",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}