{
  "id": "2024.cmcl-1.1",
  "title": "BAMBINO}-{LM}: (Bilingual-)Human-Inspired Continual Pre-training of {B}aby{LM",
  "authors": [
    "Shen, Zhewen  and\nJoshi, Aditya  and\nChen, Ruey-Cheng"
  ],
  "year": "2024",
  "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
  "abstract": "Children from bilingual backgrounds benefit from interactions with parents and teachers to re-acquire their heritage language. In this paper, we investigate how this insight from behavioral study can be incorporated into the learning of small-scale language models. We introduce BAMBINO-LM, a continual pre-training strategy for BabyLM that uses a novel combination of alternation and PPO-based perplexity reward induced from a parent Italian model. Upon evaluation on zero-shot classification tasks for English and Italian, BAMBINO-LM improves the Italian language capability of a BabyLM baseline. Our ablation analysis demonstrates that employing both the alternation strategy and PPO-based modeling is key to this effectiveness gain. We also show that, as a side effect, the proposed method leads to a similar degradation in L1 effectiveness as human children would have had in an equivalent learning scenario. Through its modeling and findings, BAMBINO-LM makes a focused contribution to the pre-training of small-scale language models by first developing a human-inspired strategy for pre-training and then showing that it results in behaviours similar to that of humans.",
  "keywords": [
    "ppo-based perplexity reward",
    "we",
    "bilingual backgrounds",
    "shot",
    "training",
    "classification",
    "backgrounds",
    "it",
    "small-scale language models",
    "learning",
    "analysis",
    "ppo",
    "zero-shot classification tasks",
    "perplexity",
    "-"
  ],
  "url": "https://aclanthology.org/2024.cmcl-1.1/",
  "provenance": {
    "collected_at": "2025-06-05 11:05:27",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}