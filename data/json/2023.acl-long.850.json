{
  "id": "2023.acl-long.850",
  "title": "Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion",
  "authors": [
    "Xu, Hongcai  and\nBao, Junpeng  and\nLiu, Wenbo"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Graph neural networks (GNNs), which effectively use topological structures in the knowledge graphs (KG) to embed entities and relations in low-dimensional spaces, have shown great power in knowledge graph completion (KGC). KG has abundant global and local structural information, however, many GNN-based KGC models cannot capture these two types of information about the graph structure by designing complex aggregation schemes, and are not designed well to learn representations of seen entities with sparse neighborhoods in isolated subgraphs. In this paper, we find that a simple attention-based method can outperform a general GNN-based approach for KGC. We then propose a double-branch multi-attention based graph neural network (MA-GNN) to learn more expressive entity representations which contain rich global-local structural information. Specifically, we first explore the graph attention network-based local aggregator to learn entity representations. Furthermore, we propose a snowball local attention mechanism by leveraging the semantic similarity between two-hop neighbors to enrich the entity embedding. Finally, we use Transformer-based self-attention to learn long-range dependence between entities to obtain richer representations with the global graph structure and entity features. Experimental results on five benchmark datasets show that MA-GNN achieves significant improvements over strong baselines for inductive KGC.",
  "keywords": [
    "kgc",
    "semantic",
    "we",
    "graph",
    "neural",
    "self",
    "a simple attention-based method",
    "information",
    "a general gnn-based approach",
    "rich",
    "the semantic similarity",
    "inductive kgc",
    "seen entities",
    "dimensional",
    "transformer"
  ],
  "url": "https://aclanthology.org/2023.acl-long.850/",
  "provenance": {
    "collected_at": "2025-06-05 09:47:18",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}