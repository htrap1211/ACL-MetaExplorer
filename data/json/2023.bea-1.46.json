{
  "id": "2023.bea-1.46",
  "title": "Transformer-based {H}ebrew {NLP} models for Short Answer Scoring in Biology",
  "authors": [
    "Gurin Schleifer, Abigail  and\nBeigman Klebanov, Beata  and\nAriely, Moriah  and\nAlexandron, Giora"
  ],
  "year": "2023",
  "venue": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
  "abstract": "Pre-trained large language models (PLMs) are adaptable to a wide range of downstream tasks by fine-tuning their rich contextual embeddings to the task, often without requiring much task-specific data. In this paper, we explore the use of a recently developed Hebrew PLM aleph-BERT for automated short answer grading of high school biology items. We show that the alephBERT-based system outperforms a strong CNN-based baseline, and that it general-izes unexpectedly well in a zero-shot paradigm to items on an unseen topic that address the same underlying biological concepts, opening up the possibility of automatically assessing new items without item-specific fine-tuning.",
  "keywords": [
    "embeddings",
    "transformer",
    "alephbert",
    "general",
    "answer",
    "a strong cnn-based baseline",
    "language",
    "nlp",
    "bert",
    "cnn",
    "it",
    "a zero-shot paradigm",
    "topic",
    "their rich contextual embeddings",
    "fine"
  ],
  "url": "https://aclanthology.org/2023.bea-1.46/",
  "provenance": {
    "collected_at": "2025-06-05 10:21:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}