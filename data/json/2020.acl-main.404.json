{
  "id": "2020.acl-main.404",
  "title": "Masking Actor Information Leads to Fairer Political Claims Detection",
  "authors": [
    "Dayanik, Erenay  and\nPad{\\'o}, Sebastian"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.",
  "keywords": [
    "bias",
    "sciences",
    "we",
    "training",
    "neural",
    "properties",
    "information",
    "debiasing",
    "personal information bias",
    "analysis",
    "i",
    "text",
    "frequency bias",
    "computational social sciences css",
    "textual properties"
  ],
  "url": "https://aclanthology.org/2020.acl-main.404/",
  "provenance": {
    "collected_at": "2025-06-05 07:47:37",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}