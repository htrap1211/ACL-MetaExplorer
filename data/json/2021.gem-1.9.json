{
  "id": "2021.gem-1.9",
  "title": "Human Evaluation of Creative {NLG} Systems: An Interdisciplinary Survey on Recent Papers",
  "authors": [
    "H{\\\"a}m{\\\"a}l{\\\"a}inen, Mika  and\nAlnajjar, Khalid"
  ],
  "year": "2021",
  "venue": "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
  "abstract": "We survey human evaluation in papers presenting work on creative natural language generation that have been published in INLG 2020 and ICCC 2020. The most typical human evaluation method is a scaled survey, typically on a 5 point scale, while many other less common methods exist. The most commonly evaluated parameters are meaning, syntactic correctness, novelty, relevance and emotional value, among many others. Our guidelines for future evaluation include clearly defining the goal of the generative system, asking questions as concrete as possible, testing the evaluation setup, using multiple different evaluation setups, reporting the entire evaluation process and potential biases clearly, and finally analyzing the evaluation results in a more profound way than merely reporting the most typical statistics.",
  "keywords": [
    "work",
    "the evaluation results",
    "potential biases",
    "generative",
    "human evaluation",
    "process",
    "creative natural language generation",
    "the generative system",
    "language",
    "generation",
    "natural",
    "human",
    "future evaluation",
    "the evaluation setup",
    "multiple different evaluation setups"
  ],
  "url": "https://aclanthology.org/2021.gem-1.9/",
  "provenance": {
    "collected_at": "2025-06-05 08:17:26",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}