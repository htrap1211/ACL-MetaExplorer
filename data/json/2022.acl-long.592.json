{
  "id": "2022.acl-long.592",
  "title": "F}lip{DA}: Effective and Robust Data Augmentation for Few-Shot Learning",
  "authors": [
    "Zhou, Jing  and\nZheng, Yanan  and\nTang, Jie  and\nJian, Li  and\nYang, Zhilin"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Most previous methods for text data augmentation are limited to simple tasks and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters). Under this setting, we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much. To address this challenge, we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data. Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data. Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustnessâ€”it substantially improves many tasks while not negatively affecting the others.",
  "keywords": [
    "generative",
    "a generative model",
    "i",
    "language",
    "natural",
    "classifier",
    "model",
    "it",
    "text",
    "we",
    "a classifier",
    "shot",
    "a large number",
    "that",
    "hard tasks"
  ],
  "url": "https://aclanthology.org/2022.acl-long.592/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}