{
  "id": "2021.acl-long.199",
  "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models",
  "authors": [
    "Liu, Tongtong  and\nFeng, Fangxiang  and\nWang, Xiaojie"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages. We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERT-S) which is with 45.9% parameters of the original LXMERT model and only 11.44% of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task.",
  "keywords": [
    "the original pre-training data",
    "image-text retrieval task",
    "several different pre-training tasks",
    "granularities",
    "different granularities",
    "we",
    "current",
    "training",
    "retrieval",
    "information",
    "pre-training models",
    "word",
    "text",
    "simplified",
    "a simplified lxmert lxmert-s"
  ],
  "url": "https://aclanthology.org/2021.acl-long.199/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:05",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}