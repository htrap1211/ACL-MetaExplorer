{
  "id": "2021.semeval-1.39",
  "title": "S}em{E}val-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents ({SEM}-{TAB}-{FACTS})",
  "authors": [
    "Wang, Nancy X. R.  and\nMahajan, Diwakar  and\nDanilevsky, Marina  and\nRosenthal, Sara"
  ],
  "year": "2021",
  "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
  "abstract": "Understanding tables is an important and relevant task that involves understanding table structure as well as being able to compare and contrast information within cells. In this paper, we address this challenge by presenting a new dataset and tasks that addresses this goal in a shared task in SemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS). Our dataset contains 981 manually-generated tables and an auto-generated dataset of 1980 tables providing over 180K statement and over 16M evidence annotations. SEM-TAB-FACTS featured two sub-tasks. In sub-task A, the goal was to determine if a statement is supported, refuted or unknown in relation to a table. In sub-task B, the focus was on identifying the specific cells of a table that provide evidence for the statement. 69 teams signed up to participate in the task with 19 successful submissions to subtask A and 12 successful submissions to subtask B. We present our results and main findings from the competition.",
  "keywords": [
    "scientific documents",
    "em",
    "scientific documents sem-tab-facts",
    "we",
    "sem",
    "information",
    "an auto-generated dataset",
    "scientific",
    "-",
    "981 manually-generated tables",
    "documents",
    "tabular data",
    "this goal",
    "the specific cells",
    "69 teams"
  ],
  "url": "https://aclanthology.org/2021.semeval-1.39/",
  "provenance": {
    "collected_at": "2025-06-05 08:20:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}