{
  "id": "P19-1214",
  "title": "Self-Supervised Learning for Contextualized Extractive Summarization",
  "authors": [
    "Wang, Hong  and\nWang, Xin  and\nXiong, Wenhan  and\nYu, Mo  and\nGuo, Xiaoxiao  and\nChang, Shiyu  and\nWang, William Yang"
  ],
  "year": "2019",
  "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Existing models for extractive summarization are usually trained from scratch with a cross-entropy loss, which does not explicitly capture the global context at the document level. In this paper, we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion. Experiments on the widely-used CNN/DM dataset validate the effectiveness of the proposed auxiliary tasks. Furthermore, we show that after pre-training, a clean model with simple building blocks is able to outperform previous state-of-the-art that are carefully designed.",
  "keywords": [
    "cross",
    "cnn",
    "model",
    "extractive summarization",
    "self",
    "loss",
    "three auxiliary pre-training tasks",
    "summarization",
    "we",
    "learning",
    "pre",
    "training",
    "entropy",
    "that",
    "the effectiveness"
  ],
  "url": "https://aclanthology.org/P19-1214/",
  "provenance": {
    "collected_at": "2025-06-05 00:35:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}