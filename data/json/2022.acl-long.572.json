{
  "id": "2022.acl-long.572",
  "title": "A Comparison of Strategies for Source-Free Domain Adaptation",
  "authors": [
    "Su, Xin  and\nZhao, Yiyun  and\nBethard, Steven"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Data sharing restrictions are common in NLP, especially in the clinical domain, but there is limited research on adapting models to new domains without access to the original training data, a setting known as source-free domain adaptation. We take algorithms that traditionally assume access to the source-domain training data—active learning, self-training, and data augmentation—and adapt them for source free domain adaptation. Then we systematically compare these different strategies across multiple tasks and domains. We find that active learning yields consistent gains across all SemEval 2021 Task 10 tasks and domains, but though the shared task saw successful self-trained and data augmented models, our systematic comparison finds these strategies to be unreliable for source-free domain adaptation.",
  "keywords": [
    "these different strategies",
    "these strategies",
    "nlp",
    "strategies",
    "self",
    "we",
    "learning",
    "training",
    "that",
    "limited research",
    "unreliable",
    "original",
    "new",
    "task",
    "data"
  ],
  "url": "https://aclanthology.org/2022.acl-long.572/",
  "provenance": {
    "collected_at": "2025-06-05 08:32:00",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}