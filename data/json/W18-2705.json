{
  "id": "W18-2705",
  "title": "Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation",
  "authors": [
    "Khayrallah, Huda  and\nThompson, Brian  and\nDuh, Kevin  and\nKoehn, Philipp"
  ],
  "year": "2018",
  "venue": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
  "abstract": "Supervised domain adaptation—where a large generic corpus and a smaller in-domain corpus are both available for training—is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model’s output word distribution and that of the out-of-domain model to prevent the model’s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU.",
  "keywords": [
    "cross",
    "general",
    "a generic model",
    "neural",
    "model",
    "the cross entropy",
    "machine",
    "it",
    "objective",
    "neural machine translation",
    "generic",
    "bleu",
    "the training objective",
    "word",
    "we"
  ],
  "url": "https://aclanthology.org/W18-2705/",
  "provenance": {
    "collected_at": "2025-06-05 00:23:02",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}