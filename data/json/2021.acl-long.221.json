{
  "id": "2021.acl-long.221",
  "title": "Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation",
  "authors": [
    "Jiao, Wenxiang  and\nWang, Xing  and\nTu, Zhaopeng  and\nShi, Shuming  and\nLyu, Michael  and\nKing, Irwin"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  "abstract": "Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.",
  "keywords": [
    "end",
    "we",
    "training",
    "translation",
    "neural",
    "self",
    "learning",
    "work",
    "neural machine translation self-training",
    "model",
    "machine",
    "the translation quality",
    "approach",
    "frequency",
    "synthetic parallel data"
  ],
  "url": "https://aclanthology.org/2021.acl-long.221/",
  "provenance": {
    "collected_at": "2025-06-05 08:02:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}