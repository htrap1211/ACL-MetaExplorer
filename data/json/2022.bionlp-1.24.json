{
  "id": "2022.bionlp-1.24",
  "title": "Low Resource Causal Event Detection from Biomedical Literature",
  "authors": [
    "Liang, Zhengzhong  and\nNoriega-Atala, Enrique  and\nMorrison, Clayton  and\nSurdeanu, Mihai"
  ],
  "year": "2022",
  "venue": "Proceedings of the 21st Workshop on Biomedical Language Processing",
  "abstract": "Recognizing causal precedence relations among the chemical interactions in biomedical literature is crucial to understanding the underlying biological mechanisms. However, detecting such causal relation can be hard because: (1) many times, such causal relations among events are not explicitly expressed by certain phrases but implicitly implied by very diverse expressions in the text, and (2) annotating such causal relation detection datasets requires considerable expert knowledge and effort. In this paper, we propose a strategy to address both challenges by training neural models with in-domain pre-training and knowledge distillation. We show that, by using very limited amount of labeled data, and sufficient amount of unlabeled data, the neural models outperform previous baselines on the causal precedence detection task, and are ten times faster at inference compared to the BERT base model.",
  "keywords": [
    "the bert base model",
    "knowledge",
    "neural",
    "bert",
    "model",
    "text",
    "sufficient amount",
    "sufficient",
    "we",
    "pre",
    "training",
    "effort",
    "interactions",
    "base",
    "certain"
  ],
  "url": "https://aclanthology.org/2022.bionlp-1.24/",
  "provenance": {
    "collected_at": "2025-06-05 08:39:51",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}