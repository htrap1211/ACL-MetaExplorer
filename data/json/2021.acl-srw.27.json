{
  "id": "2021.acl-srw.27",
  "title": "MVP}-{BERT}: Multi-Vocab Pre-training for {C}hinese {BERT",
  "authors": [
    "Zhu, Wei"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
  "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary (vocab) for these Chinese PLMs remains to be the one provided by Google Chinese BERT (CITATION), which is based on Chinese characters (chars). Second, the masked language model pre-training is based on a single vocab, limiting its downstream task performances. In this work, we first experimentally demonstrate that building a vocab via Chinese word segmentation (CWS) guided sub-word tokenization (SGT) can improve the performances of Chinese PLMs. Then we propose two versions of multi-vocab pre-training (MVP), Hi-MVP and AL-MVP, to improve the models’ expressiveness. Experiments show that: (a) MVP training strategies improve PLMs’ downstream performances, especially it can improve the PLM’s performances on span-level tasks; (b) our AL-MVP outperforms the recent AMBERT (CITATION) after large-scale pre-training, and it is more robust against adversarial attacks.",
  "keywords": [
    "a mvp training strategies",
    "al",
    "we",
    "training",
    "natural",
    "it",
    "pre-trained language models plms",
    "word",
    "google chinese bert citation",
    "processing",
    "the recent ambert citation",
    "tokenization",
    "bert",
    "strategies",
    "-"
  ],
  "url": "https://aclanthology.org/2021.acl-srw.27/",
  "provenance": {
    "collected_at": "2025-06-05 08:09:21",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}