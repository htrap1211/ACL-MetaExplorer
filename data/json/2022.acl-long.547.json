{
  "id": "2022.acl-long.547",
  "title": "Lexical Knowledge Internalization for Neural Dialog Generation",
  "authors": [
    "Wu, Zhiyong  and\nBi, Wei  and\nLi, Xiang  and\nKong, Lingpeng  and\nKao, Ben"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "We propose knowledge internalization (KI), which aims to complement the lexical knowledge into neural dialog models. Instead of further conditioning the knowledge-grounded dialog (KGD) models on externally retrieved knowledge, we seek to integrate knowledge about each input token internally into the modelâ€™s parameters. To tackle the challenge due to the large scale of lexical knowledge, we adopt the contrastive learning approach and create an effective token-level lexical knowledge retriever that requires only weak supervision mined from Wikipedia. We demonstrate the effectiveness and general applicability of our approach on various datasets and diversified model structures.",
  "keywords": [
    "general",
    "knowledge",
    "generation",
    "neural",
    "model",
    "kgd",
    "token",
    "diversified model structures",
    "dialog",
    "general applicability",
    "externally retrieved knowledge",
    "neural dialog generation",
    "we",
    "learning",
    "diversified"
  ],
  "url": "https://aclanthology.org/2022.acl-long.547/",
  "provenance": {
    "collected_at": "2025-06-05 08:31:40",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}