{
  "id": "2024.acl-long.168",
  "title": "MEL}o{RA}: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning",
  "authors": [
    "Ren, Pengjie  and\nShi, Chengshun  and\nWu, Shiguang  and\nZhang, Mengqi  and\nRen, Zhaochun  and\nde Rijke, Maarten  and\nChen, Zhumin  and\nPei, Jiahuan"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the modelsâ€™ scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential.The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.",
  "keywords": [
    "efficient",
    "we",
    "parameter",
    "instruction",
    "ensemble",
    "generalization errors",
    "natural",
    "better generalization ability",
    "core",
    "analysis",
    "tuning",
    "i",
    "pre-trained large language models",
    "generalization",
    "dimensional"
  ],
  "url": "https://aclanthology.org/2024.acl-long.168/",
  "provenance": {
    "collected_at": "2025-06-05 10:36:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}