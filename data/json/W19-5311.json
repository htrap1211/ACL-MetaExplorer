{
  "id": "W19-5311",
  "title": "The {TALP}-{UPC} Machine Translation Systems for {WMT}19 News Translation Task: Pivoting Techniques for Low Resource {MT",
  "authors": [
    "Casas, Noe  and\nFonollosa, Jos{\\'e} A. R.  and\nEscolano, Carlos  and\nBasta, Christine  and\nCosta-juss{\\`a}, Marta R."
  ],
  "year": "2019",
  "venue": "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
  "abstract": "In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for Kazakh-English. Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively. Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora.",
  "keywords": [
    "transformer",
    "finally a self-attention model",
    "language",
    "model",
    "machine",
    "19 news translation task",
    "self",
    "decoder",
    "attention",
    "we",
    "the wmt19 news translation",
    "the transformer architecture",
    "training",
    "translation",
    "the decoder part"
  ],
  "url": "https://aclanthology.org/W19-5311/",
  "provenance": {
    "collected_at": "2025-06-05 01:55:01",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}