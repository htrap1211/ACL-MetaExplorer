{
  "id": "2021.woah-1.1",
  "title": "Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers",
  "authors": [
    "Singh, Sumer  and\nLi, Sheng"
  ],
  "year": "2021",
  "venue": "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
  "abstract": "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.",
  "keywords": [
    "work",
    "bidirectional transformers",
    "transformer",
    "transformers",
    "language",
    "increasing attention",
    "model",
    "societal",
    "it",
    "albert",
    "its societal impact",
    "bidirectional transformer based methods",
    "attention",
    "we",
    "our approach albert da"
  ],
  "url": "https://aclanthology.org/2021.woah-1.1/",
  "provenance": {
    "collected_at": "2025-06-05 08:23:57",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}