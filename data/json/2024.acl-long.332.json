{
  "id": "2024.acl-long.332",
  "title": "Soft Knowledge Prompt: Help External Knowledge Become a Better Teacher to Instruct {LLM} in Knowledge-based {VQA",
  "authors": [
    "Wang, Qunbo  and\nJi, Ruyi  and\nPeng, Tianhao  and\nWu, Wenjun  and\nLi, Zechao  and\nLiu, Jing"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "LLM has achieved impressive performance on multi-modal tasks, which have received ever-increasing research attention. Recent research focuses on improving prediction performance and reliability (e.g., addressing the hallucination problem). They often prepend relevant external knowledge to the input text as an extra prompt. However, these methods would be affected by the noise in the knowledge and the context length limitation of LLM. In our work, we focus on making better use of external knowledge and propose a method to actively extract valuable information in the knowledge to produce the latent vector as a soft prompt, which is then fused with the image embedding to form a knowledge-enhanced context to instruct LLM. The experimental results on knowledge-based VQA benchmarks show that the proposed method enjoys better utilization of external knowledge and helps the model achieve better performance.",
  "keywords": [
    "ever-increasing research attention",
    "extra",
    "we",
    "llm",
    "soft knowledge prompt",
    "knowledge-based vqa benchmarks",
    "information",
    "latent",
    "a soft prompt",
    "vector",
    "prompt",
    "text",
    "the latent vector",
    "soft",
    "llm the experimental results"
  ],
  "url": "https://aclanthology.org/2024.acl-long.332/",
  "provenance": {
    "collected_at": "2025-06-05 10:38:52",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}