{
  "id": "2024.acl-long.203",
  "title": "ARL}2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
  "authors": [
    "Zhang, LingXi  and\nYu, Yue  and\nWang, Kuan  and\nZhang, Chao"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to separate training processes and the inherent black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score adaptive relevance evidence, enabling the retriever to learn from robust LLM supervision. Furthermore, ARL2 incorporates a self-training strategy to minimize the cost of API calls. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities.",
  "keywords": [
    "arl",
    "retrieval-augmented generation",
    "we",
    "llm",
    "shot",
    "training",
    "self",
    "retrieval",
    "information",
    "robust transfer learning capabilities",
    "transfer",
    "learning",
    "llms",
    "abilities",
    "strong zero-shot generalization abilities"
  ],
  "url": "https://aclanthology.org/2024.acl-long.203/",
  "provenance": {
    "collected_at": "2025-06-05 10:37:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}