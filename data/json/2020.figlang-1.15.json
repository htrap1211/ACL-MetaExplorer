{
  "id": "2020.figlang-1.15",
  "title": "D}etecting {S}arcasm in {C}onversation {C}ontext {U}sing {T}ransformer-{B}ased {M}odels",
  "authors": [
    "Avvaru, Adithya  and\nVobilisetty, Sanath  and\nMamidi, Radhika"
  ],
  "year": "2020",
  "venue": "Proceedings of the Second Workshop on Figurative Language Processing",
  "abstract": "Sarcasm detection, regarded as one of the sub-problems of sentiment analysis, is a very typical task because the introduction of sarcastic words can flip the sentiment of the sentence itself. To date, many research works revolve around detecting sarcasm in one single sentence and there is very limited research to detect sarcasm resulting from multiple sentences. Current models used Long Short Term Memory (LSTM) variants with or without attention to detect sarcasm in conversations. We showed that the models using state-of-the-art Bidirectional Encoder Representations from Transformers (BERT), to capture syntactic and semantic information across conversation sentences, performed better than the current models. Based on the data analysis, we estimated that the number of sentences in the conversation that can contribute to the sarcasm and the results agrees to this estimation. We also perform a comparative study of our different versions of BERT-based model with other variants of LSTM model and XLNet (both using the estimated number of conversation sentences) and find out that BERT-based models outperformed them.",
  "keywords": [
    "transformers",
    "conversations",
    "b",
    "lstm model",
    "semantic",
    "we",
    "onversation",
    "lstm",
    "current",
    "syntactic and semantic information",
    "u",
    "information",
    "bert-based model",
    "bert-based models",
    "the conversation"
  ],
  "url": "https://aclanthology.org/2020.figlang-1.15/",
  "provenance": {
    "collected_at": "2025-06-05 07:55:44",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}