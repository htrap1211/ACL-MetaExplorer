{
  "id": "P18-2090",
  "title": "GNEG}: Graph-Based Negative Sampling for word2vec",
  "authors": [
    "Zhang, Zheng  and\nZweigenbaum, Pierre"
  ],
  "year": "2018",
  "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Negative sampling is an important component in word2vec for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as random walk. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5% and improves the performance on word similarity tasks by about 1% compared to the skip-gram negative sampling baseline.",
  "keywords": [
    "random",
    "it",
    "word2vec negative sampling",
    "satisfies",
    "information",
    "network",
    "we",
    "word",
    "learning",
    "graph",
    "word2vec",
    "training",
    "occurrence",
    "approach",
    "distributed word representation learning"
  ],
  "url": "https://aclanthology.org/P18-2090/",
  "provenance": {
    "collected_at": "2025-06-05 00:18:42",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}