{
  "id": "2020.acl-main.378",
  "title": "Max-Margin Incremental {CCG} Parsing",
  "authors": [
    "Stanojevi{\\'c}, Milo{\\v{s}}  and\nSteedman, Mark"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.",
  "keywords": [
    "language modelling",
    "bias",
    "parsing",
    "question",
    "nlp researchers",
    "probabilities bias",
    "all",
    "we",
    "max",
    "processing",
    "three biases",
    "these three biases",
    "incremental parsing",
    "scientists",
    "biases"
  ],
  "url": "https://aclanthology.org/2020.acl-main.378/",
  "provenance": {
    "collected_at": "2025-06-05 07:47:16",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}