{
  "id": "2022.acl-long.526",
  "title": "Cluster {\\&} Tune: {B}oost Cold Start Performance in Text Classification",
  "authors": [
    "Shnarch, Eyal  and\nGera, Ariel  and\nHalfon, Alon  and\nDankin, Lena  and\nChoshen, Leshem  and\nAharonov, Ranit  and\nSlonim, Noam"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In real-world scenarios, a text classification task often begins with a cold start, when labeled data is scarce. In such cases, the common practice of fine-tuning pre-trained models, such as BERT, for a target classification task, is prone to produce poor performance. We suggest a method to boost the performance of such models by adding an intermediate unsupervised classification task, between the pre-training and fine-tuning phases. As such an intermediate task, we perform clustering and train the pre-trained model on predicting the cluster labels. We test this hypothesis on various data sets, and show that this additional classification phase can significantly improve performance, mainly for topical classification tasks, when the number of labeled instances available for fine-tuning is only a couple of dozen to a few hundred.",
  "keywords": [
    "tuning",
    "a target classification task",
    "a text classification task",
    "cluster",
    "bert",
    "model",
    "text",
    "fine-tuning pre-trained models",
    "this additional classification phase",
    "fine",
    "we",
    "topical classification tasks",
    "pre",
    "oost",
    "text classification"
  ],
  "url": "https://aclanthology.org/2022.acl-long.526/",
  "provenance": {
    "collected_at": "2025-06-05 08:31:22",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}