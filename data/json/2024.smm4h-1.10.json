{
  "id": "2024.smm4h-1.10",
  "title": "UTR}ad-{NLP} at {\\#}{SMM}4{H} 2024: Why {LLM}-Generated Texts Fail to Improve Text Classification Models",
  "authors": [
    "Yamagishi, Yosuke  and\nNakamura, Yuta"
  ],
  "year": "2024",
  "venue": "Proceedings of the 9th Social Media Mining for Health Research and Applications (SMM4H 2024) Workshop and Shared Tasks",
  "abstract": "In this paper, we present our approach to addressing the binary classification tasks, Tasks 5 and 6, as part of the Social Media Mining for Health (SMM4H) text classification challenge. Both tasks involved working with imbalanced datasets that featured a scarcity of positive examples. To mitigate this imbalance, we employed a Large Language Model to generate synthetic texts with positive labels, aiming to augment the training data for our text classification models. Unfortunately, this method did not significantly improve model performance. Through clustering analysis using text embeddings, we discovered that the generated texts significantly lacked diversity compared to the raw data. This finding highlights the challenges of using synthetic text generation for enhancing model efficacy in real-world applications, specifically in the context of health-related social media data.",
  "keywords": [
    "the generated texts",
    "llm -generated texts",
    "text classification challenge",
    "clustering analysis",
    "our text classification models",
    "we",
    "llm",
    "training",
    "classification",
    "text classification models",
    "analysis",
    "text",
    "a large language model",
    "mining",
    "embeddings"
  ],
  "url": "https://aclanthology.org/2024.smm4h-1.10/",
  "provenance": {
    "collected_at": "2025-06-05 11:10:35",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}