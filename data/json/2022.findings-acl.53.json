{
  "id": "2022.findings-acl.53",
  "title": "Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging",
  "authors": [
    "Hou, Yutai  and\nChen, Cheng  and\nLuo, Xianzhen  and\nLi, Bohan  and\nChe, Wanxiang"
  ],
  "year": "2022",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
  "abstract": "Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.",
  "keywords": [
    "inefficient",
    "few-shot learning",
    "decode",
    "we",
    "shot",
    "pieces",
    "learning",
    "prompt sentence pieces",
    "6 1 f1-scores",
    "prompt",
    "the classic prompts",
    "fast",
    "prompts",
    "model",
    "the prompting methods"
  ],
  "url": "https://aclanthology.org/2022.findings-acl.53/",
  "provenance": {
    "collected_at": "2025-06-05 08:35:33",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}