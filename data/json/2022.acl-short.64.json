{
  "id": "2022.acl-short.64",
  "title": "Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning",
  "authors": [
    "de Lhoneux, Miryam  and\nZhang, Sheng  and\nS{\\o}gaard, Anders"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  "abstract": "Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance onoutlierlanguages. We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting.",
  "keywords": [
    "cross",
    "performance onoutlierlanguages",
    "roberta",
    "parsing",
    "mbert",
    "language",
    "onoutlierlanguages",
    "syntactic parsing models wu",
    "the zero-shot setting",
    "dependency",
    "we",
    "learning",
    "transfer",
    "training",
    "shot"
  ],
  "url": "https://aclanthology.org/2022.acl-short.64/",
  "provenance": {
    "collected_at": "2025-06-05 08:33:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}