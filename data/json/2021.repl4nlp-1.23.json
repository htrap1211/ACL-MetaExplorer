{
  "id": "2021.repl4nlp-1.23",
  "title": "B}ayesian Model-Agnostic Meta-Learning with Matrix-Valued Kernels for Quality Estimation",
  "authors": [
    "Obamuyide, Abiola  and\nFomicheva, Marina  and\nSpecia, Lucia"
  ],
  "year": "2021",
  "venue": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
  "abstract": "Most current quality estimation (QE) models for machine translation are trained and evaluated in a fully supervised setting requiring significant quantities of labelled training data. However, obtaining labelled data can be both expensive and time-consuming. In addition, the test data that a deployed QE model would be exposed to may differ from its training data in significant ways. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. Thus, it is desirable to be able to adapt QE models efficiently to new user data with limited supervision data. To address these challenges, we propose a Bayesian meta-learning approach for adapting QE models to the needs and preferences of each user with limited supervision. To enhance performance, we further propose an extension to a state-of-the-art Bayesian meta-learning approach which utilizes a matrix-valued kernel for Bayesian meta-learning of quality estimation. Experiments on data with varying number of users and language characteristics demonstrates that the proposed Bayesian meta-learning approach delivers improved predictive performance in both limited and full supervision settings.",
  "keywords": [
    "end",
    "kernel",
    "machine translation",
    "we",
    "kernels",
    "quantities",
    "current",
    "training",
    "translation",
    "a matrix-valued kernel",
    "matrix-valued kernels",
    "it",
    "significant quantities",
    "learning",
    "translation quality"
  ],
  "url": "https://aclanthology.org/2021.repl4nlp-1.23/",
  "provenance": {
    "collected_at": "2025-06-05 08:19:36",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}