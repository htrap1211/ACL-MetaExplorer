{
  "id": "2020.acl-main.9",
  "title": "PLATO}: Pre-trained Dialogue Generation Model with Discrete Latent Variable",
  "authors": [
    "Bao, Siqi  and\nHe, Huang  and\nWang, Fan  and\nWu, Hua  and\nWang, Haifeng"
  ],
  "year": "2020",
  "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  "abstract": "Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.",
  "keywords": [
    "processing",
    "knowledge",
    "language generation",
    "generation",
    "language",
    "natural",
    "uni",
    "natural language processing tasks",
    "model",
    "dialogues",
    "conversations",
    "flexible attention mechanisms",
    "question",
    "latent",
    "conversational"
  ],
  "url": "https://aclanthology.org/2020.acl-main.9/",
  "provenance": {
    "collected_at": "2025-06-05 07:42:23",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}