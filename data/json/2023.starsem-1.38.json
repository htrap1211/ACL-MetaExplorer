{
  "id": "2023.starsem-1.38",
  "title": "FEED} {PET}s: Further Experimentation and Expansion on the Disambiguation of Potentially Euphemistic Terms",
  "authors": [
    "Lee, Patrick  and\nShode, Iyanuoluwa  and\nTrujillo, Alain  and\nZhao, Yuan  and\nOjo, Olumide  and\nPlancarte, Diana  and\nFeldman, Anna  and\nPeng, Jing"
  ],
  "year": "2023",
  "venue": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
  "abstract": "Transformers have been shown to work well for the task of English euphemism disambiguation, in which a potentially euphemistic term (PET) is classified as euphemistic or non-euphemistic in a particular context. In this study, we expand on the task in two ways. First, we annotate PETs for vagueness, a linguistic property associated with euphemisms, and find that transformers are generally better at classifying vague PETs, suggesting linguistic differences in the data that impact performance. Second, we present novel euphemism corpora in three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform euphemism disambiguation experiments in each language using multilingual transformer models mBERT and XLM-RoBERTa, establishing preliminary results from which to launch future work.",
  "keywords": [
    "transformer",
    "work",
    "transformers",
    "roberta",
    "mbert",
    "language",
    "we",
    "multilingual transformer models",
    "potentially euphemistic terms transformers",
    "that",
    "term",
    "yoruba",
    "mandarin chinese",
    "novel euphemism corpora",
    "the data"
  ],
  "url": "https://aclanthology.org/2023.starsem-1.38/",
  "provenance": {
    "collected_at": "2025-06-05 10:32:04",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}