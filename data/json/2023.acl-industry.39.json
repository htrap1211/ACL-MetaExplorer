{
  "id": "2023.acl-industry.39",
  "title": "Chemical Language Understanding Benchmark",
  "authors": [
    "Kim, Yunsoo  and\nKo, Hyuk  and\nLee, Jane  and\nHeo, Hyun Young  and\nYang, Jinyoung  and\nLee, Sungsoo  and\nLee, Kyu-hwang"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
  "abstract": "In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia.",
  "keywords": [
    "roberta",
    "language",
    "classification tasks",
    "nlp",
    "bert",
    "model",
    "text",
    "it",
    "the various language models",
    "we",
    "nlp research",
    "classification",
    "club chemical language",
    "both industry",
    "chemistry domain"
  ],
  "url": "https://aclanthology.org/2023.acl-industry.39/",
  "provenance": {
    "collected_at": "2025-06-05 09:52:31",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}