{
  "id": "2022.acl-long.447",
  "title": "S}a{F}e{RD}ialogues: Taking Feedback Gracefully after Conversational Safety Failures",
  "authors": [
    "Ung, Megan  and\nXu, Jing  and\nBoureau, Y-Lan"
  ],
  "year": "2022",
  "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "Current open-domain conversational models can easily be made to talk in inadequate ways. Online learning from conversational feedback given by the conversation partner is a promising avenue for a model to improve and adapt, so as to generate fewer of these safety failures. However, current state-of-the-art models tend to react to feedback with defensive or oblivious responses. This makes for an unpleasant experience and may discourage conversation partners from giving feedback in the future. This work proposes SaFeRDialogues, a task and dataset of graceful responses to conversational feedback about safety failures. We collect a dataset of 8k dialogues demonstrating safety failures, feedback signaling them, and a response acknowledging the feedback. We show how fine-tuning on this dataset results in conversations that human raters deem considerably more likely to lead to a civil conversation, without sacrificing engagingness or general conversational ability.",
  "keywords": [
    "work",
    "tuning",
    "conversational feedback",
    "general",
    "feedback",
    "model",
    "dialogues",
    "current open-domain conversational models",
    "human",
    "8k dialogues",
    "a civil conversation",
    "conversations",
    "an unpleasant experience",
    "conversational",
    "partner"
  ],
  "url": "https://aclanthology.org/2022.acl-long.447/",
  "provenance": {
    "collected_at": "2025-06-05 08:30:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}