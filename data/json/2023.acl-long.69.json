{
  "id": "2023.acl-long.69",
  "title": "NLG} Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist",
  "authors": [
    "Nimah, Iftitahu  and\nFang, Meng  and\nMenkovski, Vlado  and\nPechenizkiy, Mykola"
  ],
  "year": "2023",
  "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In this study, we analyze automatic evaluation metrics for Natural Language Generation (NLG), specifically task-agnostic metrics and human-aligned metrics. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at discerning system-level performance and quality of system outputs remain unclear. We present metric preference checklist as a framework to assess the effectiveness of automatic metrics in three NLG tasks: Text Summarization, Dialogue Response Generation, and Controlled Generation. Our proposed framework provides access: (i) for verifying whether automatic metrics are faithful to human preference, regardless of their correlation level to human; and (ii) for inspecting the strengths and limitations of NLG systems via pairwise evaluation. We show that automatic metrics provide a better guidance than human on discriminating system-level performance in Text Summarization and Controlled Generation tasks. We also show that multi-aspect human-aligned metric (UniEval) is not necessarily dominant over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic metrics (BLEU, BERTScore), particularly in Controlled Generation tasks.",
  "keywords": [
    "perplexity bleu bertscore",
    "pairwise evaluation",
    "desirable human-like qualities",
    "bleu",
    "text summarization",
    "automatic evaluation metrics",
    "summarization",
    "we",
    "dialogue",
    "qualities",
    "training",
    "bertscore",
    "natural",
    "unieval",
    "training objective"
  ],
  "url": "https://aclanthology.org/2023.acl-long.69/",
  "provenance": {
    "collected_at": "2025-06-05 09:36:17",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}