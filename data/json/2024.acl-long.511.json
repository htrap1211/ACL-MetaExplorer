{
  "id": "2024.acl-long.511",
  "title": "Large Language Models are not Fair Evaluators",
  "authors": [
    "Wang, Peiyi  and\nLi, Lei  and\nChen, Liang  and\nCai, Zefan  and\nZhu, Dawei  and\nLin, Binghuai  and\nCao, Yunbo  and\nKong, Lingpeng  and\nLiu, Qi  and\nLiu, Tianyu  and\nSui, Zhifang"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "In this paper, we uncover a positional bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. We propose a simple yet effective calibration framework to address our discovered positional bias.To evaluate the effectiveness of our framework, we manually annotate the “win/tie/lose” outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark’s question prompt. Extensive experiments demonstrate that our approach successfully alleviates evaluation bias, resulting in closer alignment with human judgments.",
  "keywords": [
    "e g",
    "the win tie",
    "alignment",
    "bias",
    "prompt",
    "language",
    "the evaluation paradigm",
    "our discovered positional bias",
    "model",
    "human",
    "large language models",
    "closer alignment",
    "queries",
    "question",
    "gpt-4"
  ],
  "url": "https://aclanthology.org/2024.acl-long.511/",
  "provenance": {
    "collected_at": "2025-06-05 10:41:20",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}