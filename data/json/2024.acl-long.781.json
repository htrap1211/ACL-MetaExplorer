{
  "id": "2024.acl-long.781",
  "title": "Emergent Word Order Universals from Cognitively-Motivated Language Models",
  "authors": [
    "Kuribayashi, Tatsuki  and\nUeda, Ryo  and\nYoshida, Ryo  and\nOseki, Yohei  and\nBriscoe, Ted  and\nBaldwin, Timothy"
  ],
  "year": "2024",
  "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "abstract": "The worldâ€™s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics.We study word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals.",
  "keywords": [
    "language",
    "object",
    "it",
    "specific parsing strategies",
    "strategies",
    "language models",
    "perplexity",
    "modeling",
    "cognitively-motivated language models",
    "such biases",
    "word",
    "we",
    "cognitively plausible biases",
    "cognitive biases",
    "biases"
  ],
  "url": "https://aclanthology.org/2024.acl-long.781/",
  "provenance": {
    "collected_at": "2025-06-05 10:45:03",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}