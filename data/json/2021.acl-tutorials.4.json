{
  "id": "2021.acl-tutorials.4",
  "title": "Pre-training Methods for Neural Machine Translation",
  "authors": [
    "Wang, Mingxuan  and\nLi, Lei"
  ],
  "year": "2021",
  "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts",
  "abstract": "This tutorial provides a comprehensive guide to make the most of pre-training for neural machine translation. Firstly, we will briefly introduce the background of NMT, pre-training methodology, and point out the main challenges when applying pre-training for NMT. Then we will focus on analysing the role of pre-training in enhancing the performance of NMT, how to design a better pre-training model for executing specific NMT tasks and how to better integrate the pre-trained model into NMT system. In each part, we will provide examples, discuss training techniques and analyse what is transferred when applying pre-training.",
  "keywords": [
    "a better pre-training model",
    "neural",
    "background",
    "the background",
    "model",
    "machine",
    "nmt pre-training methodology",
    "neural machine translation",
    "-",
    "we",
    "pre",
    "pre-training methods",
    "training",
    "translation",
    "nmt system"
  ],
  "url": "https://aclanthology.org/2021.acl-tutorials.4/",
  "provenance": {
    "collected_at": "2025-06-05 08:10:09",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}