{
  "id": "2023.findings-acl.603",
  "title": "Visual Coherence Loss for Coherent and Visually Grounded Story Generation",
  "authors": [
    "Hong, Xudong  and\nDemberg, Vera  and\nSayeed, Asad  and\nZheng, Qiankun  and\nSchiele, Bernt"
  ],
  "year": "2023",
  "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
  "abstract": "Local coherence is essential for long-form text generation models. We identify two important aspects of local coherence within the visual storytelling task: (1) the model needs to represent re-occurrences of characters within the image sequence in order to mention them correctly in the story; (2) character representations should enable us to find instances of the same characters and distinguish different characters. In this paper, we propose a loss function inspired by a linguistic theory of coherence for self-supervised learning for image sequence representations. We further propose combining features from an object and a face detector to construct stronger character features. To evaluate input-output relevance that current reference-based metrics donâ€™t measure, we propose a character matching metric to check whether the models generate referring expressions correctly for characters in input image sequences. Experiments on a visual story generation dataset show that our proposed features and loss function are effective for generating more coherent and visually grounded stories.",
  "keywords": [
    "a loss function",
    "form",
    "we",
    "current",
    "self",
    "loss",
    "sequence",
    "learning",
    "visual",
    "object",
    "metric",
    "text",
    "metrics",
    "-",
    "stories"
  ],
  "url": "https://aclanthology.org/2023.findings-acl.603/",
  "provenance": {
    "collected_at": "2025-06-05 10:16:49",
    "source": "ACL Anthology",
    "version": "1.0"
  }
}